{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd35694-5f5a-4d16-8669-ec1f5bf43393",
   "metadata": {},
   "source": [
    "# Connect with alternative LLMs\n",
    "* Talk with Open Source LLMs like Llama3 and Mixtral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4312ba1-de37-450b-89a3-c0b21d955801",
   "metadata": {},
   "source": [
    "## Caveat\n",
    "* Keep in mind that the quality of Llama3 and Mixtral is still below the quality of OpenAI's ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a6bc5-c149-4ef3-a6a7-71fbd72e098a",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 002-trying-different-llm-models.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba604a0-5ce9-42b1-92d8-ca8e27e7232b",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d072f-291b-42f3-81ee-bf2c779b2870",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **002-trying-different-llm-models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5961599-9738-454f-81ad-bb0784f180b4",
   "metadata": {},
   "source": [
    "## Track operations\n",
    "From now on, we can track the operations **and the cost** of this project from LangSmith:\n",
    "* [smith.langchain.com](https://smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398411fd-07b6-4882-9625-e87ec3751da2",
   "metadata": {},
   "source": [
    "## Intro to Groq\n",
    "* Groq is an AI Startup company. **It is not the same as Grok, the LLM from Elon Musk**.\n",
    "* It has developed a new chip call LPU (Language Processing Unit) which is specificly design to run LLMs faster and cheaper.\n",
    "* It offers a Groq Cloud where you can try Open Source LLMs like Llama3 or Mixtral.\n",
    "* **It allows you to use Llama3 or Mixtral in your apps for free using a Groq API Key with some Rate Limits**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a1309-a5f5-4797-aab2-e90d77c2dde5",
   "metadata": {},
   "source": [
    "## How to get a free Groq API Key\n",
    "* Login into Groq Cloud: [https://console.groq.com/login](https://console.groq.com/login)\n",
    "* Once logged in, click on API Keys (left sidebar).\n",
    "* Create a new API Key.\n",
    "* Copy the API Key and paste it in your .env file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4c154f-c329-4148-a57d-ed18fcd05d32",
   "metadata": {},
   "source": [
    "## How to install Groq in your project\n",
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you.\n",
    "\n",
    "LangChain has a module for it. We can install it the same way we install other LangChain modules, using PIP or (if we are working in a Poetry app) we can also install it using Poetry. Use one of the following options:\n",
    "* pip install langchain-groq\n",
    "* poetry add langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819fad70-2d71-4e4a-b8a6-20285d7eb44f",
   "metadata": {},
   "source": [
    "## How to use Groq in your LangChain or CrewAI project\n",
    "Very easy. Just add the following line at the top of your file:\n",
    "* from langchain_groq import ChatGroq\n",
    "\n",
    "And then, in the code, if you want to use Llama3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce029dd-1e8b-4b82-98f6-bf9bfd09ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGroq(\n",
    "#     model=\"llama3-70b-8192\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3f9434-fb9f-4960-af68-55af2d86d847",
   "metadata": {},
   "source": [
    "Or if you want to use Mixtral:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2232591-6853-4840-b9d4-4b338a8d81d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatGroq(\n",
    "#     model=\"mixtral-8x7b-32768\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583dcd83-520b-4dc2-a160-1578314a847e",
   "metadata": {},
   "source": [
    "## You can take a look at Groq Rate limits here\n",
    "* https://console.groq.com/settings/limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4043b0-6f4e-4834-ac5c-8f3c1427c89d",
   "metadata": {},
   "source": [
    "## Groq pricing for projects in Production\n",
    "* [Groq pricing](https://wow.groq.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e299ed0-391d-4578-aee6-2537ce294252",
   "metadata": {},
   "source": [
    "## Let's give it a try!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f6a62-115f-4d26-8809-2e604bddd82e",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93ed82f-d41b-4608-83d1-1053573905ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0548e514-cf7a-4f53-8df4-b6c3d03dc696",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv, find_dotenv\n\u001b[32m      3\u001b[39m _ = load_dotenv(find_dotenv())\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e7309f-3080-4bd0-af0f-374cc642b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llamaChatModel = ChatGroq(\n",
    "    model=\"llama3-70b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8527fb7a-69a1-44ab-9601-e3e259df642a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatGroq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m mistralChatModel = \u001b[43mChatGroq\u001b[49m(\n\u001b[32m      2\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mmixtral-8x7b-32768\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'ChatGroq' is not defined"
     ]
    }
   ],
   "source": [
    "mistralChatModel = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6968a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistralChatModel = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f41aad1b-4aba-4bf4-a06d-2925a68b4828",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are an historian expert in the Kennedy family.\"),\n",
    "    (\"human\", \"How many members of the family died tragically?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df753975-ebd0-49d0-bd01-56bee372eb47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llamaChatModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llamaResponse = \u001b[43mllamaChatModel\u001b[49m.invoke(messages)\n",
      "\u001b[31mNameError\u001b[39m: name 'llamaChatModel' is not defined"
     ]
    }
   ],
   "source": [
    "llamaResponse = llamaChatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "850e5967-ea3a-434a-8aed-406f29ed89aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llamaResponse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllamaResponse\u001b[49m.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'llamaResponse' is not defined"
     ]
    }
   ],
   "source": [
    "print(llamaResponse.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ce3b58-09d0-431b-95d2-ce9c2fbb5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistralResponse = mistralChatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f5dc68-4e8d-4006-9354-5d290901b77f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mistralResponse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmistralResponse\u001b[49m.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'mistralResponse' is not defined"
     ]
    }
   ],
   "source": [
    "print(mistralResponse.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c70e41d-5a7a-4831-a1be-978ccaceda07",
   "metadata": {},
   "source": [
    "## How to execute the code from Visual Studio Code\n",
    "* In Visual Studio Code, see the file 002-trying-different-llm-models.py\n",
    "* In terminal, make sure you are in the directory of the file and run:\n",
    "    * python 002-trying-different-llm-models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47d77fd-c1ee-4e13-9219-7b5d16069f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
