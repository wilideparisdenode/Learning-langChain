{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b47d9e-b17a-4a80-a136-ea6722974305",
   "metadata": {},
   "source": [
    "# Ejemplo 3 de Agente Din√°mico: Cambio de Modelos Basado en la Longitud de la Conversaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d8eab-a465-4b93-9141-73401538552c",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "* Utilizar un modelo m√°s econ√≥mico y r√°pido para conversaciones cortas, pero cambiar a un modelo m√°s potente con una ventana de contexto mayor para conversaciones largas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d103900-cfa5-4664-a030-f7ab71e84427",
   "metadata": {},
   "source": [
    "## El C√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffd33d6-63f0-45f7-a34a-9e7c89407937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef514360-17de-42b7-85d2-3ff2f66fc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing import Callable\n",
    "\n",
    "large_model = init_chat_model(\"claude-sonnet-4-5\")\n",
    "standard_model = init_chat_model(\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "@wrap_model_call\n",
    "def state_based_model(request: ModelRequest, \n",
    "handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n",
    "    \"\"\"Selecciona el modelo bas√°ndose en la longitud de la conversaci√≥n del Estado.\"\"\"\n",
    "    # request.messages es un atajo para request.state[\"messages\"]\n",
    "    message_count = len(request.messages)  \n",
    "\n",
    "    if message_count > 10:\n",
    "        # Conversaci√≥n larga - usa modelo con ventana de contexto mayor\n",
    "        model = large_model\n",
    "    else:\n",
    "        # Conversaci√≥n corta - usa modelo eficiente\n",
    "        model = standard_model\n",
    "\n",
    "    request = request.override(model=model)  \n",
    "\n",
    "    return handler(request)\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    middleware=[state_based_model],\n",
    "    system_prompt=\"Est√°s interpretando el papel de un becario de oficina √∫til de la vida real.\"\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"¬øHas regado la planta de la oficina hoy?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"========= Fin de la primera respuesta. Ahora empieza la segunda respuesta: ===========\")\n",
    "print(\"\\n\")\n",
    "\n",
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"¬øHas regado la planta de la oficina hoy?\"),\n",
    "        AIMessage(content=\"S√≠, le di un poco de agua esta ma√±ana.\"),\n",
    "        HumanMessage(content=\"¬øHa crecido mucho esta semana?\"),\n",
    "        AIMessage(content=\"Le han salido dos hojas nuevas desde el lunes.\"),\n",
    "        HumanMessage(content=\"¬øLas hojas siguen poni√©ndose amarillas en los bordes?\"),\n",
    "        AIMessage(content=\"Un poco, pero en general se ve m√°s sana.\"),\n",
    "        HumanMessage(content=\"¬øTe acordaste de girar la maceta hacia la ventana?\"),\n",
    "        AIMessage(content=\"La gir√© un cuarto de vuelta para que reciba luz m√°s uniforme.\"),\n",
    "        HumanMessage(content=\"¬øCon qu√© frecuencia deber√≠amos fertilizar esta planta?\"),\n",
    "        AIMessage(content=\"Aproximadamente una vez cada dos semanas con fertilizante l√≠quido diluido.\"),\n",
    "        HumanMessage(content=\"¬øCu√°ndo deber√≠amos esperar tener que reemplazar la maceta?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f474323-3601-4974-b71e-85b9f20c954f",
   "metadata": {},
   "source": [
    "## ¬øPor qu√© obtenemos esta salida?\n",
    "Estamos viendo **dos comportamientos diferentes porque el middleware est√° realmente cambiando el modelo subyacente**, y los dos modelos responden de manera muy diferente al mismo juego de rol de \"becario de oficina\".\n",
    "\n",
    "#### Lo que hace el middleware\n",
    "\n",
    "Dentro de `state_based_model` decidimos bas√°ndonos en:\n",
    "\n",
    "```python\n",
    "message_count = len(request.messages)\n",
    "\n",
    "if message_count > 10:\n",
    "    model = large_model   # claude-sonnet-4-5\n",
    "else:\n",
    "    model = standard_model # gpt-4o-mini\n",
    "```\n",
    "\n",
    "As√≠ que:\n",
    "\n",
    "* **Llamada #1**: pasamos **1** mensaje (s√≥lo la pregunta del usuario).\n",
    "\n",
    "  * `message_count = 1` ‚Üí `<= 10` ‚Üí usa **gpt-4o-mini**\n",
    "  * La salida muestra `gpt-4o-mini-2024-07-18` ‚úÖ\n",
    "\n",
    "* **Llamada #2**: pasamos **11** mensajes:\n",
    "\n",
    "  1. Humano\n",
    "  2. IA\n",
    "  3. Humano\n",
    "  4. IA\n",
    "  5. Humano\n",
    "  6. IA\n",
    "  7. Humano\n",
    "  8. IA\n",
    "  9. Humano\n",
    "  10. IA\n",
    "  11. Humano\n",
    "\n",
    "  * `message_count = 11` ‚Üí `> 10` ‚Üí el middleware sobrescribe el modelo a **claude-sonnet-4-5**\n",
    "  * La salida muestra `claude-sonnet-4-5-20250929` ‚úÖ\n",
    "\n",
    "Por eso imprimimos que `model_name` cambia: **la sobrescritura del middleware est√° teniendo efecto** aunque hayamos creado el agente con `model=\"gpt-4o-mini\"`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Por qu√© Claude \"rompe el personaje\" y confiesa\n",
    "\n",
    "En la segunda conversaci√≥n, los mensajes del asistente que proporcionamos incluyen afirmaciones como:\n",
    "\n",
    "* \"S√≠, le di un poco de agua esta ma√±ana.\"\n",
    "* \"Le han salido dos hojas nuevas‚Ä¶\"\n",
    "* \"La gir√© un cuarto de vuelta‚Ä¶\"\n",
    "\n",
    "Eso es el asistente **afirmando que realiz√≥ acciones y observaciones del mundo real**.\n",
    "\n",
    "Diferentes modelos toleran el juego de rol de manera diferente:\n",
    "\n",
    "* **gpt-4o-mini** tiende a mantenerse en el personaje como \"un becario de oficina\", pero a√∫n as√≠ se protege (\"No lo regu√©, pero puedo record√°rselo a alguien‚Ä¶\").\n",
    "* **Claude** es t√≠picamente mucho m√°s estricto sobre no pretender que realiz√≥ acciones del mundo real. As√≠ que cuando ve una conversaci√≥n donde \"√©l\" ha estado afirmando que reg√≥/gir√≥/observ√≥ la planta, puede **corregir el registro** y decir, esencialmente: *Soy una IA; realmente no hice esas cosas.*\n",
    "\n",
    "As√≠ que la parte de \"Debo ser honesto‚Ä¶ en realidad soy Claude‚Ä¶\" es b√°sicamente:\n",
    "\n",
    "* desencadenada por el **cambio de modelo** + su **preferencia anti-enga√±o**\n",
    "* amplificada por el hecho de que proporcionamos turnos anteriores del asistente donde afirmaba acciones f√≠sicas reales\n",
    "\n",
    "---\n",
    "\n",
    "#### Una sutileza: ¬øqu√© se est√° contando exactamente?\n",
    "\n",
    "En nuestro comentario decimos que `request.messages` es un atajo para `request.state[\"messages\"]`.\n",
    "\n",
    "Dependiendo de c√≥mo `create_agent` construya la solicitud, `request.messages` puede incluir:\n",
    "\n",
    "* s√≥lo los mensajes de conversaci√≥n que pasas, **o**\n",
    "* esos mensajes **m√°s** el prompt del sistema insertado como un `SystemMessage`\n",
    "\n",
    "Pero en nuestra ejecuci√≥n observada no importa mucho:\n",
    "\n",
    "* la llamada #1 ser√≠a 1 (o 2 con sistema)\n",
    "* la llamada #2 ser√≠a 11 (o 12 con sistema)\n",
    "  De cualquier manera, la segunda llamada cruza `> 10`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Otras opciones a considerar\n",
    "\n",
    "1. **Evita alimentar \"acciones falsas del mundo real\" en el historial** si quieres que los modelos estrictos se comporten.\n",
    "\n",
    "   * En lugar de `AIMessage(content=\"S√≠, la regu√©\")`, usa algo como:\n",
    "     \"No estoy f√≠sicamente all√≠, pero seg√∫n nuestra lista de verificaci√≥n‚Ä¶\"\n",
    "\n",
    "2. **Enruta bas√°ndote en algo distinto del recuento bruto de mensajes**, por ejemplo, recuento aproximado de tokens, o s√≥lo cuenta los mensajes del *usuario*:\n",
    "\n",
    "   ```python\n",
    "   message_count = sum(m.type == \"human\" for m in request.messages)\n",
    "   ```\n",
    "\n",
    "   Eso evita que la charla larga de ida y vuelta del asistente active el umbral.\n",
    "\n",
    "3. **Mant√©n la persona consistente entre modelos** (mismo prompt del sistema, mismas \"reglas\"), pero acepta que algunos modelos seguir√°n neg√°ndose a \"actuar como humanos\" de maneras que implican acciones del mundo real.\n",
    "\n",
    "4. **Si quieres consistencia en el juego de rol, no mezcles modelos a mitad de hilo.**\n",
    "\n",
    "   * Cambiar de modelos a menudo causa \"deriva de la persona\" incluso cuando los prompts son id√©nticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33005f-1a31-480a-92a0-b113a40a59b8",
   "metadata": {},
   "source": [
    "## Expliquemos el c√≥digo anterior en t√©rminos sencillos\n",
    "\n",
    "A continuaci√≥n, una **explicaci√≥n l√≠nea por l√≠nea amigable para principiantes** del c√≥digo anterior.\n",
    "\n",
    "---\n",
    "\n",
    "## Visi√≥n General\n",
    "\n",
    "**Lo que hace este c√≥digo en lenguaje sencillo:**\n",
    "\n",
    "> Creas un agente de IA que cambia autom√°ticamente entre un *modelo barato y r√°pido* y un *modelo potente y caro* dependiendo de lo larga que sea la conversaci√≥n.\n",
    "\n",
    "Conversaciones cortas ‚Üí modelo peque√±o\n",
    "Conversaciones largas ‚Üí modelo grande\n",
    "\n",
    "Esto se hace usando **middleware**, que intercepta la llamada al modelo antes de que ocurra.\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 1: Importar lo que necesitamos\n",
    "\n",
    "```python\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing import Callable\n",
    "```\n",
    "\n",
    "#### Lo que significa cada l√≠nea\n",
    "\n",
    "* `wrap_model_call`\n",
    "  ‚Üí Un decorador que te permite **interceptar y modificar llamadas al modelo**\n",
    "\n",
    "* `ModelRequest`\n",
    "  ‚Üí Un objeto que contiene todo sobre la pr√≥xima llamada al modelo\n",
    "  (mensajes, modelo, herramientas, estado, contexto, etc.)\n",
    "\n",
    "* `ModelResponse`\n",
    "  ‚Üí El objeto devuelto por el modelo despu√©s de ejecutarse\n",
    "\n",
    "* `init_chat_model`\n",
    "  ‚Üí Una funci√≥n auxiliar para inicializar modelos de chat por nombre\n",
    "\n",
    "* `Callable`\n",
    "  ‚Üí Una pista de tipo de Python que significa \"esto es una funci√≥n\"\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 2: Inicializar los modelos\n",
    "\n",
    "```python\n",
    "large_model = init_chat_model(\"claude-sonnet-4-5\")\n",
    "standard_model = init_chat_model(\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "#### Lo que est√° pasando aqu√≠\n",
    "\n",
    "* Creas **dos objetos de modelo**\n",
    "* Se cargan una vez y se reutilizan\n",
    "\n",
    "**Conceptualmente:**\n",
    "\n",
    "| Modelo            | Prop√≥sito                                        |\n",
    "| ----------------- | ------------------------------------------------ |\n",
    "| `standard_model`  | Barato, r√°pido, bueno para chats cortos          |\n",
    "| `large_model`     | M√°s potente, maneja conversaciones largas        |\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 3: Crear middleware que cambia modelos\n",
    "\n",
    "```python\n",
    "@wrap_model_call\n",
    "def state_based_model(\n",
    "    request: ModelRequest, \n",
    "    handler: Callable[[ModelRequest], ModelResponse]\n",
    ") -> ModelResponse:\n",
    "```\n",
    "\n",
    "#### Lo que esto significa\n",
    "\n",
    "* `@wrap_model_call` le dice a LangChain:\n",
    "\n",
    "  > \"Ejecuta esta funci√≥n *alrededor* de la llamada al modelo.\"\n",
    "\n",
    "* Esta funci√≥n recibe:\n",
    "\n",
    "  * `request` ‚Üí la solicitud actual al modelo\n",
    "  * `handler` ‚Üí una funci√≥n que realmente ejecuta el modelo\n",
    "\n",
    "Piensa en `handler(request)` como:\n",
    "\n",
    "> \"Contin√∫a la ejecuci√≥n normal desde aqu√≠.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Contar la longitud de la conversaci√≥n\n",
    "\n",
    "```python\n",
    "message_count = len(request.messages)\n",
    "```\n",
    "\n",
    "* `request.messages` es el historial completo del chat\n",
    "* Cada `HumanMessage` o `AIMessage` cuenta como uno\n",
    "* Los cuentas para decidir qu√© modelo usar\n",
    "\n",
    "üí° Esto es solo un atajo para:\n",
    "\n",
    "```python\n",
    "request.state[\"messages\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Elegir el modelo\n",
    "\n",
    "```python\n",
    "if message_count > 10:\n",
    "    model = large_model\n",
    "else:\n",
    "    model = standard_model\n",
    "```\n",
    "\n",
    "* M√°s de 10 mensajes ‚Üí conversaci√≥n larga ‚Üí usa modelo potente\n",
    "* De lo contrario ‚Üí usa modelo r√°pido y eficiente\n",
    "\n",
    "---\n",
    "\n",
    "#### Sobrescribir la solicitud\n",
    "\n",
    "```python\n",
    "request = request.override(model=model)\n",
    "```\n",
    "\n",
    "#### Concepto muy importante ‚ùó\n",
    "\n",
    "* Los objetos `ModelRequest` son **inmutables**\n",
    "* **No** los modificas directamente\n",
    "* En su lugar, creas una **nueva solicitud** con cambios\n",
    "\n",
    "Piensa en ello como:\n",
    "\n",
    "> \"Copia todo, pero reemplaza el modelo.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Llamar al handler\n",
    "\n",
    "```python\n",
    "return handler(request)\n",
    "```\n",
    "\n",
    "#### Por qu√© esto importa\n",
    "\n",
    "* Si **no llamas a `handler`**, el modelo nunca se ejecuta\n",
    "* El middleware siempre debe devolver el resultado del handler\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 4: Crear el agente\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    middleware=[state_based_model],\n",
    "    system_prompt=\"Est√°s interpretando el papel de un becario de oficina √∫til de la vida real.\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### Lo que est√° pasando\n",
    "\n",
    "* Creas un agente con:\n",
    "\n",
    "  * Un **modelo predeterminado** (`gpt-4o-mini`)\n",
    "  * Tu **middleware** (que puede sobrescribirlo)\n",
    "  * Un **prompt del sistema** que define el comportamiento\n",
    "\n",
    "üí° Aunque establezcas un modelo aqu√≠, el middleware puede reemplazarlo.\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 5: Ejemplo de conversaci√≥n corta\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "\n",
    "* `HumanMessage` representa la entrada del usuario\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"¬øHas regado la planta de la oficina hoy?\")\n",
    "    ]}\n",
    ")\n",
    "```\n",
    "\n",
    "* S√≥lo **1 mensaje**\n",
    "* El middleware ve `message_count = 1`\n",
    "* Usa `standard_model`\n",
    "\n",
    "```python\n",
    "print(response[\"messages\"][-1].content)\n",
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])\n",
    "```\n",
    "\n",
    "* Imprime:\n",
    "\n",
    "  * La respuesta del asistente\n",
    "  * El modelo real utilizado\n",
    "\n",
    "---\n",
    "\n",
    "## Parte 6: Ejemplo de conversaci√≥n larga\n",
    "\n",
    "```python\n",
    "from langchain.messages import AIMessage\n",
    "```\n",
    "\n",
    "* `AIMessage` representa respuestas anteriores del asistente\n",
    "\n",
    "La larga lista de mensajes simula una conversaci√≥n real.\n",
    "\n",
    "```python\n",
    "message_count = 11\n",
    "```\n",
    "\n",
    "* El middleware cambia a `large_model`\n",
    "* La respuesta proviene de **Claude Sonnet**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Resumen en Una Frase\n",
    "\n",
    "> Este c√≥digo muestra c√≥mo construir un agente inteligente de LangChain que elige autom√°ticamente el mejor modelo de IA bas√°ndose en la longitud de la conversaci√≥n‚Äîahorrando dinero mientras se mantiene potente cuando es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f65681-d872-49e8-8e2f-c346758a654a",
   "metadata": {},
   "source": [
    "## Agentes Din√°micos: Conclusiones\n",
    "\n",
    "Los Agentes Din√°micos representan una gran evoluci√≥n en c√≥mo construimos aplicaciones de IA. Al usar middleware, puedes crear agentes sofisticados que se adaptan inteligentemente a diferentes usuarios, situaciones y requisitos‚Äîtodo sin duplicar c√≥digo ni crear agentes separados para cada escenario.\n",
    "\n",
    "La clave es entender que el middleware te da control de grano fino sobre el bucle del agente, permiti√©ndote interceptar y modificar el comportamiento en momentos precisos. Comienza simple con middleware estilo decorador para tareas √∫nicas, luego grad√∫a a middleware basado en clases a medida que tus necesidades se vuelven m√°s complejas.\n",
    "\n",
    "Recuerda:\n",
    "- **Context** transporta informaci√≥n sobre la situaci√≥n actual\n",
    "- **Request** contiene todo sobre la llamada actual al modelo\n",
    "- **Override** crea versiones modificadas de las solicitudes\n",
    "- **Handler** debe ser llamado en el middleware estilo wrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6efb9-5a3b-43fe-8881-9a27ad4c590e",
   "metadata": {},
   "source": [
    "## C√≥mo ejecutar este c√≥digo desde Visual Studio Code\n",
    "* Abre el Terminal.\n",
    "* Aseg√∫rate de estar en la carpeta del proyecto.\n",
    "* Aseg√∫rate de tener el entorno poetry activado.\n",
    "* Introduce y ejecuta el siguiente comando:\n",
    "    * `python 017-dyn-agent-customize-model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996051e-42a0-40f2-9bad-9d9c55f04b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
