{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc4c108-1255-4ac2-9b31-cd20c7a5de02",
   "metadata": {},
   "source": [
    "# MCP in LangChain 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761e744b-cdd4-48a0-b1f1-663d49dd0606",
   "metadata": {},
   "source": [
    "## What is MCP?\n",
    "* Model Context Protocol (MCP) is an open protocol that standardizes how we can connect external tools to our LLM applications and Agents (as you know, Agents are just a type of LLM application)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbd3bf4-d6c6-4616-9e8d-4d3946b781d8",
   "metadata": {},
   "source": [
    "## How can Agents use MCP in LangChain 1.0?\n",
    "* LangChain agents can use tools defined on MCP servers using the [langchain-mcp-adapters](https://github.com/langchain-ai/langchain-mcp-adapters) library.\n",
    "* You can find available MCP tools in many sources like [this website](https://mcp.so/server/time/modelcontextprotocol).\n",
    "\n",
    "#### How to use external tools with MCP: the basics\n",
    "* First, we need to install the langchain-mcp-adapters package:\n",
    "`pip install langchain-mcp-adapters`\n",
    "\n",
    "* Then, we will use `MultiServerMCPClient`, a module that for each tool invocation creates a fresh MCP session, executes the tool, and then cleans up:\n",
    "\n",
    "```python\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient  \n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "client = MultiServerMCPClient(  \n",
    "    {\n",
    "        \"math\": {\n",
    "            \"transport\": \"stdio\",  # Local subprocess communication\n",
    "            \"command\": \"python\",\n",
    "            # Absolute path to your math_server.py file\n",
    "            \"args\": [\"/path/to/math_server.py\"],\n",
    "        },\n",
    "        \"weather\": {\n",
    "            \"transport\": \"http\",  # HTTP-based remote server\n",
    "            # Ensure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()  \n",
    "\n",
    "agent = create_agent(\n",
    "    \"gpt-4o-mini\",\n",
    "    tools  \n",
    ")\n",
    "\n",
    "math_response = await agent.ainvoke(\n",
    "    {\"messages\": [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"what's (3 + 5) x 12?\"}]}\n",
    ")\n",
    "\n",
    "weather_response = await agent.ainvoke(\n",
    "    {\"messages\": [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"what is the weather in nyc?\"}]}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75504bbd-6d4b-41f5-81b6-3320b3e6368d",
   "metadata": {},
   "source": [
    "## Let's see this in a basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccf1b30-d296-46e3-99fb-f5a4bb33c4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f764c1cb-e921-41c1-ab2f-10ece6ca9d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current time in Madrid is 06:23 AM on Sunday, January 4, 2026.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.messages import HumanMessage\n",
    "from pprint import pprint\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            # the next lines are pasted from the website of the MCP provider\n",
    "            # see https://mcp.so/server/time/modelcontextprotocol\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "question = HumanMessage(content=\"What time is it in Madrid?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0a3ed-4fdd-43f2-8563-19b5b76f3857",
   "metadata": {},
   "source": [
    "## OK. Let's explain the previous code in simple terms\n",
    "\n",
    "#### Imports (lines 1-3)\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "**What it does:** Imports a function that allows you to create an AI agent.\n",
    "**In simple terms:** An agent is like an intelligent assistant that can use tools to solve problems. This function is the \"factory\" that builds that assistant.\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "**What it does:** Imports a class to create user messages.\n",
    "**In simple terms:** When you want to \"talk\" to the agent, you need to package your question in a special format. `HumanMessage` is that packaging.\n",
    "\n",
    "```python\n",
    "from pprint import pprint\n",
    "```\n",
    "**What it does:** Imports a function to print data in a nice way.\n",
    "**In simple terms:** Although it's not used in this code, `pprint` helps to see complex results in a more readable way. (Note: the code uses regular `print` at the end, not `pprint`).\n",
    "\n",
    "\n",
    "#### Connection to MCP server (lines 4-18)\n",
    "\n",
    "```python\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "```\n",
    "**What it does:** Imports a client to connect to MCP servers.\n",
    "**In simple terms:** MCP (Model Context Protocol) is a standard that allows your agent to use external tools. This client is the \"bridge\" that connects your agent with those tools.\n",
    "\n",
    "```python\n",
    "client = MultiServerMCPClient(\n",
    "```\n",
    "**What it does:** Creates an instance of the MCP client.\n",
    "**In simple terms:** You're creating your connection to the tool servers. \"Multi\" means you can connect to multiple servers at once.\n",
    "\n",
    "```python\n",
    "    {\n",
    "        \"time\": {\n",
    "```\n",
    "**What it does:** Defines a server called \"time\".\n",
    "**In simple terms:** You're telling the client: \"Connect to a server called 'time' that has tools related to dates and times\".\n",
    "\n",
    "```python\n",
    "            \"transport\": \"stdio\",\n",
    "```\n",
    "**What it does:** Defines how it communicates with the server.\n",
    "**In simple terms:** \"stdio\" means \"Standard Input/Output\". It's like saying \"I'm going to talk to this server using simple text messages through the terminal\".\n",
    "\n",
    "```python\n",
    "            \"command\": \"uvx\",\n",
    "```\n",
    "**What it does:** Specifies the command to execute the server.\n",
    "**In simple terms:** `uvx` is a Python tool that runs applications. It's like saying \"use uvx to start the server\".\n",
    "\n",
    "```python\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "```\n",
    "**What it does:** Provides the arguments (parameters) for the command.\n",
    "**In simple terms:** It tells `uvx` to execute `mcp-server-time` (the time server) configured for the New York timezone. It's like saying: \"run this program with this specific configuration\".\n",
    "\n",
    "\n",
    "#### Getting tools (line 19)\n",
    "\n",
    "```python\n",
    "tools = await client.get_tools()\n",
    "```\n",
    "**What it does:** Gets all available tools from the MCP server.\n",
    "**In simple terms:** The client asks the server: \"What tools do you have available?\" and saves them in the `tools` variable. The `await` means that the program waits to receive the response before continuing (because it's an asynchronous operation).\n",
    "\n",
    "\n",
    "#### Agent creation (lines 20-23)\n",
    "\n",
    "```python\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=tools,\n",
    ")\n",
    "```\n",
    "**What it does:** Creates the agent with an AI model and the MCP tools.\n",
    "**In simple terms:** Here you build your intelligent assistant. You tell it:\n",
    "- \"Use the GPT-4o-mini brain\" (OpenAI's AI model)\n",
    "- \"These are the tools you can use\" (the ones you got from the MCP server)\n",
    "\n",
    "The agent now knows how to think (GPT-4o-mini) and what tools it has available.\n",
    "\n",
    "\n",
    "#### Question and execution (lines 24-28)\n",
    "\n",
    "```python\n",
    "question = HumanMessage(content=\"What time is it in Madrid?\")\n",
    "```\n",
    "**What it does:** Creates a message with the user's question.\n",
    "**In simple terms:** You package your question \"What time is it in Madrid?\" in the format that the agent understands.\n",
    "\n",
    "```python\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "```\n",
    "**What it does:** Sends the question to the agent and waits for its response.\n",
    "**In simple terms:** You tell the agent: \"Here's my question, solve it\". The agent:\n",
    "1. Reads the question\n",
    "2. Decides if it needs to use any tool\n",
    "3. Uses the MCP server to get the time\n",
    "4. Returns the answer to you\n",
    "\n",
    "The `await` means you wait for this entire process to finish.\n",
    "\n",
    "\n",
    "#### Display result (line 29)\n",
    "\n",
    "```python\n",
    "print(response['messages'][-1].content)\n",
    "```\n",
    "**What it does:** Prints the last message of the conversation.\n",
    "**In simple terms:** \n",
    "- `response['messages']` is a list with all the messages of the conversation\n",
    "- `[-1]` takes the last message (the agent's final response)\n",
    "- `.content` extracts the text from that message\n",
    "- `print()` displays it on screen\n",
    "\n",
    "\n",
    "#### General summary\n",
    "\n",
    "This code does the following in order:\n",
    "\n",
    "1. **Sets up the connection** to an MCP server that has time/date tools\n",
    "2. **Gets the available tools** from that server\n",
    "3. **Creates an intelligent agent** that knows how to use those tools\n",
    "4. **Asks a question**: \"What time is it in Madrid?\"\n",
    "5. **The agent reasons**: \"I need to use the time tool to answer this\"\n",
    "6. **Shows the answer** that the agent obtained\n",
    "\n",
    "It's a perfect example of how LangChain agents can use external tools (MCP) to answer questions that require real-time information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e300f2-ad00-4a15-9c1d-c9b0573daec7",
   "metadata": {},
   "source": [
    "#### If you use pprint to see the detailed response, you will see that we are indeed using the external tool via MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42005ebc-06c5-4d7d-a740-464c54d45514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it in Madrid?', additional_kwargs={}, response_metadata={}, id='441c9705-4670-462d-84d3-4d2fb9fdf593'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 217, 'total_tokens': 234, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-CuAs19asAid3hUvFpBqicOZx2a8u6', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b8775-96b3-77a2-839d-2f5ae0c89689-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'Europe/Madrid'}, 'id': 'call_xbOgqvy1doAZxhw4izo4jUWx', 'type': 'tool_call'}], usage_metadata={'input_tokens': 217, 'output_tokens': 17, 'total_tokens': 234, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"Europe/Madrid\",\\n  \"datetime\": \"2026-01-04T06:23:10+01:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_b2fefabf-4d16-4d80-aa90-9dcdad90c191'}], name='get_current_time', id='ffff1999-4737-40c4-9a16-fc4fad7d150d', tool_call_id='call_xbOgqvy1doAZxhw4izo4jUWx'),\n",
      "              AIMessage(content='The current time in Madrid is 06:23 AM on Sunday, January 4, 2026.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 292, 'total_tokens': 315, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-CuAs2o5L6hKFsQ6YM5cRqVQ8iEF13', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8775-9b92-7142-ac25-e5222ea2c4a4-0', usage_metadata={'input_tokens': 292, 'output_tokens': 23, 'total_tokens': 315, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed9dd24-b3e7-45bb-bf94-8e8974c4bdbf",
   "metadata": {},
   "source": [
    "## How to run this code from Visual Studio Code\n",
    "* Open Terminal.\n",
    "* Make sure you are in the project folder.\n",
    "* Make sure you have the poetry env activated.\n",
    "* Enter and run the following command:\n",
    "    * `python 008-mcp.py`\n",
    " \n",
    "#### Important note about a small change in the .py file\n",
    "\n",
    "Jupyter notebooks handle async code differently than regular Python scripts.\n",
    "\n",
    "**In Jupyter notebooks:**\n",
    "- Jupyter (specifically IPython) has a built-in event loop that's always running\n",
    "- This allows you to use `await` directly at the top level of cells without wrapping it in an `async def` function\n",
    "- Jupyter automatically handles the async execution for you\n",
    "\n",
    "**In regular Python scripts (terminal):**\n",
    "- There's no event loop running by default\n",
    "- You must explicitly create an event loop using `asyncio.run()`\n",
    "- All `await` statements must be inside `async def` functions\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# ✅ Works in Jupyter notebook\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# ✅ Works in terminal/script\n",
    "async def main():\n",
    "    tools = await client.get_tools()\n",
    "    \n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "This is one of the convenient features of Jupyter for working with async code - it makes experimentation easier. But when you move that code to a `.py` file to run from the terminal, you need to add the async function wrapper and `asyncio.run()`.\n",
    "\n",
    "This is a common \"gotcha\" when transitioning code from Jupyter notebooks to Python scripts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400a2715-e132-4343-9b2f-d86eb2342db3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
