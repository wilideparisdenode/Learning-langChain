{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010c4bc0-8242-44c3-8744-ba2c448c08e0",
   "metadata": {},
   "source": [
    "# Ejemplo 1 de Agente Dinámico: Responder en el Idioma del Usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219c138-0891-43fd-b20c-5bcf65119559",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "* Hacer que el agente responda en español cuando habla con hispanohablantes, y en inglés para los anglohablantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed0304-6b16-42fa-b33a-1440fdd493e6",
   "metadata": {},
   "source": [
    "## El código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174698fd-6553-4668-a05f-27793b838251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259fb30c-0bbf-4d5a-83e7-fefdf5d21e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dataclass\n",
    "class LanguageContext:\n",
    "    user_language: str = \"English\"\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_language_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_language = request.runtime.context.user_language\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if user_language != \"English\":\n",
    "        return f\"{base_prompt} only respond in {user_language}.\"\n",
    "    elif user_language == \"English\":\n",
    "        return base_prompt\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    context_schema=LanguageContext,\n",
    "    middleware=[user_language_prompt]\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"message\": [HumanMessage(content=\"Hola, ¿cómo estás?\")]},\n",
    "    context=LanguageContext(user_language=\"Spanish\")\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fdf6c8-80de-42d7-8658-0a0d17890251",
   "metadata": {},
   "source": [
    "## Expliquemos el código anterior en términos sencillos\n",
    "\n",
    "A continuación encontrarás una **explicación línea por línea para principiantes** del código.\n",
    "\n",
    "---\n",
    "\n",
    "## Qué hace este código (visión general)\n",
    "\n",
    "Este código crea un **agente dinámico de LangChain** que:\n",
    "\n",
    "* Conoce el idioma preferido del usuario (inglés, español, etc.)\n",
    "* **Cambia automáticamente el prompt del sistema**\n",
    "* Fuerza a la IA a **responder únicamente en ese idioma**\n",
    "\n",
    "Piénsalo como:\n",
    "\n",
    "> \"Antes de que la IA responda, comprueba en qué idioma quiere hablar el usuario y ajusta las instrucciones en consecuencia.\"\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Importaciones y configuración\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "```\n",
    "\n",
    "#### ¿Qué está pasando aquí?\n",
    "\n",
    "* `dataclass` (de Python) se utiliza para definir **contenedores de datos simples**\n",
    "* `dynamic_prompt` es un **decorador de LangChain** que marca una función como algo que puede **modificar prompts en tiempo de ejecución**\n",
    "* `ModelRequest` es un objeto que contiene **todo sobre la petición actual**, incluyendo:\n",
    "\n",
    "  * el modelo que se está utilizando\n",
    "  * mensajes\n",
    "  * contexto de ejecución (como la configuración del usuario)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Definir el contexto (estado compartido)\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class LanguageContext:\n",
    "    user_language: str = \"English\"\n",
    "```\n",
    "\n",
    "#### ¿Qué está pasando aquí?\n",
    "\n",
    "* Esto define un **objeto de contexto** que viaja con cada petición\n",
    "* Almacena **información adicional** que NO forma parte del mensaje del usuario\n",
    "* En este caso, almacenamos:\n",
    "\n",
    "  * `user_language`: el idioma que prefiere el usuario\n",
    "\n",
    "#### Por qué esto es importante\n",
    "\n",
    "En lugar de adivinar el idioma del mensaje, nosotros:\n",
    "\n",
    "* Le decimos explícitamente al agente qué idioma utilizar\n",
    "* Mantenemos esta lógica limpia y predecible\n",
    "\n",
    "Piensa en `LanguageContext` como:\n",
    "\n",
    "> \"Configuración adicional sobre el usuario que la IA debería conocer.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Crear un prompt de sistema dinámico\n",
    "\n",
    "```python\n",
    "@dynamic_prompt\n",
    "def user_language_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user language.\"\"\"\n",
    "```\n",
    "\n",
    "#### ¿Qué está pasando?\n",
    "\n",
    "* `@dynamic_prompt` le dice a LangChain:\n",
    "\n",
    "  > \"Esta función generará un **prompt de sistema** de forma dinámica para cada petición.\"\n",
    "* LangChain llamará automáticamente a esta función **antes** de que se ejecute el modelo\n",
    "\n",
    "---\n",
    "\n",
    "#### Acceder al contexto\n",
    "\n",
    "```python\n",
    "    user_language = request.runtime.context.user_language\n",
    "```\n",
    "\n",
    "* `request` es el objeto de petición completo\n",
    "* `request.runtime.context` contiene el objeto de contexto que pasamos\n",
    "* `user_language` se lee de `LanguageContext`\n",
    "\n",
    "En español sencillo:\n",
    "\n",
    "> \"Busca el idioma preferido del usuario.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Instrucción base del sistema\n",
    "\n",
    "```python\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "```\n",
    "\n",
    "* Esta es la instrucción por defecto del sistema\n",
    "* Todo lo demás se construye sobre esto\n",
    "\n",
    "---\n",
    "\n",
    "#### Lógica condicional\n",
    "\n",
    "```python\n",
    "    if user_language != \"English\":\n",
    "        return f\"{base_prompt} only respond in {user_language}.\"\n",
    "    elif user_language == \"English\":\n",
    "        return base_prompt\n",
    "```\n",
    "\n",
    "#### ¿Qué está pasando?\n",
    "\n",
    "* Si el idioma del usuario **no es inglés**:\n",
    "\n",
    "  * Añadimos una instrucción estricta diciéndole al modelo que responda únicamente en ese idioma\n",
    "* Si el idioma **es inglés**:\n",
    "\n",
    "  * Mantenemos el prompt simple\n",
    "\n",
    "Ejemplos de salida:\n",
    "\n",
    "* Español →\n",
    "  `\"You are a helpful assistant. only respond in Spanish.\"`\n",
    "* Inglés →\n",
    "  `\"You are a helpful assistant.\"`\n",
    "\n",
    "Esta cadena devuelta se convierte en el **prompt del sistema** enviado al modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Crear el agente\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "\n",
    "Esto importa la función auxiliar de LangChain para construir agentes.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    context_schema=LanguageContext,\n",
    "    middleware=[user_language_prompt]\n",
    ")\n",
    "```\n",
    "\n",
    "#### ¿Qué está pasando aquí?\n",
    "\n",
    "* `model=\"gpt-4o-mini\"`\n",
    "  → Selecciona el LLM a utilizar\n",
    "\n",
    "* `context_schema=LanguageContext`\n",
    "  → Le dice a LangChain:\n",
    "\n",
    "  > \"Cada petición puede incluir un objeto `LanguageContext`\"\n",
    "\n",
    "* `middleware=[user_language_prompt]`\n",
    "  → Registra nuestra función de prompt dinámico\n",
    "  → LangChain la ejecutará **antes de cada llamada al modelo**\n",
    "\n",
    "Piensa en el middleware como:\n",
    "\n",
    "> \"Código que se ejecuta *entre* la entrada del usuario y la respuesta de la IA.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Enviar un mensaje al agente\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "\n",
    "* `HumanMessage` representa un mensaje del usuario en el formato de mensajes de LangChain\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "    {\"message\": [HumanMessage(content=\"Hola, ¿cómo estás?\")]},\n",
    "    context=LanguageContext(user_language=\"Spanish\")\n",
    ")\n",
    "```\n",
    "\n",
    "#### ¿Qué está pasando paso a paso?\n",
    "\n",
    "1. El usuario envía un mensaje en español\n",
    "2. Pasamos explícitamente:\n",
    "\n",
    "   ```python\n",
    "   context=LanguageContext(user_language=\"Spanish\")\n",
    "   ```\n",
    "3. LangChain:\n",
    "\n",
    "   * Almacena este contexto en `request.runtime.context`\n",
    "   * Llama a `user_language_prompt`\n",
    "   * Construye el prompt del sistema:\n",
    "\n",
    "     ```\n",
    "     You are a helpful assistant. only respond in Spanish.\n",
    "     ```\n",
    "4. El modelo recibe:\n",
    "\n",
    "   * Prompt del sistema (del middleware)\n",
    "   * Mensaje del usuario\n",
    "5. El modelo responde en español\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Imprimir la respuesta final\n",
    "\n",
    "```python\n",
    "print(response[\"messages\"][-1].content)\n",
    "```\n",
    "\n",
    "#### ¿Qué está pasando?\n",
    "\n",
    "* `response[\"messages\"]` es una lista de todos los mensajes (sistema, usuario, asistente)\n",
    "* `[-1]` obtiene el **último mensaje**, que es la respuesta de la IA\n",
    "* `.content` extrae el texto\n",
    "\n",
    "---\n",
    "\n",
    "## Modelo mental final (¡importante!)\n",
    "\n",
    "Piensa en este flujo:\n",
    "\n",
    "```\n",
    "Mensaje del usuario\n",
    "   ↓\n",
    "Contexto (LanguageContext)\n",
    "   ↓\n",
    "Middleware de prompt dinámico\n",
    "   ↓\n",
    "Se genera el prompt del sistema\n",
    "   ↓\n",
    "Se llama al LLM\n",
    "   ↓\n",
    "Respuesta\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Por qué este patrón es poderoso\n",
    "\n",
    "* ✅ Separación clara de responsabilidades\n",
    "* ✅ Sin trucos de prompt en los mensajes del usuario\n",
    "* ✅ Comportamiento basado en contexto\n",
    "* ✅ Fácil de extender (tono, rol, permisos, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698d38e-b33c-4d3a-8058-0dd01316c98a",
   "metadata": {},
   "source": [
    "## Cómo ejecutar este código desde Visual Studio Code\n",
    "* Abre el Terminal.\n",
    "* Asegúrate de estar en la carpeta del proyecto.\n",
    "* Asegúrate de tener activado el entorno de poetry.\n",
    "* Introduce y ejecuta el siguiente comando:\n",
    "    * `python 015-dyn-agent-customize-language.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7926f-7d52-49b0-bcae-9f2c4533e104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
