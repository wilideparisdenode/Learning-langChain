{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978a668c-bca8-4934-9519-91fdc0134600",
   "metadata": {},
   "source": [
    "# Short-Term Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa413d00-ec55-4466-ac48-3317c2db37f8",
   "metadata": {},
   "source": [
    "## By default, an Agent has no memory of our conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a01bfbf-1284-4082-bc09-be4e72a67239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1e78190-4ac7-4765-8231-f33d023aa98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Julio! Vespas are great! They have a classic design and are fun to ride. Do you have a favorite model or are you looking to get one?\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hello, my name is Julio and I like vespas.\")]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "347a0aca-1ee5-4d03-af74-9de969a4ae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to personal data about you unless you share it with me during our conversation. I also cannot know your favorite scooter. If you'd like to tell me your name or your favorite scooter, I'd be happy to remember that for the duration of our chat!\n"
     ]
    }
   ],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name? What is my favorite scooter?\")]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d44ef-1da8-4597-9514-7f8329f4ca20",
   "metadata": {},
   "source": [
    "#### If we use `pprint` to see what is in the response variable, you will see that there is no record of our previous question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00feb3df-82b8-4627-bce6-29c74c676933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is my name? What is my favorite scooter?', additional_kwargs={}, response_metadata={}, id='aba8b6d7-92c6-4d68-86ca-6f3814a1e33a'),\n",
      "              AIMessage(content=\"I'm sorry, but I don't have access to personal data about you unless you share it with me during our conversation. I also cannot know your favorite scooter. If you'd like to tell me your name or your favorite scooter, I'd be happy to remember that for the duration of our chat!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 28, 'total_tokens': 85, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuA6EJT8PJSk417YFjC8s0ycRt14v', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8748-5f07-7a51-9711-3f6c92415351-0', usage_metadata={'input_tokens': 28, 'output_tokens': 57, 'total_tokens': 85, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a974b7f-d9f3-4a0c-9ec7-68c0d24e480d",
   "metadata": {},
   "source": [
    "## What is short-term memory (aka State)?\n",
    "* Short-term memory is limited to the current conversation you are having with an LLM App.\n",
    "* The short-term memory of an Agent in LangChain is currently referred to as State."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ba9d8-bcb4-425a-981c-c80ce9fe29b1",
   "metadata": {},
   "source": [
    "## How to add short-term memory to an Agent in LangChain 1.0\n",
    "* To add short-term memory to an agent, you need to specify a `checkpointer` when creating an agent and we need to associate the conversation with a conversation ID (aka thread ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb8e6f1e-487b-4f3f-a393-b00ead31b550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Julio! That's great to hear! Vespas are classic scooters with a lot of charm. Do you have a favorite model or a particular reason you like them?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# This is where we add the short-term memory ability to our agent\n",
    "agent2 = create_agent(\n",
    "    \"gpt-4o-mini\",\n",
    "    checkpointer=InMemorySaver(),  \n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=\"Hello my name is Julio and I like vespas.\")\n",
    "\n",
    "# This is where we set the conversation ID\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# This is where we associate our conversation with the conversation ID\n",
    "response = agent2.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e863fc-8445-41b9-930a-859fc7ccfcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Julio, and you mentioned that you like Vespas, but you didn't specify a favorite model. Do you have a specific model in mind that you like the most?\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What is my name? What is my favorite scooter?\")\n",
    "\n",
    "response = agent2.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d3c8f-5281-4f84-b04e-20084634ec62",
   "metadata": {},
   "source": [
    "#### Now, if we use `pprint` to see what is in the response variable, we can see that the whole conversation is being recorded. Meaning: our Agent remembers our conversation.\n",
    "* Remember: the short-memory will only last until we finish our conversation. If we close our app and open it again tomorrow, our app will have no memory of the conversation we had the previous day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1834d2a-dddb-4af8-b03e-b6c4e003ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Hello my name is Julio and I like vespas.', additional_kwargs={}, response_metadata={}, id='5af36357-28b4-4c8e-ae06-00784bcaa8e4'),\n",
      "              AIMessage(content=\"Hi Julio! That's great to hear! Vespas are classic scooters with a lot of charm. Do you have a favorite model or a particular reason you like them?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 19, 'total_tokens': 53, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuA6FC58FWdnF0X9KGAGR58RVB3sd', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8748-65d9-72b0-9136-26a1da8a6402-0', usage_metadata={'input_tokens': 19, 'output_tokens': 34, 'total_tokens': 53, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              HumanMessage(content='What is my name? What is my favorite scooter?', additional_kwargs={}, response_metadata={}, id='5ff238a0-cf13-4d26-ab01-56d646699356'),\n",
      "              AIMessage(content=\"Your name is Julio, and you mentioned that you like Vespas, but you didn't specify a favorite model. Do you have a specific model in mind that you like the most?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 72, 'total_tokens': 109, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuA6G5XE4N6ZVNhxYnsv5Ewurfr2m', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8748-68b8-7473-b390-afbc62e7b060-0', usage_metadata={'input_tokens': 72, 'output_tokens': 37, 'total_tokens': 109, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df1146-3ecc-4b3d-b6fb-aee02b1de257",
   "metadata": {},
   "source": [
    "#### Note: InMemorySaver vs. MemorySaver\n",
    "* Note that the checkpointer we used in the previous exercise was `InMemorySaver.` As you can see in the import statement, this is a functionality we take from LangGraph.\n",
    "* Both `MemorySaver` and `InMemorySaver` exist and work in LangGraph. Recent documentation and community examples show both names being used interchangeably, with `MemorySaver` being more commonly used in recent Python examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32979303-3ec0-4aa6-a001-8a7e84d858cb",
   "metadata": {},
   "source": [
    "#### Instead of InMemorySaver, in production we will use we a checkpointer backed by a database\n",
    "* We will have to install a package like the postgres package:\n",
    "`pip install langgraph-checkpoint-postgres`\n",
    "\n",
    "* This way we can use a checkpointer backed by a database:\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from langgraph.checkpoint.postgres import PostgresSaver  \n",
    "\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    checkpointer.setup() # auto create tables in PostgresSql\n",
    "    agent = create_agent(\n",
    "        \"gpt-5\",\n",
    "        tools=[get_user_info],\n",
    "        checkpointer=checkpointer,  \n",
    "    )\n",
    "```\n",
    "\n",
    "* Remember that this is still short-term memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5812a1-f577-4d29-ad44-e3f62cc80e9d",
   "metadata": {},
   "source": [
    "## Customizing the Short-Memory Format\n",
    "* LangChain agents use `AgentState` to manage short term memory.\n",
    "* By default, `AgentState` saves the conversation history in the `messages` field.\n",
    "* If we need it, we can add additional fields to `AgentState`. See how we do it below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f96aa82-9dc5-4525-be48-adbb4ef54c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's great! San Francisco is a vibrant city known for its iconic landmarks like the Golden Gate Bridge, Alcatraz Island, and its picturesque neighborhoods such as Chinatown and Haight-Ashbury. The city's diverse culture, culinary scene, and beautiful views make it a popular destination. What do you love most about San Francisco?\n"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import AgentState\n",
    "\n",
    "# This is where we set the format (aka schema) of our custom shor-term memory (aka State)\n",
    "# As of LangChain 1.0, custom state schemas must be TypedDict types. \n",
    "# Pydantic models and dataclasses are no longer supported.\n",
    "class CustomAgentState(AgentState):  \n",
    "    user_id: str\n",
    "    user_preferences: dict\n",
    "\n",
    "# This is where we add the short-term memory ability to our agent\n",
    "agent3 = create_agent(\n",
    "    \"gpt-4o-mini\",\n",
    "    state_schema=CustomAgentState, \n",
    "    checkpointer=InMemorySaver(),  \n",
    ")\n",
    "\n",
    "response = agent3.invoke(\n",
    "    {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"My favorite city is San Francisco.\"}],\n",
    "        \"user_id\": \"user_123\",  # This is just for demo, we are not using this\n",
    "        \"user_preferences\": {\"converation_style\": \"Casual\"}  # This is just for demo, we are not using this\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fc71db4-7874-43d6-a08a-713c4af0de1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='My favorite city is San Francisco.', additional_kwargs={}, response_metadata={}, id='8bdabe5a-701b-4351-84ef-ae540edbbc2e'),\n",
      "              AIMessage(content=\"That's great! San Francisco is a vibrant city known for its iconic landmarks like the Golden Gate Bridge, Alcatraz Island, and its picturesque neighborhoods such as Chinatown and Haight-Ashbury. The city's diverse culture, culinary scene, and beautiful views make it a popular destination. What do you love most about San Francisco?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 14, 'total_tokens': 79, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuA6IAIHVZtGfJFgVQzZl6aAIJHaz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8748-6f32-74d0-bbb5-058fe465eeb8-0', usage_metadata={'input_tokens': 14, 'output_tokens': 65, 'total_tokens': 79, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
      " 'user_id': 'user_123',\n",
      " 'user_preferences': {'converation_style': 'Casual'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e59f6e7-38fa-4e4c-a7fe-ebc59c7c5287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you mentioned that your favorite city is San Francisco! As for your preferred conversation style, I don't have specific information about that yet. If you let me know what style you prefer—formal, casual, concise, detailed, etc.—I'd be happy to adjust my responses accordingly!\n"
     ]
    }
   ],
   "source": [
    "response = agent3.invoke(\n",
    "    {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Do you know my favorite city? And my preferred conversation style?\"}],\n",
    "        \"user_id\": \"user_123\",  \n",
    "        \"user_preferences\": {\"converation_style\": \"Casual\"}  \n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b364ea5-e21e-40c3-9b1b-95a8a9fde50e",
   "metadata": {},
   "source": [
    "#### Using pprint, see how the short-term memory is being processed\n",
    "* As you can see, the conversation memory and the user data are being processed separately. That is why if we ask the Agent about our preferred conversation style he answers that he does not have this data in the conversation memory.\n",
    "* This data is, indeed, in the short-term memory, but to access it we need to use a different approach as you will see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a6e9832-0f29-4436-8d04-22658dcdd682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='My favorite city is San Francisco.', additional_kwargs={}, response_metadata={}, id='8bdabe5a-701b-4351-84ef-ae540edbbc2e'),\n",
      "              AIMessage(content=\"That's great! San Francisco is a vibrant city known for its iconic landmarks like the Golden Gate Bridge, Alcatraz Island, and its picturesque neighborhoods such as Chinatown and Haight-Ashbury. The city's diverse culture, culinary scene, and beautiful views make it a popular destination. What do you love most about San Francisco?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 14, 'total_tokens': 79, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuA6IAIHVZtGfJFgVQzZl6aAIJHaz', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8748-6f32-74d0-bbb5-058fe465eeb8-0', usage_metadata={'input_tokens': 14, 'output_tokens': 65, 'total_tokens': 79, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              HumanMessage(content='Do you know my favorite city? And my preferred conversation style?', additional_kwargs={}, response_metadata={}, id='3623e2cc-2324-4a78-bcd4-924947b123b1'),\n",
      "              AIMessage(content=\"Yes, you mentioned that your favorite city is San Francisco! As for your preferred conversation style, I don't have specific information about that yet. If you let me know what style you prefer—formal, casual, concise, detailed, etc.—I'd be happy to adjust my responses accordingly!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 57, 'prompt_tokens': 100, 'total_tokens': 157, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuA6J3kcT7IJsV3em84G4RHiICdMB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b8748-7504-7392-a780-65f7e82c4158-0', usage_metadata={'input_tokens': 100, 'output_tokens': 57, 'total_tokens': 157, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
      " 'user_id': 'user_123',\n",
      " 'user_preferences': {'converation_style': 'Casual'}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5998ae-a330-4bbb-be2f-2d82aa9617ed",
   "metadata": {},
   "source": [
    "#### See below how we access to the user preferences data stored in the short-term memory of our agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947cf2d5-e26b-4225-9407-7d9871ef819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'converation_style': 'Casual'}\n"
     ]
    }
   ],
   "source": [
    "print(response['user_preferences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a91c44-da30-455c-bfb0-1e82b53b0e48",
   "metadata": {},
   "source": [
    "## What if the conversation is longer than the context window?\n",
    "* In production apps, long conversations can exceed the LLM’s context window. Common solutions to this problem are:\n",
    "    * Summarize messages.\n",
    "    * Trim messages.\n",
    "    * Delete messages.\n",
    "    * Other custom strategies.\n",
    "\n",
    "* We will see how to implement some of this solutions in future lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94734cbc-5ed6-4eeb-b0ec-45718d1aa520",
   "metadata": {},
   "source": [
    "## Long-Term Memory\n",
    "* For long-term memory (aka Store) we will use a different approach, also inspired by LangGraph. As the LangChain Team states in the LangChain documentation, this is an advanced topic that requires knowledge of LangGraph to use, therefore you will be much better equiped to master it after you complete our \"2026 Bootcamp: Understand and Build Professional AI Agents\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a960588-36b3-439a-8e1f-cfec5d85b894",
   "metadata": {},
   "source": [
    "## How to run this code from Visual Studio Code\n",
    "* Open Terminal.\n",
    "* Make sure you are in the project folder.\n",
    "* Make sure you have the poetry env activated.\n",
    "* Enter and run the following command:\n",
    "    * `python 007-short-term-memory.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6d6b5-899b-4049-95a7-0df073a4caba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
