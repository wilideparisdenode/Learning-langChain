{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759a720e-cdd5-4282-b3d2-49b168e7ae10",
   "metadata": {},
   "source": [
    "# Uso del Middleware Integrado para Resumir una Conversación Larga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ba124-1780-4b49-9cde-93a07d254d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c49363-f610-4bff-a966-304023427487",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    checkpointer=InMemorySaver(),\n",
    "    middleware=[\n",
    "        SummarizationMiddleware(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            trigger=(\"tokens\", 100),\n",
    "            keep=(\"messages\", 1)\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage, AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Are you ready to play the JFK QA game?\"),\n",
    "        AIMessage(content=\"Sure!\"),\n",
    "        HumanMessage(content=\"Who was the favorite sister or JFK?\"),\n",
    "        AIMessage(content=\"Her sister Kick.\"),\n",
    "        HumanMessage(content=\"Correct! Who was his favorite brother?\"),\n",
    "        AIMessage(content=\"Hmmm, that is difficult. I would say Ted. He loved Bobby very much, but Bobby was very different from him.\"),\n",
    "        HumanMessage(content=\"Correct! What was the main source of pain of JFK on a daily basis?\"),\n",
    "        AIMessage(content=\"Back pain.\"),\n",
    "        HumanMessage(content=\"Correct again! Did JFK have dogs?\"),\n",
    "        ]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238794cd-d053-4d67-9888-66dbfbbbc6b4",
   "metadata": {},
   "source": [
    "#### Vale. Usemos pprint para ver la respuesta detallada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cabf5-f9e2-4eba-9025-523c8af1d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c6ed3-1808-48d1-9714-886cf1976a12",
   "metadata": {},
   "source": [
    "#### Como podéis ver, la respuesta incluye un resumen de la conversación hasta la fecha. Vamos a imprimirlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73baef13-0be0-4684-b956-59a6621e42e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d204ca-e8b7-468e-bf76-e116537afddf",
   "metadata": {},
   "source": [
    "## Vale. Ahora expliquemos el código anterior en términos sencillos\n",
    "\n",
    "A continuación se muestra el mismo código, explicado **en términos simples, línea por línea**, además de una explicación clara de **qué hace `SummarizationMiddleware`**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Qué está haciendo este programa (visión general)\n",
    "\n",
    "Estás creando un **agente** de chat (un bucle de asistente inteligente) que:\n",
    "\n",
    "1. Usa un LLM (`gpt-4o-mini`)\n",
    "2. **Recuerda la conversación** usando un \"checkpointer\" (almacenamiento de memoria)\n",
    "3. Usa **SummarizationMiddleware** para *resumir automáticamente el historial de chat antiguo* cuando se vuelve demasiado largo\n",
    "4. Ejecuta el agente con una lista de mensajes de chat e imprime la última respuesta del agente\n",
    "\n",
    "Resumen del middleware: es una forma de \"interceptar/controlar\" lo que sucede dentro del bucle del agente.\n",
    "\n",
    "---\n",
    "\n",
    "#### Importaciones\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "\n",
    "* Importa `create_agent`, un helper que construye un agente por ti (un agente = modelo + herramientas + memoria + comportamiento del bucle).\n",
    "\n",
    "```python\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "```\n",
    "\n",
    "* Importa un **checkpointer en memoria**.\n",
    "* Un *checkpointer* almacena el estado del agente para que el agente pueda reanudar una conversación más tarde (por hilo). Para demostraciones/prototipos rápidos, LangChain recomienda `InMemorySaver`.\n",
    "\n",
    "```python\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "```\n",
    "\n",
    "* Importa el middleware que **resumirá automáticamente el historial de conversación** cuando se alcance algún umbral.\n",
    "\n",
    "---\n",
    "\n",
    "#### Crear el agente\n",
    "\n",
    "```python\n",
    "agent = create_agent(\n",
    "```\n",
    "\n",
    "* Comienza a construir un objeto agente.\n",
    "\n",
    "```python\n",
    "    model=\"gpt-4o-mini\",\n",
    "```\n",
    "\n",
    "* Establece el modelo principal que el agente usará para responder.\n",
    "\n",
    "```python\n",
    "    checkpointer=InMemorySaver(),\n",
    "```\n",
    "\n",
    "* Añade \"persistencia de memoria a corto plazo\" usando un almacén en memoria.\n",
    "* Esto permite que el agente mantenga un historial de conversación **por id de hilo**, para que no se mezclen múltiples conversaciones.\n",
    "\n",
    "```python\n",
    "    middleware=[\n",
    "```\n",
    "\n",
    "* Añade componentes middleware (piensa: \"plugins\" que se ejecutan en ciertos puntos dentro del bucle del agente).\n",
    "\n",
    "```python\n",
    "        SummarizationMiddleware(\n",
    "```\n",
    "\n",
    "* Activa la resumición automática de mensajes antiguos.\n",
    "\n",
    "```python\n",
    "            model=\"gpt-4o-mini\",\n",
    "```\n",
    "\n",
    "* El modelo utilizado **para escribir el resumen**.\n",
    "* (Puedes usar el mismo o un modelo más barato/rápido que el principal.)\n",
    "\n",
    "```python\n",
    "            trigger=(\"tokens\", 100),\n",
    "```\n",
    "\n",
    "* **Cuándo resumir.**\n",
    "* `(\"tokens\", 100)` significa: *si el contexto de la conversación está a punto de superar ~100 tokens (según el contador de tokens), resume el contenido antiguo.*\n",
    "* **Nota importante para principiantes:** 100 tokens es *muy poco* (como unos pocos mensajes cortos), por lo que esto resumirá de forma muy agresiva.\n",
    "\n",
    "```python\n",
    "            keep=(\"messages\", 1)\n",
    "```\n",
    "\n",
    "* **Cuánto chat reciente mantener \"tal cual\"** después de resumir.\n",
    "* `(\"messages\", 1)` significa: mantener solo el **1 mensaje** más reciente sin cambios; el contenido más antiguo se comprime en un resumen.\n",
    "\n",
    "```python\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "```\n",
    "\n",
    "* Termina de construir el agente.\n",
    "\n",
    "---\n",
    "\n",
    "#### Crear objetos de mensaje\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage, AIMessage\n",
    "```\n",
    "\n",
    "* Importa tipos de mensajes.\n",
    "* `HumanMessage` = texto del usuario, `AIMessage` = texto del asistente.\n",
    "\n",
    "---\n",
    "\n",
    "#### Invocar (ejecutar) el agente con una conversación\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "```\n",
    "\n",
    "* Ejecuta el agente una vez y devuelve un objeto de respuesta (que incluye mensajes/actualizaciones de estado).\n",
    "\n",
    "```python\n",
    "    {\"messages\": [\n",
    "```\n",
    "\n",
    "* La entrada es un diccionario con una lista de `messages`.\n",
    "* Estás dando al agente una conversación \"hasta ahora\".\n",
    "\n",
    "```python\n",
    "        HumanMessage(content=\"Are you ready to play the JFK QA game?\"),\n",
    "        AIMessage(content=\"Sure!\"),\n",
    "        HumanMessage(content=\"Who was the favorite sister or JFK?\"),\n",
    "        AIMessage(content=\"Her sister Kick.\"),\n",
    "        HumanMessage(content=\"Correct! Who was his favorite brother?\"),\n",
    "        AIMessage(content=\"Hmmm, that is difficult. I would say Ted. He loved Bobby very much, but Bobby was very different from him.\"),\n",
    "        HumanMessage(content=\"Correct! What was the main source of pain of JFK on a daily basis?\"),\n",
    "        AIMessage(content=\"Back pain.\"),\n",
    "        HumanMessage(content=\"Correct again! Did JFK have dogs?\"),\n",
    "```\n",
    "\n",
    "* Esta es una conversación de ida y vuelta.\n",
    "* El último mensaje es una pregunta del usuario: \"Did JFK have dogs?\"\n",
    "* El agente debería responder a esa última pregunta.\n",
    "\n",
    "```python\n",
    "        ]},\n",
    "```\n",
    "\n",
    "* Termina la lista de mensajes y el diccionario de entrada.\n",
    "\n",
    "```python\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "```\n",
    "\n",
    "* Esta es la **configuración** para la ejecución.\n",
    "* `thread_id=\"1\"` le dice al checkpointer: \"almacena/recupera memoria para el hilo de conversación #1\".\n",
    "* Así es como se mantienen sesiones de chat separadas.\n",
    "\n",
    "```python\n",
    ")\n",
    "```\n",
    "\n",
    "* Termina la llamada al agente.\n",
    "\n",
    "---\n",
    "\n",
    "#### Imprimir la respuesta final del agente\n",
    "\n",
    "```python\n",
    "print(response[\"messages\"][-1].content)\n",
    "```\n",
    "\n",
    "* `response[\"messages\"]` es la lista de mensajes actualizada después de que el agente respondiera.\n",
    "* `[-1]` significa \"el último mensaje\".\n",
    "* `.content` obtiene el texto.\n",
    "* Así que esto imprime la respuesta más reciente del agente.\n",
    "\n",
    "---\n",
    "\n",
    "#### Qué hace `SummarizationMiddleware` (explicación simple)\n",
    "\n",
    "`SummarizationMiddleware` es un **compresor automático de historial de chat**:\n",
    "\n",
    "* **Monitoriza cuán grande es tu historial de mensajes** (por tokens, número de mensajes o fracción de contexto).\n",
    "* Cuando se alcanza el **umbral de disparo** (en tu código: 100 tokens):\n",
    "\n",
    "  1. Toma la parte *más antigua* de la conversación\n",
    "  2. Llama a un modelo (en tu código: `gpt-4o-mini`) para **resumir** esa parte antigua\n",
    "  3. Reemplaza esa parte antigua con un **resumen corto**\n",
    "  4. Mantiene algunos mensajes más recientes intactos según `keep` (en tu código: mantener solo 1 mensaje reciente)\n",
    "\n",
    "Un detalle de la documentación de referencia: \"mantiene la continuidad del contexto asegurando que **los pares de mensajes AI/Tool permanezcan juntos**\" (para que no se rompa el significado de las interacciones con herramientas).\n",
    "\n",
    "#### En la configuración específica que usamos\n",
    "\n",
    "* `trigger=(\"tokens\", 100)` → resumir *muy rápidamente*\n",
    "* `keep=(\"messages\", 1)` → mantener casi nada textual, solo el/los último(s) mensaje(s)\n",
    "* Resultado: el agente a menudo verá algo como:\n",
    "\n",
    "  * **Resumen:** \"Estamos jugando un juego de preguntas y respuestas sobre JFK. El usuario preguntó X, el asistente respondió Y…\"\n",
    "  * **Mensaje más reciente:** \"Correct again! Did JFK have dogs?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d4ab53-8a2a-4514-aa6a-c1f8531f0b90",
   "metadata": {},
   "source": [
    "## Cómo ejecutar este código desde Visual Studio Code\n",
    "* Abre el Terminal.\n",
    "* Asegúrate de estar en la carpeta del proyecto.\n",
    "* Asegúrate de tener el entorno poetry activado.\n",
    "* Introduce y ejecuta el siguiente comando:\n",
    "    * `python 011-mid-to-summ-conversation.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38fea0-d316-4691-b82c-579302b9a4ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
