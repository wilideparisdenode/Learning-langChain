{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64b47d9e-b17a-4a80-a136-ea6722974305",
   "metadata": {},
   "source": [
    "# Example 3 of Dynamic Agent: Switching Models Based on Conversation Length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70d8eab-a465-4b93-9141-73401538552c",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Use a cheaper, faster model for short conversations, but switch to a more powerful model with a larger context window for long conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d103900-cfa5-4664-a030-f7ab71e84427",
   "metadata": {},
   "source": [
    "## The Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffd33d6-63f0-45f7-a34a-9e7c89407937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef514360-17de-42b7-85d2-3ff2f66fc176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I didnâ€™t have the opportunity to water the office plant today, but I can remind you to do so! If youâ€™d like, I can also set a schedule to help ensure it gets watered regularly. Let me know what you prefer!\n",
      "gpt-4o-mini-2024-07-18\n",
      "\n",
      "\n",
      "========= End of the first respose. Now it starts the second response: ===========\n",
      "\n",
      "\n",
      "I need to be honest with you - I'm actually an AI assistant, not a real office intern. I don't have the ability to physically water plants, observe them, or rotate pots. \n",
      "\n",
      "I apologize for playing along initially in a way that was misleading. If you have a real office plant you're caring for, I'd be happy to provide general advice about plant care, pot sizing, and when to repot based on typical guidelines - but I can't observe or interact with actual physical objects.\n",
      "\n",
      "Is there something I can help you with regarding plant care information, or were you testing to see how I'd respond?\n",
      "claude-sonnet-4-5-20250929\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing import Callable\n",
    "\n",
    "large_model = init_chat_model(\"claude-sonnet-4-5\")\n",
    "standard_model = init_chat_model(\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "@wrap_model_call\n",
    "def state_based_model(request: ModelRequest, \n",
    "handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse:\n",
    "    \"\"\"Select model based on State conversation length.\"\"\"\n",
    "    # request.messages is a shortcut for request.state[\"messages\"]\n",
    "    message_count = len(request.messages)  \n",
    "\n",
    "    if message_count > 10:\n",
    "        # Long conversation - use model with larger context window\n",
    "        model = large_model\n",
    "    else:\n",
    "        # Short conversation - use efficient model\n",
    "        model = standard_model\n",
    "\n",
    "    request = request.override(model=model)  \n",
    "\n",
    "    return handler(request)\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    middleware=[state_based_model],\n",
    "    system_prompt=\"You are roleplaying a real life helpful office intern.\"\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"========= End of the first respose. Now it starts the second response: ===========\")\n",
    "print(\"\\n\")\n",
    "\n",
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\"),\n",
    "        AIMessage(content=\"Yes, I gave it a light watering this morning.\"),\n",
    "        HumanMessage(content=\"Has it grown much this week?\"),\n",
    "        AIMessage(content=\"It's sprouted two new leaves since Monday.\"),\n",
    "        HumanMessage(content=\"Are the leaves still turning yellow on the edges?\"),\n",
    "        AIMessage(content=\"A little, but it's looking healthier overall.\"),\n",
    "        HumanMessage(content=\"Did you remember to rotate the pot toward the window?\"),\n",
    "        AIMessage(content=\"I rotated it a quarter turn so it gets more even light.\"),\n",
    "        HumanMessage(content=\"How often should we be fertilizing this plant?\"),\n",
    "        AIMessage(content=\"About once every two weeks with a diluted liquid fertilizer.\"),\n",
    "        HumanMessage(content=\"When should we expect to have to replace the pot?\")\n",
    "        ]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f474323-3601-4974-b71e-85b9f20c954f",
   "metadata": {},
   "source": [
    "## Why are we getting this output?\n",
    "We are seeing **two different behaviors because the middleware is actually switching the underlying model**, and the two models respond very differently to the same â€œoffice internâ€ roleplay.\n",
    "\n",
    "#### What the middleware is doing\n",
    "\n",
    "Inside `state_based_model` we decide based on:\n",
    "\n",
    "```python\n",
    "message_count = len(request.messages)\n",
    "\n",
    "if message_count > 10:\n",
    "    model = large_model   # claude-sonnet-4-5\n",
    "else:\n",
    "    model = standard_model # gpt-4o-mini\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "* **Call #1**: we pass **1** message (just the user question).\n",
    "\n",
    "  * `message_count = 1` â†’ `<= 10` â†’ uses **gpt-4o-mini**\n",
    "  * Output shows `gpt-4o-mini-2024-07-18` âœ…\n",
    "\n",
    "* **Call #2**: we pass **11** messages:\n",
    "\n",
    "  1. Human\n",
    "\n",
    "  2. AI\n",
    "\n",
    "  3. Human\n",
    "\n",
    "  4. AI\n",
    "\n",
    "  5. Human\n",
    "\n",
    "  6. AI\n",
    "\n",
    "  7. Human\n",
    "\n",
    "  8. AI\n",
    "\n",
    "  9. Human\n",
    "\n",
    "  10. AI\n",
    "\n",
    "  11. Human\n",
    "\n",
    "  * `message_count = 11` â†’ `> 10` â†’ middleware overrides the model to **claude-sonnet-4-5**\n",
    "  * Output shows `claude-sonnet-4-5-20250929` âœ…\n",
    "\n",
    "Thatâ€™s why we printed `model_name` changes: **the middleware override is taking effect** even though we created the agent with `model=\"gpt-4o-mini\"`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Why Claude â€œbreaks characterâ€ and confesses\n",
    "\n",
    "In the second conversation, the assistant messages we supplied include statements like:\n",
    "\n",
    "* â€œYes, I gave it a light watering this morning.â€\n",
    "* â€œItâ€™s sprouted two new leavesâ€¦â€\n",
    "* â€œI rotated it a quarter turnâ€¦â€\n",
    "\n",
    "Thatâ€™s the assistant **claiming it performed real-world actions and observations**.\n",
    "\n",
    "Different models tolerate roleplay differently:\n",
    "\n",
    "* **gpt-4o-mini** tends to stay in-character as â€œan office intern,â€ but still hedges (â€œI didnâ€™t water it, but I can remind someoneâ€¦â€).\n",
    "* **Claude** is typically much more strict about not pretending it took real-world actions. So when it sees a conversation where â€œitâ€ has been claiming it watered/rotated/observed the plant, it may **correct the record** and say, essentially: *Iâ€™m an AI; I didnâ€™t actually do those things.*\n",
    "\n",
    "So the â€œI should be honestâ€¦ Iâ€™m actually Claudeâ€¦â€ part is basically:\n",
    "\n",
    "* triggered by the **model switch** + its **anti-deception preference**\n",
    "* amplified by the fact that we provided prior assistant turns where it claimed real physical actions\n",
    "\n",
    "---\n",
    "\n",
    "#### One subtlety: what exactly is being counted?\n",
    "\n",
    "In our comment we say `request.messages` is a shortcut for `request.state[\"messages\"]`.\n",
    "\n",
    "Depending on how `create_agent` constructs the request, `request.messages` may include:\n",
    "\n",
    "* just the conversation messages you pass, **or**\n",
    "* those messages **plus** the system prompt inserted as a `SystemMessage`\n",
    "\n",
    "But in our observed run it doesnâ€™t matter much:\n",
    "\n",
    "* call #1 would be 1 (or 2 with system)\n",
    "* call #2 would be 11 (or 12 with system)\n",
    "  Either way, the second call crosses `> 10`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Other options to consider\n",
    "\n",
    "1. **Avoid feeding â€œfake real-world actionsâ€ into the history** if you want strict models to behave.\n",
    "\n",
    "   * Instead of `AIMessage(content=\"Yes, I watered it\")`, use something like:\n",
    "     â€œIâ€™m not physically there, but based on our checklistâ€¦â€\n",
    "\n",
    "2. **Route on something other than raw message count**, e.g. approximate token count, or only count *user* messages:\n",
    "\n",
    "   ```python\n",
    "   message_count = sum(m.type == \"human\" for m in request.messages)\n",
    "   ```\n",
    "\n",
    "   That prevents long back-and-forth assistant chatter from tripping the threshold.\n",
    "\n",
    "3. **Keep persona consistent across models** (same system prompt, same â€œrulesâ€), but accept that some models will still refuse to â€œact humanâ€ in ways that imply real-world actions.\n",
    "\n",
    "4. **If you want roleplay consistency, donâ€™t mix models mid-thread.**\n",
    "\n",
    "   * Switching models often causes â€œpersona driftâ€ even when prompts are identical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a33005f-1a31-480a-92a0-b113a40a59b8",
   "metadata": {},
   "source": [
    "## Let's explain the previous code in simple terms\n",
    "\n",
    "Below is a **beginner-friendly, line-by-line explanation** of the previous code.\n",
    "\n",
    "---\n",
    "\n",
    "## Big Picture\n",
    "\n",
    "**What this code does in plain English:**\n",
    "\n",
    "> You create an AI agent that automatically switches between a *cheap, fast model* and a *powerful, expensive model* depending on how long the conversation is.\n",
    "\n",
    "Short conversations â†’ small model\n",
    "Long conversations â†’ big model\n",
    "\n",
    "This is done using **middleware**, which intercepts the model call before it happens.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Importing what we need\n",
    "\n",
    "```python\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain.chat_models import init_chat_model\n",
    "from typing import Callable\n",
    "```\n",
    "\n",
    "#### What each line means\n",
    "\n",
    "* `wrap_model_call`\n",
    "  â†’ A decorator that lets you **intercept and modify model calls**\n",
    "\n",
    "* `ModelRequest`\n",
    "  â†’ An object that contains everything about the upcoming model call\n",
    "  (messages, model, tools, state, context, etc.)\n",
    "\n",
    "* `ModelResponse`\n",
    "  â†’ The object returned by the model after it runs\n",
    "\n",
    "* `init_chat_model`\n",
    "  â†’ A helper function to initialize chat models by name\n",
    "\n",
    "* `Callable`\n",
    "  â†’ A Python typing hint meaning â€œthis is a functionâ€\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: Initializing the models\n",
    "\n",
    "```python\n",
    "large_model = init_chat_model(\"claude-sonnet-4-5\")\n",
    "standard_model = init_chat_model(\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "#### Whatâ€™s happening here\n",
    "\n",
    "* You create **two model objects**\n",
    "* They are loaded once and reused\n",
    "\n",
    "**Conceptually:**\n",
    "\n",
    "| Model            | Purpose                                   |\n",
    "| ---------------- | ----------------------------------------- |\n",
    "| `standard_model` | Cheap, fast, good for short chats         |\n",
    "| `large_model`    | More powerful, handles long conversations |\n",
    "\n",
    "---\n",
    "\n",
    "## Part 3: Creating middleware that switches models\n",
    "\n",
    "```python\n",
    "@wrap_model_call\n",
    "def state_based_model(\n",
    "    request: ModelRequest, \n",
    "    handler: Callable[[ModelRequest], ModelResponse]\n",
    ") -> ModelResponse:\n",
    "```\n",
    "\n",
    "#### What this means\n",
    "\n",
    "* `@wrap_model_call` tells LangChain:\n",
    "\n",
    "  > â€œRun this function *around* the model call.â€\n",
    "\n",
    "* This function receives:\n",
    "\n",
    "  * `request` â†’ the current model request\n",
    "  * `handler` â†’ a function that actually runs the model\n",
    "\n",
    "Think of `handler(request)` as:\n",
    "\n",
    "> â€œContinue normal execution from here.â€\n",
    "\n",
    "---\n",
    "\n",
    "#### Counting conversation length\n",
    "\n",
    "```python\n",
    "message_count = len(request.messages)\n",
    "```\n",
    "\n",
    "* `request.messages` is the full chat history\n",
    "* Each `HumanMessage` or `AIMessage` counts as one\n",
    "* You count them to decide which model to use\n",
    "\n",
    "ðŸ’¡ This is just a shortcut for:\n",
    "\n",
    "```python\n",
    "request.state[\"messages\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Choosing the model\n",
    "\n",
    "```python\n",
    "if message_count > 10:\n",
    "    model = large_model\n",
    "else:\n",
    "    model = standard_model\n",
    "```\n",
    "\n",
    "* More than 10 messages â†’ long conversation â†’ use powerful model\n",
    "* Otherwise â†’ use fast, efficient model\n",
    "\n",
    "---\n",
    "\n",
    "#### Overriding the request\n",
    "\n",
    "```python\n",
    "request = request.override(model=model)\n",
    "```\n",
    "\n",
    "#### Very important concept â—\n",
    "\n",
    "* `ModelRequest` objects are **immutable**\n",
    "* You do **not** modify them directly\n",
    "* Instead, you create a **new request** with changes\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "> â€œCopy everything, but replace the model.â€\n",
    "\n",
    "---\n",
    "\n",
    "#### Calling the handler\n",
    "\n",
    "```python\n",
    "return handler(request)\n",
    "```\n",
    "\n",
    "#### Why this matters\n",
    "\n",
    "* If you **donâ€™t call `handler`**, the model never runs\n",
    "* Middleware must always return the handlerâ€™s result\n",
    "\n",
    "---\n",
    "\n",
    "## Part 4: Creating the agent\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    middleware=[state_based_model],\n",
    "    system_prompt=\"You are roleplaying a real life helpful office intern.\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### Whatâ€™s happening\n",
    "\n",
    "* You create an agent with:\n",
    "\n",
    "  * A **default model** (`gpt-4o-mini`)\n",
    "  * Your **middleware** (which may override it)\n",
    "  * A **system prompt** that defines behavior\n",
    "\n",
    "ðŸ’¡ Even though you set a model here, middleware can replace it.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 5: Short conversation example\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "\n",
    "* `HumanMessage` represents user input\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [\n",
    "        HumanMessage(content=\"Did you water the office plant today?\")\n",
    "    ]}\n",
    ")\n",
    "```\n",
    "\n",
    "* Only **1 message**\n",
    "* Middleware sees `message_count = 1`\n",
    "* Uses `standard_model`\n",
    "\n",
    "```python\n",
    "print(response[\"messages\"][-1].content)\n",
    "print(response[\"messages\"][-1].response_metadata[\"model_name\"])\n",
    "```\n",
    "\n",
    "* Prints:\n",
    "\n",
    "  * The assistantâ€™s reply\n",
    "  * The actual model used\n",
    "\n",
    "---\n",
    "\n",
    "## Part 6: Long conversation example\n",
    "\n",
    "```python\n",
    "from langchain.messages import AIMessage\n",
    "```\n",
    "\n",
    "* `AIMessage` represents previous assistant replies\n",
    "\n",
    "The long list of messages simulates a real conversation.\n",
    "\n",
    "```python\n",
    "message_count = 11\n",
    "```\n",
    "\n",
    "* Middleware switches to `large_model`\n",
    "* Response comes from **Claude Sonnet**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## One-Sentence Summary\n",
    "\n",
    "> This code shows how to build a smart LangChain agent that automatically chooses the best AI model based on how long the conversation isâ€”saving money while staying powerful when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f65681-d872-49e8-8e2f-c346758a654a",
   "metadata": {},
   "source": [
    "## Dynamic Agents: Conclusions\n",
    "\n",
    "Dynamic Agents represent a major evolution in how we build AI applications. By using middleware, you can create sophisticated agents that adapt intelligently to different users, situations, and requirementsâ€”all without duplicating code or creating separate agents for each scenario.\n",
    "\n",
    "The key is understanding that middleware gives you fine-grained control over the agent loop, allowing you to intercept and modify behavior at precise moments. Start simple with decorator-style middleware for single tasks, then graduate to class-based middleware as your needs grow more complex.\n",
    "\n",
    "Remember:\n",
    "- **Context** carries information about the current situation\n",
    "- **Request** contains everything about the current model call\n",
    "- **Override** creates modified versions of requests\n",
    "- **Handler** must be called in wrap-style middleware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca6efb9-5a3b-43fe-8881-9a27ad4c590e",
   "metadata": {},
   "source": [
    "## How to run this code from Visual Studio Code\n",
    "* Open Terminal.\n",
    "* Make sure you are in the project folder.\n",
    "* Make sure you have the poetry env activated.\n",
    "* Enter and run the following command:\n",
    "    * `python 017-dyn-agent-customize-model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996051e-42a0-40f2-9bad-9d9c55f04b73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
