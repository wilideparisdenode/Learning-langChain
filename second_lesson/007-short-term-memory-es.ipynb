{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978a668c-bca8-4934-9519-91fdc0134600",
   "metadata": {},
   "source": [
    "# Memoria a Corto Plazo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa413d00-ec55-4466-ac48-3317c2db37f8",
   "metadata": {},
   "source": [
    "## Por defecto, un Agente no tiene memoria de nuestra conversación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a01bfbf-1284-4082-bc09-be4e72a67239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e78190-4ac7-4765-8231-f33d023aa98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Hello, my name is Julio and I like vespas.\")]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a0aca-1ee5-4d03-af74-9de969a4ae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is my name? What is my favorite scooter?\")]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d44ef-1da8-4597-9514-7f8329f4ca20",
   "metadata": {},
   "source": [
    "#### Si usamos `pprint` para ver qué hay en la variable response, veremos que no hay registro de nuestra pregunta anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00feb3df-82b8-4627-bce6-29c74c676933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a974b7f-d9f3-4a0c-9ec7-68c0d24e480d",
   "metadata": {},
   "source": [
    "## ¿Qué es la memoria a corto plazo (también conocida como Estado)?\n",
    "* La memoria a corto plazo se limita a la conversación actual que estás manteniendo con una aplicación LLM.\n",
    "* La memoria a corto plazo de un Agente en LangChain se denomina actualmente Estado (State)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ba9d8-bcb4-425a-981c-c80ce9fe29b1",
   "metadata": {},
   "source": [
    "## Cómo añadir memoria a corto plazo a un Agente en LangChain 1.0\n",
    "* Para añadir memoria a corto plazo a un agente, necesitas especificar un `checkpointer` al crear un agente y necesitamos asociar la conversación con un ID de conversación (también conocido como thread ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e6f1e-487b-4f3f-a393-b00ead31b550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Aquí es donde añadimos la capacidad de memoria a corto plazo a nuestro agente\n",
    "agent2 = create_agent(\n",
    "    \"gpt-4o-mini\",\n",
    "    checkpointer=InMemorySaver(),  \n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=\"Hello my name is Julio and I like vespas.\")\n",
    "\n",
    "# Aquí es donde establecemos el ID de conversación\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# Aquí es donde asociamos nuestra conversación con el ID de conversación\n",
    "response = agent2.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e863fc-8445-41b9-930a-859fc7ccfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = HumanMessage(content=\"What is my name? What is my favorite scooter?\")\n",
    "\n",
    "response = agent2.invoke(\n",
    "    {\"messages\": [question]},\n",
    "    config,  \n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d3c8f-5281-4f84-b04e-20084634ec62",
   "metadata": {},
   "source": [
    "#### Ahora, si usamos `pprint` para ver qué hay en la variable response, podemos ver que se está grabando toda la conversación. Es decir: nuestro Agente recuerda nuestra conversación.\n",
    "* Recordad: la memoria a corto plazo solo durará hasta que finalicemos nuestra conversación. Si cerramos nuestra aplicación y la abrimos de nuevo mañana, nuestra aplicación no tendrá memoria de la conversación que mantuvimos el día anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1834d2a-dddb-4af8-b03e-b6c4e003ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df1146-3ecc-4b3d-b6fb-aee02b1de257",
   "metadata": {},
   "source": [
    "#### Nota: InMemorySaver vs. MemorySaver\n",
    "* Observad que el checkpointer que utilizamos en el ejercicio anterior fue `InMemorySaver`. Como podéis ver en la declaración de importación, esta es una funcionalidad que tomamos de LangGraph.\n",
    "* Tanto `MemorySaver` como `InMemorySaver` existen y funcionan en LangGraph. La documentación reciente y los ejemplos de la comunidad muestran que ambos nombres se utilizan indistintamente, siendo `MemorySaver` más utilizado en ejemplos recientes de Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32979303-3ec0-4aa6-a001-8a7e84d858cb",
   "metadata": {},
   "source": [
    "#### En lugar de InMemorySaver, en producción usaremos un checkpointer respaldado por una base de datos\n",
    "* Tendremos que instalar un paquete como el paquete postgres:\n",
    "`pip install langgraph-checkpoint-postgres`\n",
    "\n",
    "* De esta manera podemos usar un checkpointer respaldado por una base de datos:\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from langgraph.checkpoint.postgres import PostgresSaver  \n",
    "\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    checkpointer.setup() # crear automáticamente tablas en PostgresSql\n",
    "    agent = create_agent(\n",
    "        \"gpt-5\",\n",
    "        tools=[get_user_info],\n",
    "        checkpointer=checkpointer,  \n",
    "    )\n",
    "```\n",
    "\n",
    "* Recordad que esto sigue siendo memoria a corto plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5812a1-f577-4d29-ad44-e3f62cc80e9d",
   "metadata": {},
   "source": [
    "## Personalizando el formato de la memoria a corto plazo\n",
    "* Los agentes de LangChain utilizan `AgentState` para gestionar la memoria a corto plazo.\n",
    "* Por defecto, `AgentState` guarda el historial de conversación en el campo `messages`.\n",
    "* Si lo necesitamos, podemos añadir campos adicionales a `AgentState`. Ved cómo lo hacemos a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f96aa82-9dc5-4525-be48-adbb4ef54c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain.agents import AgentState\n",
    "\n",
    "# Aquí es donde establecemos el formato (también conocido como esquema) de nuestra memoria a corto plazo personalizada (también conocida como Estado)\n",
    "# A partir de LangChain 1.0, los esquemas de estado personalizados deben ser tipos TypedDict. \n",
    "# Los modelos Pydantic y las dataclasses ya no son compatibles.\n",
    "class CustomAgentState(AgentState):  \n",
    "    user_id: str\n",
    "    user_preferences: dict\n",
    "\n",
    "# Aquí es donde añadimos la capacidad de memoria a corto plazo a nuestro agente\n",
    "agent3 = create_agent(\n",
    "    \"gpt-4o-mini\",\n",
    "    state_schema=CustomAgentState, \n",
    "    checkpointer=InMemorySaver(),  \n",
    ")\n",
    "\n",
    "response = agent3.invoke(\n",
    "    {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"My favorite city is San Francisco.\"}],\n",
    "        \"user_id\": \"user_123\",  # Esto es solo para demostración, no lo estamos usando\n",
    "        \"user_preferences\": {\"converation_style\": \"Casual\"}  # Esto es solo para demostración, no lo estamos usando\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc71db4-7874-43d6-a08a-713c4af0de1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e59f6e7-38fa-4e4c-a7fe-ebc59c7c5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent3.invoke(\n",
    "    {\n",
    "        \"messages\": [{\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Do you know my favorite city? And my preferred conversation style?\"}],\n",
    "        \"user_id\": \"user_123\",  \n",
    "        \"user_preferences\": {\"converation_style\": \"Casual\"}  \n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b364ea5-e21e-40c3-9b1b-95a8a9fde50e",
   "metadata": {},
   "source": [
    "#### Usando pprint, ved cómo se está procesando la memoria a corto plazo\n",
    "* Como podéis ver, la memoria de conversación y los datos del usuario se están procesando por separado. Por eso, si preguntamos al Agente sobre nuestro estilo de conversación preferido, responde que no tiene estos datos en la memoria de conversación.\n",
    "* Estos datos están, de hecho, en la memoria a corto plazo, pero para acceder a ellos necesitamos usar un enfoque diferente como veréis a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e9832-0f29-4436-8d04-22658dcdd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5998ae-a330-4bbb-be2f-2d82aa9617ed",
   "metadata": {},
   "source": [
    "#### Ved a continuación cómo accedemos a los datos de preferencias del usuario almacenados en la memoria a corto plazo de nuestro agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947cf2d5-e26b-4225-9407-7d9871ef819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['user_preferences'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a91c44-da30-455c-bfb0-1e82b53b0e48",
   "metadata": {},
   "source": [
    "## ¿Qué pasa si la conversación es más larga que la ventana de contexto?\n",
    "* En aplicaciones de producción, las conversaciones largas pueden exceder la ventana de contexto del LLM. Las soluciones comunes a este problema son:\n",
    "    * Resumir mensajes.\n",
    "    * Recortar mensajes.\n",
    "    * Eliminar mensajes.\n",
    "    * Otras estrategias personalizadas.\n",
    "\n",
    "* Veremos cómo implementar algunas de estas soluciones en futuras lecciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94734cbc-5ed6-4eeb-b0ec-45718d1aa520",
   "metadata": {},
   "source": [
    "## Memoria a largo plazo\n",
    "* Para la memoria a largo plazo (también conocida como Store) usaremos un enfoque diferente, también inspirado en LangGraph. Como indica el equipo de LangChain en la documentación de LangChain, este es un tema avanzado que requiere conocimientos de LangGraph para usar, por lo tanto, estaréis mucho mejor preparados para dominarlo después de completar nuestro \"2026 Bootcamp: Understand and Build Professional AI Agents\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a960588-36b3-439a-8e1f-cfec5d85b894",
   "metadata": {},
   "source": [
    "## Cómo ejecutar este código desde Visual Studio Code\n",
    "* Abrid Terminal.\n",
    "* Aseguraos de que estáis en la carpeta del proyecto.\n",
    "* Aseguraos de tener el entorno poetry activado.\n",
    "* Introducid y ejecutad el siguiente comando:\n",
    "    * `python 007-short-term-memory.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6d6b5-899b-4049-95a7-0df073a4caba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
