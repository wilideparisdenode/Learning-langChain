{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db618cb5-48f4-4bec-af5f-84f46f97b029",
   "metadata": {},
   "source": [
    "# Uso de un modelo LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af86638-b9b2-4839-9059-776d4725359f",
   "metadata": {},
   "source": [
    "## LLM (tambi√©n conocidos como Modelos)\n",
    "* Los LLM (Large Language Models, tambi√©n llamados \"Modelos\") pueden interpretar y generar texto como los humanos. Son lo suficientemente vers√°tiles como para escribir contenido, traducir idiomas, resumir y responder preguntas sin necesidad de entrenamiento especializado para cada tarea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2032b13-9b0a-4d93-a7ae-6d01124151f1",
   "metadata": {},
   "source": [
    "## ¬øQu√© m√°s pueden hacer los Modelos?\n",
    "Adem√°s de la generaci√≥n de texto, muchos modelos soportan:\n",
    "* **Llamadas a herramientas** - invocar herramientas externas (como consultas a bases de datos o llamadas API) y usar los resultados en sus respuestas.\n",
    "* **Salida estructurada** - donde la respuesta del modelo est√° limitada a seguir un formato definido.\n",
    "* **Multimodalidad** - procesar y devolver datos distintos al texto, como\n",
    "    * im√°genes,\n",
    "    * audio,\n",
    "    * y v√≠deo.\n",
    "* **Razonamiento** - los modelos realizan razonamiento en m√∫ltiples pasos para llegar a una conclusi√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98b418-72e5-44e0-b079-4e0c876c40ce",
   "metadata": {},
   "source": [
    "## El papel de los Modelos en las aplicaciones LLM y agentes IA\n",
    "Los modelos se pueden utilizar de dos formas:\n",
    "* Con agentes - Los modelos pueden especificarse din√°micamente al crear un agente.\n",
    "* De forma independiente - Los modelos se pueden llamar directamente (fuera del bucle del agente) para tareas como\n",
    "    * generaci√≥n de texto,\n",
    "    * clasificaci√≥n,\n",
    "    * o extracci√≥n\n",
    "    * sin necesidad de un framework de agentes.\n",
    "\n",
    "La misma interfaz de modelo funciona en ambos contextos, lo que te da la flexibilidad de empezar de forma sencilla y escalar a flujos de trabajo m√°s complejos basados en agentes seg√∫n sea necesario.\n",
    "* **Los modelos son el motor de razonamiento de las aplicaciones LLM y los agentes IA**. Impulsan el proceso de toma de decisiones del agente, determinando qu√© herramientas llamar, c√≥mo interpretar los resultados y cu√°ndo proporcionar una respuesta final.\n",
    "* **La calidad y capacidades del modelo que elijas impactan directamente en la fiabilidad y rendimiento base de tu agente**. Diferentes modelos destacan en diferentes tareas - algunos son mejores siguiendo instrucciones complejas, otros en razonamiento estructurado, y algunos soportan ventanas de contexto m√°s grandes para manejar m√°s informaci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97684c2c-1e8e-4a7a-a9a9-6496968c3c13",
   "metadata": {},
   "source": [
    "## Recuerda, para usar un Modelo necesitar√°s cargar el archivo .env para asegurarte de que LangChain conoce la clave API del modelo que quieres usar\n",
    "* Cargar√°s el archivo .env al inicio de tu ejercicio as√≠:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0858d887-cd4b-4d6a-8df9-b27bbd400956",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a22e8-a8df-4e43-8f65-64741ae73ba2",
   "metadata": {},
   "source": [
    "## LangChain proporciona una forma estandarizada de usar Modelos\n",
    "* Cada modelo tiene una API diferente. En lugar de tener que aprender cada una de estas APIs, con LangChain tienes una forma estandarizada de acceder y usar la mayor√≠a de modelos que existen.\n",
    "* Para una lista completa de modelos soportados en LangChain, consulta la p√°gina de [integraciones](https://docs.langchain.com/oss/python/integrations/providers/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4976ce-494f-45f6-9a87-4aeded027d9e",
   "metadata": {},
   "source": [
    "## ¬øCu√°l es el mejor modelo LLM de OpenAI para usar en la clase de principiantes?\n",
    "Para una clase de **principiantes** de LangChain en enero de 2026, elegiremos **gpt-4o-mini** como predeterminado.\n",
    "\n",
    "### ¬øPor qu√© **gpt-4o-mini** es el mejor \"valor predeterminado para ense√±anza\" ahora?\n",
    "\n",
    "* **Comportamiento predecible para agentes/llamadas a herramientas**: Los principiantes necesitan ejecuciones que \"simplemente funcionen\" para poder aprender *conceptos de LangChain* (mensajes, herramientas, memoria, salida estructurada) sin perseguir rarezas del modelo.\n",
    "* **Coste muy bajo** mientras sigue siendo fuerte en seguimiento de instrucciones y ayuda b√°sica con c√≥digo.\n",
    "* **Contexto abundante** para demostraciones de clase (contexto de 128k).\n",
    "\n",
    "### Cu√°ndo usar **gpt-5-mini**\n",
    "\n",
    "Usaremos **gpt-5-mini** cuando ense√±emos:\n",
    "\n",
    "* **patrones ag√©nticos** m√°s complejos,\n",
    "* **selecci√≥n/planificaci√≥n de herramientas** m√°s robusta,\n",
    "* cadenas m√°s largas donde un poco m√°s de \"inteligencia\" ayuda a los estudiantes a ver menos fallos extra√±os.\n",
    "\n",
    "Sigue siendo relativamente asequible comparado con los modelos insignia, pero notablemente m√°s caro que 4o-mini.\n",
    "\n",
    "### Por qu√© *no* usar√≠amos **gpt-5-nano** como predeterminado (como hace a menudo el equipo de LangChain en la documentaci√≥n de LangChain)\n",
    "\n",
    "* Se comercializa como la variante GPT-5 **m√°s r√°pida/m√°s barata** y es genial para **resumen/clasificaci√≥n**.\n",
    "* Pero se han reportado **problemas de integraci√≥n reales con LangChain** donde `gpt-5-nano` puede devolver **contenido vac√≠o** sin errores obvios‚Äîexactamente el tipo de confusi√≥n \"¬øpor qu√© no se imprimi√≥ nada?\" que descarrila una clase de principiantes.\n",
    "\n",
    "### Nuestra recomendaci√≥n pr√°ctica\n",
    "\n",
    "* **Recomendaci√≥n predeterminada para todos los estudiantes:** **gpt-4o-mini**\n",
    "* **Secci√≥n \"avanzada / bonus\":** cambiar a **gpt-5-mini**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f954b-aac5-48f1-9ac6-1eac5b6b8a49",
   "metadata": {},
   "source": [
    "## Conectar con el modelo LLM\n",
    "* Para este ejercicio, nos conectaremos con el modelo gpt-5-nano de OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e09c43-c6aa-4ae7-b7df-6b393b8ba65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5589364-dd7b-454e-bc82-a9277ecba3cc",
   "metadata": {},
   "source": [
    "## Qu√© hace este c√≥digo\n",
    "\n",
    "```python\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "```\n",
    "\n",
    "Este c√≥digo **crea un modelo de IA basado en chat** con el que tu aplicaci√≥n de LangChain puede hablar.\n",
    "\n",
    "Piensa en ello como **conectar un cerebro de IA a tu programa** y decidir *c√≥mo debe comportarse*.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `init_chat_model(...)`\n",
    "\n",
    "Esta es una **funci√≥n auxiliar de LangChain** que:\n",
    "\n",
    "* Se conecta a un modelo de chat de OpenAI\n",
    "* Lo envuelve en una interfaz compatible con LangChain\n",
    "* Devuelve un objeto que puedes usar para enviar mensajes y recibir respuestas\n",
    "\n",
    "En t√©rminos simples:\n",
    "\n",
    "> \"Configura un modelo de chat de IA para que mi c√≥digo pueda hablar con √©l.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `model=\"gpt-4o-mini\"`\n",
    "\n",
    "Esto elige **qu√© modelo de IA** quieres usar.\n",
    "\n",
    "* **gpt-4o-mini** es:\n",
    "\n",
    "  * R√°pido ‚ö°\n",
    "  * M√°s barato üí∏\n",
    "  * A√∫n muy capaz para razonamiento, res√∫menes, clasificaci√≥n y redacci√≥n\n",
    "\n",
    "Piensa en ello como:\n",
    "\n",
    "> \"Usa una versi√≥n ligera pero inteligente de GPT-4.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `temperature=0.0`\n",
    "\n",
    "Esto controla **cu√°n creativo vs. predecible** es el modelo.\n",
    "\n",
    "* `temperature = 0.0` ‚Üí muy factual, consistente, aburrido\n",
    "* `temperature = 1.0` ‚Üí m√°s creativo, variado, similar a humanos\n",
    "* `temperature > 1.0` ‚Üí muy imaginativo, a veces ca√≥tico\n",
    "\n",
    "As√≠ que aqu√≠:\n",
    "\n",
    "> \"Proporciona respuestas muy factuales y consistentes.\"\n",
    "\n",
    "\n",
    "### Resumen en una frase\n",
    "\n",
    "> Este c√≥digo configura un modelo de chat de nivel GPT-4 r√°pido con respuestas factuales para que tu aplicaci√≥n LangChain pueda hablar con una IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf73780-0018-4c2d-a7ac-beb528154b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"¬øQui√©n mat√≥ a JFK?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da88ac-b763-44f0-aac4-088310c83daa",
   "metadata": {},
   "source": [
    "## .invoke vs. .stream para usar el Modelo en LangChain\n",
    ".invoke y .stream son las formas m√°s frecuentes de usar un Modelo.\n",
    "* .invoke generar√° respuestas en peque√±os fragmentos.\n",
    "* .stream generar√° respuestas en un flujo continuo, como lo que puedes ver en chatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d3ac8b-1a6c-46f0-8d41-6107c86fc873",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"¬øCu√°l es la mejor cafeter√≠a de San Francisco?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512d6d8-961c-4888-8f1a-765e017e96f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"¬øCu√°l es el mejor restaurante espa√±ol de San Francisco?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abeecb-e017-4720-a268-4b76587c679d",
   "metadata": {},
   "source": [
    "#### Qu√© hacen `.invoke()` vs `.stream()`:\n",
    "\n",
    "- **`.invoke()`**: Espera a la respuesta completa, luego te la da toda de una vez\n",
    "- **`.stream()`**: Te da la respuesta pieza por pieza mientras se est√° generando (como ver escribir a ChatGPT en tiempo real)\n",
    "\n",
    "#### Qu√© hace `end=\"|\"` en el ejemplo de .stream():\n",
    "\n",
    "Normalmente, la funci√≥n `print()` de Python a√±ade un **salto de l√≠nea** al final, as√≠ que cada impresi√≥n aparece en una nueva l√≠nea:\n",
    "```\n",
    "fragmento1\n",
    "fragmento2\n",
    "fragmento3\n",
    "```\n",
    "\n",
    "Al usar `end=\"|\"`, le est√°s diciendo a Python: \"En lugar de un salto de l√≠nea, pon un car√°cter de tuber√≠a `|` al final.\" Esto hace que todo se imprima en la **misma l√≠nea**, separado por tuber√≠as:\n",
    "```\n",
    "fragmento1|fragmento2|fragmento3|\n",
    "```\n",
    "\n",
    "#### ¬øPor qu√© usarlo en este ejemplo?\n",
    "\n",
    "El `end=\"|\"` es principalmente **con fines demostrativos** - te ayuda a **visualizar** que la respuesta est√° llegando en fragmentos separados en lugar de todo a la vez. Puedes ver literalmente d√≥nde empieza y termina cada pieza.\n",
    "\n",
    "En una aplicaci√≥n real, normalmente usar√≠as `end=\"\"` en su lugar (que significa \"no a√±adas nada al final\"), para que el texto fluya suavemente sin separadores - justo como ChatGPT se te muestra.\n",
    "\n",
    "La parte `flush=True` asegura que cada fragmento se muestre inmediatamente en lugar de esperar en un b√∫fer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0912c-e391-4cff-9f98-5c9a2e65e295",
   "metadata": {},
   "source": [
    "## Mira c√≥mo funciona .stream en la mayor√≠a de los casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b646a944-4fb5-445d-96ac-02e165c87c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in model.stream(\"¬øCu√°l es el mejor Meetup para expatriados europeos en San Francisco?\"):\n",
    "    print(chunk.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbd0a0-716b-494c-90e0-9e5211920273",
   "metadata": {},
   "source": [
    "## Opciones avanzadas para streaming en LangChain 1.0\n",
    "En LangChain 1.0 tenemos algunas opciones avanzadas con streaming:\n",
    "* Transmitir progreso del agente ‚Äî obtener actualizaciones de estado despu√©s de cada paso del agente.\n",
    "    * Esta opci√≥n es muy com√∫n en la mayor√≠a de agentes IA de nivel de producci√≥n.\n",
    "* Transmitir tokens LLM ‚Äî transmitir tokens del modelo de lenguaje a medida que se generan.\n",
    "* Transmitir actualizaciones personalizadas ‚Äî emitir se√±ales definidas por el usuario (ej., \"Obtenidos 10/100 registros\").\n",
    "* Transmitir m√∫ltiples modos ‚Äî elegir entre actualizaciones (progreso del agente), mensajes (tokens LLM + metadatos) o personalizado (datos arbitrarios del usuario).\n",
    "\n",
    "Puedes ver c√≥mo implementar estos enfoques avanzados en esta [p√°gina de documentaci√≥n de LangChain](https://docs.langchain.com/oss/python/langchain/streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e2b46-392a-49b9-8cfd-0ab7ce565b33",
   "metadata": {},
   "source": [
    "## Modelos alternativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed938b3-5709-4f39-b6cf-7944194055da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = init_chat_model(model=\"claude-sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951eddf8-67f2-4e76-ae96-a19b6e30b7f9",
   "metadata": {},
   "source": [
    "#### Algunos modelos como Gemini requieren importaciones personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee5e56-5849-4e91-8c44-d5aebee751ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model3 = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7307c00-89d2-45f4-b230-dab45d6feeb8",
   "metadata": {},
   "source": [
    "## C√≥mo ejecutar este c√≥digo desde Visual Studio Code\n",
    "* Abre el Terminal.\n",
    "* Aseg√∫rate de estar en la carpeta del proyecto.\n",
    "* Aseg√∫rate de tener el entorno poetry activado.\n",
    "* Introduce y ejecuta el siguiente comando:\n",
    "    * `python 001-using-llm-model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfe9f5-a509-478f-a7dc-bc696fa91a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
