{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "438625ae-3a55-4879-84d9-fdcfd50498f2",
   "metadata": {},
   "source": [
    "# Creando nuestro primer \"Agente de IA\" con LangChain 1.0\n",
    "* Como verás más adelante, esto realmente no es un Agente de IA, aunque LangChain lo llame Agente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582d4cf4-3cfc-4bab-82be-dabd723b1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093cb20-5e92-4d2a-b85f-068c33938df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "system_prompt = \"Eres un periodista de investigación.\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"¿Quién mató realmente a JFK?\")]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f1231-940f-4073-b6b5-9e651b42e4a7",
   "metadata": {},
   "source": [
    "#### Expliquemos el código anterior en términos sencillos\n",
    "Vamos a repasarlo línea por línea y explicar qué hace cada parte en un lenguaje claro.\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "\n",
    "* Esto importa una función llamada `create_agent` de LangChain.\n",
    "* Piensa en `create_agent` como un \"ayudante\" que construye un agente de IA por ti.\n",
    "\n",
    "```python\n",
    "system_prompt = \"Eres un periodista de investigación.\"\n",
    "```\n",
    "\n",
    "* Esto crea una variable llamada `system_prompt`.\n",
    "* El texto dentro de ella es una instrucción que establece el *rol/personalidad* de la IA.\n",
    "* Aquí, le estás diciendo a la IA: \"Actúa como un periodista de investigación.\"\n",
    "\n",
    "```python\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "```\n",
    "\n",
    "* Esto crea el agente y lo almacena en una variable llamada `agent`.\n",
    "* `model=\"gpt-4o-mini\"` le dice a LangChain qué modelo de IA usar.\n",
    "* `system_prompt=system_prompt` le da al agente la instrucción que escribiste arriba.\n",
    "* Después de esto, `agent` es básicamente tu \"bot periodista\" de IA listo para usar.\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "\n",
    "* Esto importa `HumanMessage`, que es una clase de LangChain que representa un mensaje escrito por un usuario humano.\n",
    "* LangChain usa objetos de mensaje para estructurar conversaciones (mensajes de usuario, mensajes de IA, mensajes del sistema, etc.).\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"¿Quién mató realmente a JFK?\")]}\n",
    ")\n",
    "```\n",
    "\n",
    "* Aquí es donde *ejecutas* el agente.\n",
    "* `invoke(...)` significa: \"Toma esta entrada, llama al modelo y devuélveme el resultado.\"\n",
    "* Pasas un diccionario con una clave `\"messages\"`.\n",
    "* El valor es una lista de mensajes — aquí es solo un `HumanMessage`.\n",
    "* El `content` de ese mensaje es la pregunta del usuario: `\"¿Quién mató realmente a JFK?\"`\n",
    "* El resultado se almacena en `response`.\n",
    "\n",
    "```python\n",
    "print(response['messages'][-1].content)\n",
    "```\n",
    "\n",
    "* `response` es un objeto estructurado (a menudo una estructura similar a un diccionario) que incluye una lista de mensajes.\n",
    "* `response['messages']` obtiene los mensajes de conversación devueltos por el agente.\n",
    "* `[-1]` significa \"el último elemento de la lista\" (normalmente la respuesta final del agente).\n",
    "* `.content` obtiene el texto real de ese último mensaje.\n",
    "* `print(...)` lo muestra en tu terminal.\n",
    "\n",
    "\n",
    "#### Resumen general (en una frase)\n",
    "\n",
    "Creas un agente de IA con un rol de \"periodista\", le envías una pregunta de usuario como un mensaje estructurado, y luego imprimes la respuesta final del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507837b-d76e-4c27-b08f-996796a562f0",
   "metadata": {},
   "source": [
    "## ¿Es esto realmente un Agente de IA?\n",
    "**Este código no muestra realmente un comportamiento agéntico** - es simplemente un chatbot elegante, aunque LangChain lo llame \"agente\".\n",
    "\n",
    "#### ¿Qué hace que una IA sea \"Agéntica\"?\n",
    "\n",
    "Un verdadero agente de IA debería ser capaz de:\n",
    "\n",
    "1. **Tomar decisiones de forma autónoma** - elegir qué hacer a continuación sin instrucciones humanas constantes\n",
    "2. **Usar herramientas** - elegir y usar diferentes herramientas (motores de búsqueda, calculadoras, bases de datos) según sea necesario\n",
    "3. **Planificar tareas de múltiples pasos** - dividir objetivos complejos en pasos más pequeños\n",
    "4. **Iterar y adaptarse** - probar diferentes enfoques si algo no funciona\n",
    "\n",
    "#### Lo que este código realmente hace\n",
    "\n",
    "Este código solo:\n",
    "- Recibe una pregunta\n",
    "- Genera una respuesta\n",
    "- Se detiene\n",
    "\n",
    "Eso es un **chatbot**, no realmente un agente. Es como hacer una pregunta a alguien y obtener una respuesta - no hay toma de decisiones autónoma.\n",
    "\n",
    "#### Dónde aparecería un verdadero comportamiento agéntico\n",
    "\n",
    "Una versión de agente real se vería más así:\n",
    "\n",
    "```python\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[web_search, calculator, database_query],  # ← Herramientas que puede usar\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "```\n",
    "\n",
    "Entonces, cuando preguntes \"¿Quién mató a JFK?\", el agente podría:\n",
    "1. **Decidir** \"Necesito información actualizada\" \n",
    "2. **Usar** la herramienta web_search\n",
    "3. **Analizar** los resultados\n",
    "4. **Decidir** \"También necesito registros históricos\"\n",
    "5. **Usar** la herramienta database_query\n",
    "6. **Sintetizar** todo en una respuesta\n",
    "\n",
    "**La diferencia clave:** Un agente elige su propio camino para resolver tu problema, usando las herramientas que cree necesarias. Este ejemplo simplemente responde directamente sin ningún uso autónomo de herramientas o toma de decisiones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3333bb-837d-454b-89fd-4cfa0c3f3e26",
   "metadata": {},
   "source": [
    "## Usando .stream en lugar de .invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b516ddf7-3de3-46f8-a7c9-d75656154503",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"De verdad, ¿quién mató realmente a JFK?\")]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "    \n",
    "    if token.content:\n",
    "        print(token.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87877e07-3776-48c9-81b3-eb213a30c2aa",
   "metadata": {},
   "source": [
    "## Expliquemos el código anterior en términos sencillos\n",
    "**El streaming** es donde el comportamiento del agente empieza a *sentirse vivo*.\n",
    "\n",
    "Lo explicaremos **línea por línea**, y luego te daremos la **idea general** al final.\n",
    "\n",
    "\n",
    "#### Explicación línea por línea\n",
    "\n",
    "```python\n",
    "for token, metadata in agent.stream(\n",
    "```\n",
    "\n",
    "* Esto inicia un bucle `for`.\n",
    "* En lugar de obtener **una respuesta final**, estás pidiendo al agente que **transmita su respuesta pieza por pieza**.\n",
    "* Cada iteración del bucle te da:\n",
    "\n",
    "  * `token` → un pequeño fragmento de la salida de la IA\n",
    "  * `metadata` → información adicional sobre ese fragmento (a menudo ignorada a nivel principiante)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    {\"messages\": [HumanMessage(content=\"De verdad, ¿quién mató realmente a JFK?\")]},\n",
    "```\n",
    "\n",
    "* Esta es la entrada que envías al agente.\n",
    "* Misma idea que antes:\n",
    "\n",
    "  * Pasas una lista de mensajes\n",
    "  * Aquí es un mensaje humano\n",
    "* La pregunta es ligeramente diferente, pero la estructura es idéntica.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    stream_mode=\"messages\"\n",
    "```\n",
    "\n",
    "* Esto le dice a LangChain **cómo** quieres que se transmita la salida.\n",
    "* `\"messages\"` significa:\n",
    "\n",
    "  * \"Transmite fragmentos de mensajes estructurados, no texto sin procesar\"\n",
    "* Cada `token` que recibas se comportará como un objeto de mensaje de IA parcial.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "):\n",
    "```\n",
    "\n",
    "* Esto cierra la llamada `agent.stream(...)`.\n",
    "* En este punto, LangChain comienza a llamar al modelo y a generar fragmentos de salida.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    if token.content:\n",
    "```\n",
    "\n",
    "* No todos los fragmentos transmitidos contienen texto.\n",
    "* Algunos fragmentos pueden ser:\n",
    "\n",
    "  * señales de control\n",
    "  * vacíos\n",
    "  * solo metadatos\n",
    "* Esta línea comprueba:\n",
    "\n",
    "  > \"¿Este fragmento realmente contiene texto?\"\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "        print(token.content, end=\"\", flush=True)\n",
    "```\n",
    "\n",
    "* Esto imprime el fragmento **inmediatamente**, sin un salto de línea.\n",
    "* `end=\"\"`:\n",
    "\n",
    "  * Evita saltos de línea para que el texto aparezca continuamente.\n",
    "* `flush=True`:\n",
    "\n",
    "  * Obliga a Python a imprimir instantáneamente en lugar de almacenar en búfer.\n",
    "* Resultado:\n",
    "\n",
    "  * La respuesta aparece **palabra por palabra**, como si alguien estuviera escribiendo.\n",
    "\n",
    "---\n",
    "\n",
    "#### Lo que realmente está pasando por debajo\n",
    "\n",
    "En lugar de esto:\n",
    "\n",
    "```\n",
    "[ el agente piensa ]\n",
    "[ el agente termina ]\n",
    "[ obtienes la respuesta completa ]\n",
    "```\n",
    "\n",
    "Ahora obtienes esto:\n",
    "\n",
    "```\n",
    "[ el agente comienza ]\n",
    "\"Bueno,\"\n",
    "\" basado\"\n",
    "\" en\"\n",
    "\" la\"\n",
    "\" evidencia\"\n",
    "\" disponible\"\n",
    "...\n",
    "```\n",
    "\n",
    "Así que estás **viendo al agente hablar en tiempo real**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Por qué esto es importante para los agentes (no es solo para una mejor Experiencia de Usuario)\n",
    "\n",
    "El streaming no es solo un \"efecto genial\".\n",
    "\n",
    "Permite:\n",
    "\n",
    "* retroalimentación en tiempo real\n",
    "* interrupción o cancelación\n",
    "* encadenar agentes mientras uno todavía está hablando\n",
    "* construir interfaces de chat, terminales o sistemas de voz\n",
    "\n",
    "En sistemas de agentes, el streaming se usa a menudo para:\n",
    "\n",
    "* observar el razonamiento\n",
    "* canalizar salida parcial a otro agente\n",
    "* reaccionar dinámicamente a lo que se está diciendo\n",
    "\n",
    "---\n",
    "\n",
    "#### Modelo mental para principiantes (muy útil)\n",
    "\n",
    "Piénsalo así:\n",
    "\n",
    "* `invoke()` → **Dame la respuesta terminada**\n",
    "* `stream()` → **Déjame escuchar mientras hablas**\n",
    "\n",
    "Mismo agente. Mismo rol. Modo de interacción diferente.\n",
    "\n",
    "---\n",
    "\n",
    "## Resumen en una frase\n",
    "\n",
    "Este código pide al agente que responda una pregunta **de forma incremental**, e imprime cada pequeño fragmento de su respuesta tan pronto como se genera, creando una salida en streaming en vivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78afbb69-d130-4d01-9936-0c0b2edcc356",
   "metadata": {},
   "source": [
    "## Cómo ejecutar este código desde Visual Studio Code\n",
    "* Abre el Terminal.\n",
    "* Asegúrate de estar en la carpeta del proyecto.\n",
    "* Asegúrate de tener el entorno poetry activado.\n",
    "* Introduce y ejecuta el siguiente comando:\n",
    "    * `python 002-creating-an-agent.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3bec79-3177-4e9a-85ea-8ffe4fe9deca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
