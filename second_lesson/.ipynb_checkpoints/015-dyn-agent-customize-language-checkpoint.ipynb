{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010c4bc0-8242-44c3-8744-ba2c448c08e0",
   "metadata": {},
   "source": [
    "# Example 1 of Dynamic Agent: Responding in the User's Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a219c138-0891-43fd-b20c-5bcf65119559",
   "metadata": {},
   "source": [
    "## Goal\n",
    "* Make the agent respond in Spanish when talking to Spanish speakers, and in English for English speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aed0304-6b16-42fa-b33a-1440fdd493e6",
   "metadata": {},
   "source": [
    "## The code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174698fd-6553-4668-a05f-27793b838251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259fb30c-0bbf-4d5a-83e7-fefdf5d21e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "@dataclass\n",
    "class LanguageContext:\n",
    "    user_language: str = \"English\"\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_language_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user role.\"\"\"\n",
    "    user_language = request.runtime.context.user_language\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if user_language != \"English\":\n",
    "        return f\"{base_prompt} only respond in {user_language}.\"\n",
    "    elif user_language == \"English\":\n",
    "        return base_prompt\n",
    "\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    context_schema=LanguageContext,\n",
    "    middleware=[user_language_prompt]\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"message\": [HumanMessage(content=\"Hola, ¿cómo estás?\")]},\n",
    "    context=LanguageContext(user_language=\"Spanish\")\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fdf6c8-80de-42d7-8658-0a0d17890251",
   "metadata": {},
   "source": [
    "## Let's explain the previous code in simple terms\n",
    "\n",
    "Below is a **beginner-friendly, line-by-line explanation** of the code.\n",
    "\n",
    "---\n",
    "\n",
    "## What this code does (big picture)\n",
    "\n",
    "This code creates a **LangChain dynamic agent** that:\n",
    "\n",
    "* Knows the user’s preferred language (English, Spanish, etc.)\n",
    "* Automatically **changes the system prompt**\n",
    "* Forces the AI to **reply only in that language**\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "> “Before the AI answers, check what language the user wants, and adjust the instructions accordingly.”\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Imports and setup\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "```\n",
    "\n",
    "#### What’s happening here?\n",
    "\n",
    "* `dataclass` (from Python itself) is used to define **simple data containers**\n",
    "* `dynamic_prompt` is a **LangChain decorator** that marks a function as something that can **modify prompts at runtime**\n",
    "* `ModelRequest` is an object that contains **everything about the current request**, including:\n",
    "\n",
    "  * the model being used\n",
    "  * messages\n",
    "  * runtime context (like user settings)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Defining the context (shared state)\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class LanguageContext:\n",
    "    user_language: str = \"English\"\n",
    "```\n",
    "\n",
    "#### What’s happening here?\n",
    "\n",
    "* This defines a **context object** that travels with every request\n",
    "* It stores **extra information** that is NOT part of the user message\n",
    "* In this case, we store:\n",
    "\n",
    "  * `user_language`: the language the user prefers\n",
    "\n",
    "#### Why this matters\n",
    "\n",
    "Instead of guessing the language from the message, we:\n",
    "\n",
    "* Explicitly tell the agent what language to use\n",
    "* Keep this logic clean and predictable\n",
    "\n",
    "Think of `LanguageContext` as:\n",
    "\n",
    "> “Extra settings about the user that the AI should know.”\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Creating a dynamic system prompt\n",
    "\n",
    "```python\n",
    "@dynamic_prompt\n",
    "def user_language_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"Generate system prompt based on user language.\"\"\"\n",
    "```\n",
    "\n",
    "#### What’s happening?\n",
    "\n",
    "* `@dynamic_prompt` tells LangChain:\n",
    "\n",
    "  > “This function will generate a **system prompt** dynamically for each request.”\n",
    "* LangChain will automatically call this function **before** the model runs\n",
    "\n",
    "---\n",
    "\n",
    "#### Accessing the context\n",
    "\n",
    "```python\n",
    "    user_language = request.runtime.context.user_language\n",
    "```\n",
    "\n",
    "* `request` is the full request object\n",
    "* `request.runtime.context` contains the context object we passed in\n",
    "* `user_language` is read from `LanguageContext`\n",
    "\n",
    "In plain English:\n",
    "\n",
    "> “Look up the user’s preferred language.”\n",
    "\n",
    "---\n",
    "\n",
    "#### Base system instruction\n",
    "\n",
    "```python\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "```\n",
    "\n",
    "* This is the default system instruction\n",
    "* Everything else builds on top of this\n",
    "\n",
    "---\n",
    "\n",
    "#### Conditional logic\n",
    "\n",
    "```python\n",
    "    if user_language != \"English\":\n",
    "        return f\"{base_prompt} only respond in {user_language}.\"\n",
    "    elif user_language == \"English\":\n",
    "        return base_prompt\n",
    "```\n",
    "\n",
    "#### What’s happening?\n",
    "\n",
    "* If the user language is **not English**:\n",
    "\n",
    "  * We add a strict instruction telling the model to reply only in that language\n",
    "* If the language **is English**:\n",
    "\n",
    "  * We keep the prompt simple\n",
    "\n",
    "Example outputs:\n",
    "\n",
    "* Spanish →\n",
    "  `\"You are a helpful assistant. only respond in Spanish.\"`\n",
    "* English →\n",
    "  `\"You are a helpful assistant.\"`\n",
    "\n",
    "This returned string becomes the **system prompt** sent to the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Creating the agent\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "\n",
    "This imports LangChain’s helper function for building agents.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    context_schema=LanguageContext,\n",
    "    middleware=[user_language_prompt]\n",
    ")\n",
    "```\n",
    "\n",
    "#### What’s happening here?\n",
    "\n",
    "* `model=\"gpt-4o-mini\"`\n",
    "  → Selects the LLM to use\n",
    "\n",
    "* `context_schema=LanguageContext`\n",
    "  → Tells LangChain:\n",
    "\n",
    "  > “Every request may include a `LanguageContext` object”\n",
    "\n",
    "* `middleware=[user_language_prompt]`\n",
    "  → Registers our dynamic prompt function\n",
    "  → LangChain will run it **before each model call**\n",
    "\n",
    "Think of middleware as:\n",
    "\n",
    "> “Code that runs *between* the user input and the AI response.”\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Sending a message to the agent\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "\n",
    "* `HumanMessage` represents a user message in LangChain’s message format\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "    {\"message\": [HumanMessage(content=\"Hola, ¿cómo estás?\")]},\n",
    "    context=LanguageContext(user_language=\"Spanish\")\n",
    ")\n",
    "```\n",
    "\n",
    "#### What’s happening step by step?\n",
    "\n",
    "1. The user sends a message in Spanish\n",
    "2. We explicitly pass:\n",
    "\n",
    "   ```python\n",
    "   context=LanguageContext(user_language=\"Spanish\")\n",
    "   ```\n",
    "3. LangChain:\n",
    "\n",
    "   * Stores this context in `request.runtime.context`\n",
    "   * Calls `user_language_prompt`\n",
    "   * Builds the system prompt:\n",
    "\n",
    "     ```\n",
    "     You are a helpful assistant. only respond in Spanish.\n",
    "     ```\n",
    "4. The model receives:\n",
    "\n",
    "   * System prompt (from middleware)\n",
    "   * User message\n",
    "5. The model replies in Spanish\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Printing the final answer\n",
    "\n",
    "```python\n",
    "print(response[\"messages\"][-1].content)\n",
    "```\n",
    "\n",
    "#### What’s happening?\n",
    "\n",
    "* `response[\"messages\"]` is a list of all messages (system, user, assistant)\n",
    "* `[-1]` gets the **last message**, which is the AI’s reply\n",
    "* `.content` extracts the text\n",
    "\n",
    "---\n",
    "\n",
    "## Final mental model (important!)\n",
    "\n",
    "Think of this flow:\n",
    "\n",
    "```\n",
    "User message\n",
    "   ↓\n",
    "Context (LanguageContext)\n",
    "   ↓\n",
    "Dynamic prompt middleware\n",
    "   ↓\n",
    "System prompt is generated\n",
    "   ↓\n",
    "LLM is called\n",
    "   ↓\n",
    "Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# Why this pattern is powerful\n",
    "\n",
    "* ✅ Clean separation of concerns\n",
    "* ✅ No prompt hacks in user messages\n",
    "* ✅ Context-driven behavior\n",
    "* ✅ Easy to extend (tone, role, permissions, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698d38e-b33c-4d3a-8058-0dd01316c98a",
   "metadata": {},
   "source": [
    "## How to run this code from Visual Studio Code\n",
    "* Open Terminal.\n",
    "* Make sure you are in the project folder.\n",
    "* Make sure you have the poetry env activated.\n",
    "* Enter and run the following command:\n",
    "    * `python 015-dyn-agent-customize-language.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7926f-7d52-49b0-bcae-9f2c4533e104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
