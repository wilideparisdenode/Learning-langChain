{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33c242e-61a9-4e54-bf60-47cbef343d6f",
   "metadata": {},
   "source": [
    "# A LangChain 1.0 app that can talk with a PDF Document\n",
    "* As you will see, this is not an agent yet. We will make it an agent in the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33780ffa-5163-4b6f-8409-bdb8b7409110",
   "metadata": {},
   "source": [
    "## What is Semantic Search?\n",
    "\n",
    "**Semantic search** is a search technique that understands the **meaning** (semantics) of your query rather than just matching exact keywords. Instead of looking for exact word matches, it finds content that is conceptually similar to your question.\n",
    "\n",
    "**Why is this code an example of semantic search?**\n",
    "- Traditional keyword search would look for exact words like \"Gartner\" or \"percentage\"\n",
    "- Semantic search converts both your question AND the document chunks into numerical vectors (embeddings)\n",
    "- It then finds chunks whose vectors are closest in mathematical space to your question's vector\n",
    "- This means it can find relevant answers even if the exact wording is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a427d97e-1671-4baa-ad23-04d2306b6f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ed0e93-b531-48d7-97ad-81e390156add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Generative AI in 2026:\n",
      "Transforming Business and\n",
      " Professional Value\n",
      "As we progress through 2026, Generative AI has reached a critical inflection point. After years\n",
      "of experimentation and pilot programs, businesses are now demanding concrete returns on\n",
      "their AI investments. This year marks the transition from proof-of-concept to production-scale\n",
      "deployment, with organizations shifting their focus from \"what's possible\" to \"what's\n",
      "profitable.\"\n",
      "The 2026 AI Landscape: From Hype to Reality\n",
      "The generative AI market has experienced explosive growth, with global investment tripling\n",
      "from 2024 to 2025, reaching approximately $37 billion. Gartner research indicates that by\n",
      "2026, more than 80% of enterprises will use generative AI APIs or deploy generative\n",
      "AI-enabled applications in production environmentsâ€”a staggering increase from just 5% in\n",
      "2023.\n",
      "However, this growth comes with a sobering reality check. MIT research revealed a 95%' metadata={'producer': 'ReportLab PDF Library - (opensource)', 'creator': '(unspecified)', 'creationdate': '2026-01-14T11:57:12+01:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2026-01-14T11:57:12+01:00', 'subject': '(unspecified)', 'title': '(anonymous)', 'trapped': '/False', 'source': 'gen-ai-in-2026.pdf', 'total_pages': 4, 'page': 0, 'page_label': '1', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"gen-ai-in-2026.pdf\")\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "results = vector_store.similarity_search(\n",
    "    \"According to Gartner, what percentage of enterprises will use Generative AI APis or deploy generative AI-enabled applications in production environments in 2026?\"\n",
    ")\n",
    "\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c881b-1e43-4560-b7d4-4c418370ef64",
   "metadata": {},
   "source": [
    "## Explaining the previous code in simple terms\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "```\n",
    "**Imports the PDF loader** - This is a tool that can read PDF files and convert them into a format LangChain can work with.\n",
    "\n",
    "```python\n",
    "loader = PyPDFLoader(\"gen-ai-in-2026.pdf\")\n",
    "```\n",
    "**Creates a loader object** for the specific PDF file. Think of this as pointing to your document.\n",
    "\n",
    "```python\n",
    "data = loader.load()\n",
    "```\n",
    "**Loads the PDF** and converts it into Document objects. Each page becomes a document with `page_content` (the text) and `metadata` (information about the page).\n",
    "\n",
    "```python\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "```\n",
    "**Imports the text splitter** - PDFs are often too long to process all at once, so we need to break them into smaller pieces.\n",
    "\n",
    "```python\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "```\n",
    "**Configures how to split the text**:\n",
    "- `chunk_size=1000`: Each piece will be about 1000 characters long\n",
    "- `chunk_overlap=200`: Chunks overlap by 200 characters to avoid cutting sentences awkwardly\n",
    "- `add_start_index=True`: Remembers where each chunk came from in the original document\n",
    "\n",
    "```python\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "```\n",
    "**Actually performs the splitting** - Takes the full PDF and breaks it into smaller, manageable chunks.\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "```\n",
    "**Imports the embedding model** - This will convert text into numerical vectors.\n",
    "\n",
    "```python\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "```\n",
    "**Creates an embeddings object** using OpenAI's text-embedding-3-large model. This model is specialized in understanding the meaning of text and converting it to numbers.\n",
    "\n",
    "```python\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "```\n",
    "**Imports the vector database** - This is a special database designed to store and search through vectors efficiently.\n",
    "\n",
    "```python\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "```\n",
    "**Creates a vector database** that will use our embeddings model. \"InMemory\" means it stores everything in RAM (fast but temporary).\n",
    "\n",
    "```python\n",
    "ids = vector_store.add_documents(documents=all_splits)\n",
    "```\n",
    "**Adds all document chunks to the database** - Each chunk gets converted to a vector and stored. The `ids` are unique identifiers for each stored chunk.\n",
    "\n",
    "```python\n",
    "results = vector_store.similarity_search(\n",
    "    \"According to Gartner, what percentage of enterprises will use Generative AI APis or deploy generative AI-enabled applications in production environments in 2026?\"\n",
    ")\n",
    "```\n",
    "**Performs the semantic search**:\n",
    "1. Converts your question into a vector\n",
    "2. Compares it to all stored chunk vectors\n",
    "3. Returns the most similar chunks (by default, 4 chunks)\n",
    "4. \"Similarity\" is measured by how close the vectors are in mathematical space\n",
    "\n",
    "```python\n",
    "print(results[0])\n",
    "```\n",
    "**Prints the first (most similar) result** - This shows the chunk of text that is most semantically similar to your question.\n",
    "\n",
    "\n",
    "#### Why Are The Results Not Very Good?\n",
    "\n",
    "When you execute `print(results[0])`, you get a **raw document chunk**, not an actual answer. The problems are:\n",
    "\n",
    "1. **No Answer Extraction**: You're just seeing the retrieved text, but there's no AI reading it and answering your question\n",
    "2. **Raw Format**: The output includes metadata and the full chunk, making it hard to read\n",
    "3. **No Context Synthesis**: If the answer requires information from multiple chunks, you only see one\n",
    "4. **No Reasoning**: There's no LLM to interpret the retrieved content and formulate a proper response\n",
    "\n",
    "\n",
    "## How we will Improve the Results in the next exercise with RAG.\n",
    "\n",
    "This code demonstrates **semantic search** (finding relevant information by meaning), but to get good answers, you need **RAG** (Retrieval-Augmented Generation) which combines:\n",
    "1. **Retrieval**: Finding relevant chunks (what we have)\n",
    "2. **Generation**: Using an LLM to read those chunks and generate a coherent answer (what we're missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7721df-dfb3-48f0-9a25-2d32d2dc10fb",
   "metadata": {},
   "source": [
    "## How to run this code from Visual Studio Code\n",
    "* Open Terminal.\n",
    "* Make sure you are in the project folder.\n",
    "* Make sure you have the poetry env activated.\n",
    "* Enter and run the following command:\n",
    "    * `python 020-pdf-agent.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a57d87-7870-4a79-b9ba-1739f8cc3621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
