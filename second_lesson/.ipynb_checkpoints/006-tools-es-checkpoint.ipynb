{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55174e41-abf9-414d-b2d4-1ce572cdfd16",
   "metadata": {},
   "source": [
    "# Uso de Herramientas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63b6ee-f6b3-4203-af2d-be53326f7f68",
   "metadata": {},
   "source": [
    "## ¬øQu√© son las herramientas?\n",
    "* Algunos casos de uso requieren que los Modelos interact√∫en directamente con herramientas: sistemas externos‚Äîcomo APIs, bases de datos o sistemas de archivos‚Äîutilizando entradas estructuradas.\n",
    "* Algunos Modelos (p. ej., OpenAI, Anthropic y Gemini) cuentan con herramientas integradas que se ejecutan del lado del servidor, como b√∫squeda web e int√©rpretes de c√≥digo. Consultad la p√°gina de [integraciones](https://docs.langchain.com/oss/python/integrations) para m√°s informaci√≥n al respecto.\n",
    "* Tambi√©n pod√©is crear herramientas personalizadas.\n",
    "* Las herramientas ampl√≠an las capacidades del Modelo permiti√©ndole interactuar con el mundo a trav√©s de entradas y salidas bien definidas.\n",
    "* Las Aplicaciones LLM Ag√©nticas (tambi√©n conocidas como Agentes) pueden decidir si necesitan usar una herramienta disponible para alcanzar un objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5badac8f-3262-402d-96b2-e24ac6fe072c",
   "metadata": {},
   "source": [
    "## Como siempre, primero carguemos el archivo .env\n",
    "* Recordad que en el archivo .env tenemos nuestra clave API de OpenAI y la clave API de Tavily (esto nos permitir√° usar la herramienta m√°s adelante)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0004362f-c1d2-4d45-a043-82d74ddf593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06587e-beac-4305-94de-413893f5b9a4",
   "metadata": {},
   "source": [
    "#### Recordad c√≥mo pod√©is obtener vuestra clave API de Tavily\n",
    "\n",
    "Aqu√≠ ten√©is c√≥mo obtener vuestra clave API de Tavily en pasos sencillos:\n",
    "\n",
    "**1. Id al sitio web de Tavily**\n",
    "Visitad [tavily.com](https://tavily.com) y haced clic en \"Get API Key\" o \"Sign Up\"\n",
    "\n",
    "**2. Cread una cuenta**\n",
    "Registraos usando vuestra direcci√≥n de correo electr√≥nico o mediante un login social (Google, GitHub, etc.)\n",
    "\n",
    "**3. Verificad vuestro correo electr√≥nico**\n",
    "Revisad vuestra bandeja de entrada para encontrar un correo de verificaci√≥n y haced clic en el enlace para confirmar vuestra cuenta\n",
    "\n",
    "**4. Obtened vuestra clave API gratuita**\n",
    "Una vez que hay√°is iniciado sesi√≥n, ver√©is vuestra clave API en el panel de control. Tavily ofrece un nivel gratuito que incluye 1.000 llamadas API al mes, que es m√°s que suficiente para aprender y hacer los ejercicios del curso\n",
    "\n",
    "**5. Copiad y guardad la clave**\n",
    "Copiad la clave API (se parece a una larga cadena de letras y n√∫meros). Guardadla en un lugar seguro - la necesitar√©is para vuestros ejercicios de LangChain\n",
    "\n",
    "**6. A√±adidla en vuestro archivo .env**\n",
    "```\n",
    "TAVILY_API_KEY=vuestra-clave-api-aqui\n",
    "```\n",
    "\n",
    "**Consejo importante:** Nunca compart√°is vuestra clave API p√∫blicamente ni la sub√°is a GitHub. El nivel gratuito deber√≠a ser suficiente para la mayor√≠a de los ejercicios del curso, pero siempre pod√©is consultar vuestro uso en el panel de control de Tavily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f990caf-8f89-4914-afb3-f2450e6b97d4",
   "metadata": {},
   "source": [
    "## Primero, probemos un \"Agente\" sin herramientas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19e499-b0f5-480c-bc75-e3066ea32e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "system_prompt = \"Eres un asistente de pron√≥stico del tiempo.\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"¬øC√≥mo est√° el tiempo hoy (3 de enero de 2026) en San Francisco?\")]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fbaa8-59cb-4c15-8c1b-08fbdb0db3c2",
   "metadata": {},
   "source": [
    "## Luego, probemos un Agente muy b√°sico usando una herramienta externa de b√∫squeda en l√≠nea (Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb1448a-98f4-4ab6-9be9-ef5408a7da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Buscar informaci√≥n en la web\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)\n",
    "\n",
    "#probando la herramienta\n",
    "web_search.invoke(\"¬øC√≥mo est√° el tiempo hoy (3 de enero de 2026) en San Francisco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a7ca65-f0dc-4f7e-ad4b-22689510fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "weather_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[web_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b6ba0-5cdb-4851-8d64-f0f423a95303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=\"¬øC√≥mo est√° el tiempo hoy (3 de enero de 2026) en San Francisco?\")\n",
    "\n",
    "response = weather_agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8725dd-94f8-4946-a82e-a7fd1c87bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff40859-5362-4bad-9503-9855e0909c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"messages\"][1].tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e932c6-fe53-4a38-a18f-6f604a6f17b3",
   "metadata": {},
   "source": [
    "## Expliquemos el c√≥digo anterior en t√©rminos sencillos\n",
    "\n",
    "A continuaci√≥n se muestra el mismo c√≥digo, explicado **en t√©rminos sencillos, l√≠nea por l√≠nea**, como si fueseis nuevos en Python + LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) Cargar variables de entorno (claves API, secretos)\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "```\n",
    "\n",
    "* Importa una funci√≥n auxiliar del paquete `python-dotenv`.\n",
    "* Este paquete os permite almacenar secretos (como claves API) en un archivo `.env`.\n",
    "\n",
    "```python\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "* Lee vuestro archivo `.env` y carga sus valores en \"variables de entorno\".\n",
    "* Ejemplo: si vuestro `.env` contiene `TAVILY_API_KEY=...`, vuestro programa ahora puede acceder a ella autom√°ticamente.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) Importaciones para crear una herramienta de LangChain + tipado\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "```\n",
    "\n",
    "* Importa el decorador `@tool`.\n",
    "* Una \"herramienta\" en LangChain es b√°sicamente una funci√≥n normal de Python que el agente puede llamar.\n",
    "\n",
    "```python\n",
    "from typing import Dict, Any\n",
    "```\n",
    "\n",
    "* Importa sugerencias de tipo:\n",
    "\n",
    "  * `Dict[str, Any]` significa \"un diccionario con claves de cadena y valores de cualquier tipo\".\n",
    "* Las sugerencias de tipo ayudan a los humanos (y editores) a entender qu√© devuelve la funci√≥n.\n",
    "\n",
    "```python\n",
    "from tavily import TavilyClient\n",
    "```\n",
    "\n",
    "* Importa el cliente Python de Tavily.\n",
    "* Tavily es un servicio de API de b√∫squeda web.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) Crear un cliente Tavily (el objeto que realiza b√∫squedas)\n",
    "\n",
    "```python\n",
    "tavily_client = TavilyClient()\n",
    "```\n",
    "\n",
    "* Crea una instancia de cliente Tavily.\n",
    "* Entre bastidores, este cliente lee vuestra clave API de Tavily de las variables de entorno (por eso cargasteis `.env` antes).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4) Definir una funci√≥n herramienta de LangChain: `web_search`\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "```\n",
    "\n",
    "* `@tool` convierte esta funci√≥n de Python en una **herramienta de LangChain**.\n",
    "* El agente puede llamarla m√°s tarde.\n",
    "* La funci√≥n toma una entrada:\n",
    "\n",
    "  * `query: str` ‚Üí una cadena de b√∫squeda\n",
    "* Devuelve:\n",
    "\n",
    "  * `Dict[str, Any]` ‚Üí resultados de b√∫squeda en forma de diccionario\n",
    "\n",
    "```python\n",
    "    \"\"\"Buscar informaci√≥n en la web\"\"\"\n",
    "```\n",
    "\n",
    "* Esta es la docstring de la funci√≥n (una descripci√≥n).\n",
    "* Los agentes a veces pueden leer esta descripci√≥n para decidir cu√°ndo usar la herramienta.\n",
    "\n",
    "```python\n",
    "    return tavily_client.search(query)\n",
    "```\n",
    "\n",
    "* Realmente realiza una b√∫squeda web usando Tavily.\n",
    "* Lo que sea que Tavily devuelva (normalmente un diccionario con resultados) es devuelto por vuestra herramienta.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Probar la herramienta directamente (sin un agente)\n",
    "\n",
    "```python\n",
    "#probando la herramienta\n",
    "web_search.invoke(\"¬øC√≥mo est√° el tiempo hoy (3 de enero de 2026) en San Francisco?\")\n",
    "```\n",
    "\n",
    "* Esto llama a la herramienta **manualmente**.\n",
    "* `invoke(...)` es la forma est√°ndar de LangChain de ejecutar una herramienta.\n",
    "* Esto es solo para confirmar \"la herramienta funciona\" antes de d√°rsela a un agente.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Crear un agente y darle vuestra herramienta\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "\n",
    "* Importa una funci√≥n auxiliar que construye un \"agente\".\n",
    "* Un **agente** es una aplicaci√≥n LLM que puede decidir:\n",
    "\n",
    "  * \"Deber√≠a responder directamente\" O\n",
    "  * \"Deber√≠a llamar a una herramienta para obtener informaci√≥n\"\n",
    "\n",
    "```python\n",
    "weather_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[web_search]\n",
    ")\n",
    "```\n",
    "\n",
    "* Crea un agente llamado `weather_agent`.\n",
    "* `model=\"gpt-4o-mini\"`: elige qu√© LLM usar.\n",
    "* `tools=[web_search]`: da permiso al agente para llamar a vuestra herramienta `web_search`.\n",
    "* As√≠ que el agente puede hacer cosas como: \"Para responder c√≥mo est√° el tiempo hoy en una ubicaci√≥n particular, deber√≠a buscar en la web.\" (Recordad: como vimos con el Agente anterior, el LLM no puede responder a esta pregunta directamente ya que fue entrenado con datos pasados, no datos de hoy. Por eso necesitar√° Tavily para eso).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Crear un mensaje de usuario\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "\n",
    "* Importa el \"objeto mensaje\" que representa algo que dice un humano/usuario.\n",
    "\n",
    "```python\n",
    "question = HumanMessage(content=\"¬øC√≥mo est√° el tiempo hoy (3 de enero de 2026) en San Francisco?\")\n",
    "```\n",
    "\n",
    "* Construye un mensaje humano con vuestra pregunta dentro.\n",
    "* Los agentes a menudo trabajan con \"mensajes\" tipo chat en lugar de cadenas simples.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Hacer la pregunta al agente (el agente puede llamar herramientas)\n",
    "\n",
    "```python\n",
    "response = weather_agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "```\n",
    "\n",
    "* Ejecuta el agente.\n",
    "* Le pas√°is un diccionario con `\"messages\"` que contiene la conversaci√≥n hasta el momento.\n",
    "* El agente:\n",
    "\n",
    "  1. Leer√° la pregunta\n",
    "  2. Decidir√° si necesita la web\n",
    "  3. Potencialmente llamar√° a `web_search(...)`\n",
    "  4. Producir√° un mensaje de respuesta final\n",
    "\n",
    "El resultado (`response`) es t√≠picamente un diccionario que contiene una lista `\"messages\"` (la conversaci√≥n incluyendo llamadas a herramientas + respuesta final).\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Imprimir el texto de respuesta final\n",
    "\n",
    "```python\n",
    "print(response['messages'][-1].content)\n",
    "```\n",
    "\n",
    "* `response['messages']` es una lista de mensajes.\n",
    "* `[-1]` significa \"el √∫ltimo mensaje\".\n",
    "* `.content` es el texto de ese mensaje.\n",
    "* As√≠ que esto imprime la respuesta final del agente.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Imprimir con formato bonito el historial completo de mensajes\n",
    "\n",
    "```python\n",
    "from pprint import pprint\n",
    "```\n",
    "\n",
    "* Importa una funci√≥n de impresi√≥n m√°s bonita para objetos Python complejos.\n",
    "\n",
    "```python\n",
    "pprint(response['messages'])\n",
    "```\n",
    "\n",
    "* Imprime *todos* los mensajes de forma ordenada.\n",
    "* Esto a menudo incluye:\n",
    "\n",
    "  * vuestro HumanMessage\n",
    "  * solicitud(es) de llamada a herramientas\n",
    "  * mensaje(s) de salida de herramientas\n",
    "  * respuesta final de IA\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Inspeccionar las llamadas a herramientas que hizo el agente\n",
    "\n",
    "```python\n",
    "print(response[\"messages\"][1].tool_calls)\n",
    "```\n",
    "\n",
    "* Observa el **segundo mensaje** en la lista (`[1]`).\n",
    "* A menudo, el mensaje `[1]` es el mensaje del agente donde decidi√≥ llamar a herramientas (depende del comportamiento del agente/framework).\n",
    "* `.tool_calls` muestra qu√© herramienta(s) intent√≥ llamar, incluyendo argumentos (como la consulta que us√≥).\n",
    "\n",
    "---\n",
    "\n",
    "# Resumen del modelo mental (s√∫per simple)\n",
    "\n",
    "* Creaste una **herramienta de b√∫squeda web** (`web_search`) que usa Tavily.\n",
    "* Creaste un **agente** que puede usar esa herramienta.\n",
    "* Preguntaste: \"¬øC√≥mo est√° el tiempo?\"\n",
    "* El agente puede decidir: \"Necesito la web ‚Üí llamar a `web_search` ‚Üí leer resultados ‚Üí responder.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c3bfa-b7d8-4c42-bd41-04c50572b767",
   "metadata": {},
   "source": [
    "## C√≥mo ejecutar este c√≥digo desde Visual Studio Code\n",
    "* Abrid Terminal.\n",
    "* Aseguraos de estar en la carpeta del proyecto.\n",
    "* Aseguraos de tener el entorno poetry activado.\n",
    "* Introducid y ejecutad el siguiente comando:\n",
    "    * `python 006-tools.py` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011c1bf-684d-4230-a771-e787c2bf6a3e",
   "metadata": {},
   "source": [
    "## Consejo Avanzado: Las herramientas son m√°s potentes cuando pueden acceder al estado del agente, contexto de ejecuci√≥n y memoria a largo plazo.\n",
    "Esto permite a las herramientas tomar decisiones conscientes del contexto, personalizar respuestas y mantener informaci√≥n a trav√©s de conversaciones.\n",
    "\n",
    "Las herramientas pueden acceder a informaci√≥n de ejecuci√≥n a trav√©s del par√°metro `ToolRuntime`, que proporciona:\n",
    "* State (Estado) - Datos mutables que fluyen a trav√©s de la ejecuci√≥n (p. ej., mensajes, contadores, campos personalizados)\n",
    "* Context (Contexto) - Configuraci√≥n inmutable como IDs de usuario, detalles de sesi√≥n o configuraci√≥n espec√≠fica de la aplicaci√≥n\n",
    "* Store (Almac√©n) - Memoria persistente a largo plazo a trav√©s de conversaciones\n",
    "* Stream Writer (Escritor de flujo) - Transmitir actualizaciones personalizadas mientras se ejecutan las herramientas\n",
    "* Config (Configuraci√≥n) - RunnableConfig para la ejecuci√≥n\n",
    "* Tool Call ID (ID de llamada a herramienta) - ID de la llamada a herramienta actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8ff0d-d613-4f89-9128-d510f0afb204",
   "metadata": {},
   "source": [
    "## El par√°metro `ToolRuntime`\n",
    "Permitidnos explicar el par√°metro `ToolRuntime` en t√©rminos sencillos.\n",
    "\n",
    "#### ¬øQu√© es `ToolRuntime`?\n",
    "\n",
    "Pensad en `ToolRuntime` como una **mochila m√°gica** a la que vuestra herramienta obtiene acceso autom√°ticamente cuando se ejecuta. Esta mochila contiene informaci√≥n y capacidades importantes que vuestra herramienta podr√≠a necesitar.\n",
    "\n",
    "\n",
    "#### ¬øQu√© hay dentro de la mochila?\n",
    "\n",
    "`ToolRuntime` da a vuestra herramienta acceso a 6 cosas importantes:\n",
    "\n",
    "1. **State (Estado)** - Informaci√≥n que cambia a medida que progresa la conversaci√≥n (como los mensajes en el chat)\n",
    "2. **Context (Contexto)** - Configuraci√≥n fija (como qui√©n es el usuario)\n",
    "3. **Store (Almac√©n)** - Memoria a largo plazo (como una base de datos para recordar cosas entre conversaciones)\n",
    "4. **Streaming (Transmisi√≥n)** - Capacidad de enviar actualizaciones mientras la herramienta est√° trabajando\n",
    "5. **Config (Configuraci√≥n)** - Ajustes t√©cnicos\n",
    "6. **Tool Call ID (ID de llamada a herramienta)** - Un identificador √∫nico para esta ejecuci√≥n espec√≠fica de la herramienta\n",
    "\n",
    "\n",
    "#### ¬øPor qu√© es √∫til esto?\n",
    "\n",
    "Antes de `ToolRuntime`, ten√≠ais que usar 4 m√©todos complicados diferentes para acceder a estas cosas. Ahora, solo necesit√°is **un par√°metro simple** - mucho m√°s f√°cil. `ToolRuntime` reemplaza el patr√≥n antiguo de usar anotaciones separadas de\n",
    "    * InjectedState,\n",
    "    * InjectedStore,\n",
    "    * get_runtime,\n",
    "    * e InjectedToolCallId.\n",
    "\n",
    "#### Ejemplo 1: Contando Mensajes\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Acceder al estado actual de la conversaci√≥n\n",
    "@tool\n",
    "def summarize_conversation(\n",
    "    runtime: ToolRuntime\n",
    ") -> str:\n",
    "    \"\"\"Resumir la conversaci√≥n hasta ahora.\"\"\"\n",
    "    messages = runtime.state[\"messages\"]\n",
    "\n",
    "    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n",
    "    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n",
    "    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n",
    "\n",
    "    return f\"La conversaci√≥n tiene {human_msgs} mensajes de usuario, {ai_msgs} respuestas de IA, y {tool_msgs} resultados de herramientas\"\n",
    "```\n",
    "\n",
    "**Qu√© est√° pasando aqu√≠:**\n",
    "- La herramienta recibe `runtime` autom√°ticamente (no lo pas√°is manualmente)\n",
    "- Busca dentro de `runtime.state` para encontrar todos los mensajes en la conversaci√≥n\n",
    "- Cuenta cu√°ntos mensajes vinieron del humano, la IA y las herramientas\n",
    "- Devuelve un resumen como \"La conversaci√≥n tiene 3 mensajes de usuario, 3 respuestas de IA, y 2 resultados de herramientas\"\n",
    "\n",
    "\n",
    "#### Ejemplo 2: Obteniendo Preferencias del Usuario\n",
    "\n",
    "```python\n",
    "# Acceder a campos de estado personalizados\n",
    "@tool\n",
    "def get_user_preference(\n",
    "    pref_name: str,\n",
    "    runtime: ToolRuntime  # El par√°metro ToolRuntime no es visible para el modelo\n",
    ") -> str:\n",
    "    \"\"\"Obtener un valor de preferencia del usuario.\"\"\"\n",
    "    preferences = runtime.state.get(\"user_preferences\", {})\n",
    "    return preferences.get(pref_name, \"No establecido\")\n",
    "```\n",
    "\n",
    "**Qu√© est√° pasando aqu√≠:**\n",
    "- Esta herramienta toma DOS par√°metros: `pref_name` (qu√© preferencia buscar) y `runtime`\n",
    "- **Importante**: El modelo de IA solo \"ve\" `pref_name` - no sabe sobre `runtime`\n",
    "- `runtime` es proporcionado autom√°ticamente por el sistema entre bastidores\n",
    "- La herramienta busca en el estado de la conversaci√≥n las preferencias del usuario\n",
    "- Si la preferencia existe, devuelve el valor; de lo contrario devuelve \"No establecido\"\n",
    "\n",
    "\n",
    "#### Conclusi√≥n Clave\n",
    "\n",
    "`ToolRuntime` es como tener un **ayudante especial** que da a vuestra herramienta acceso a todo el contexto que necesita sin saturar los par√°metros visibles de vuestra herramienta. Es proporcionado autom√°ticamente, la IA no lo ve, y hace vuestras herramientas mucho m√°s potentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5bf1b5-4517-4d78-82f4-ed12f27ce3c0",
   "metadata": {},
   "source": [
    "## Ejemplo B√°sico: usando ToolRuntime para acceder a Contexto Personalizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a556e3-a846-4ba4-8618-f8fbde4277f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# Aqu√≠ es donde creamos el contexto personalizado\n",
    "@dataclass\n",
    "class Preferences:\n",
    "    vehicle: str = \"Vespa\"\n",
    "    city: str = \"San Francisco\"\n",
    "\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Aqu√≠ es donde usamos el contexto personalizado\n",
    "@tool\n",
    "def get_preferred_vehicle(runtime: ToolRuntime[Preferences]) -> str:\n",
    "    \"\"\"Obtener el veh√≠culo preferido\"\"\"\n",
    "    return runtime.context.vehicle\n",
    "\n",
    "@tool\n",
    "def get_preferred_city(runtime: ToolRuntime[Preferences]) -> str:\n",
    "    \"\"\"Obtener la ciudad preferida\"\"\"\n",
    "    return runtime.context.city\n",
    "\n",
    "# Aqu√≠ es donde a√±adimos las herramientas y el contexto personalizado al agente\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[get_preferred_vehicle, get_preferred_city],\n",
    "    context_schema=Preferences\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"¬øCu√°l es el veh√≠culo preferido?\")]},\n",
    "    context=Preferences()\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeebe1c8-5e2f-444e-839e-22abad49f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387c7f7-a64d-4b9f-a320-2929e34b3f84",
   "metadata": {},
   "source": [
    "## Expliquemos el c√≥digo anterior en t√©rminos sencillos\n",
    "\n",
    "A continuaci√≥n se muestra una **explicaci√≥n amigable para principiantes, l√≠nea por l√≠nea** del c√≥digo. Explicaremos **qu√© hace cada parte** y **por qu√© existe**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Importando `dataclass`\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "```\n",
    "\n",
    "* Esto importa `dataclass`, una funci√≥n auxiliar de Python que facilita la creaci√≥n de clases simples usadas para almacenar datos.\n",
    "* Pensad en una `dataclass` como un **contenedor limpio para variables**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Creando un contexto personalizado (memoria compartida)\n",
    "\n",
    "```python\n",
    "# Aqu√≠ es donde creamos el contexto personalizado\n",
    "\n",
    "@dataclass\n",
    "class Preferences:\n",
    "```\n",
    "\n",
    "* Est√°is definiendo un **objeto de contexto personalizado** llamado `Preferences`.\n",
    "* \"Contexto\" significa **informaci√≥n a la que el agente y las herramientas pueden acceder mientras se ejecutan**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    vehicle: str = \"Vespa\"\n",
    "    city: str = \"San Francisco\"\n",
    "```\n",
    "\n",
    "* Este contexto tiene dos campos:\n",
    "\n",
    "  * `vehicle` ‚Üí valor por defecto `\"Vespa\"`\n",
    "  * `city` ‚Üí valor por defecto `\"San Francisco\"`\n",
    "* Si no proporcion√°is nada m√°s, se usan estos valores por defecto.\n",
    "* Pod√©is pensar en esto como un **perfil de usuario u objeto de configuraci√≥n**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Importando utilidades de herramientas de LangChain\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "```\n",
    "\n",
    "* `@tool` es un decorador que convierte una funci√≥n de Python en una **herramienta que un agente puede llamar**.\n",
    "* `ToolRuntime` es un objeto que LangChain pasa a las herramientas para que puedan:\n",
    "\n",
    "  * Acceder al contexto\n",
    "  * Acceder a datos de ejecuci√≥n (como memoria, configuraci√≥n, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Creando una herramienta que lee del contexto\n",
    "\n",
    "```python\n",
    "# Aqu√≠ es donde usamos el contexto personalizado\n",
    "\n",
    "@tool\n",
    "def get_preferred_vehicle(runtime: ToolRuntime[Preferences]) -> str:\n",
    "```\n",
    "\n",
    "* Esto define una **herramienta** llamada `get_preferred_vehicle`.\n",
    "* `@tool` le dice a LangChain:\n",
    "  üëâ \"El agente puede llamar a esta funci√≥n.\"\n",
    "* `runtime: ToolRuntime[Preferences]` significa:\n",
    "\n",
    "  * LangChain pasar√° un objeto `runtime`\n",
    "  * Ese runtime **contiene un contexto `Preferences`**\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    \"\"\"Obtener el veh√≠culo preferido\"\"\"\n",
    "```\n",
    "\n",
    "* Esta es una docstring.\n",
    "* LangChain usa esta descripci√≥n para ayudar al LLM a entender **cu√°ndo usar la herramienta**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    return runtime.context.vehicle\n",
    "```\n",
    "\n",
    "* `runtime.context` es el objeto `Preferences` actual.\n",
    "* `.vehicle` lee el campo `vehicle`.\n",
    "* Esto devuelve `\"Vespa\"` (a menos que se haya cambiado el contexto).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Segunda herramienta: ciudad preferida\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def get_preferred_city(runtime: ToolRuntime[Preferences]) -> str:\n",
    "```\n",
    "\n",
    "* Misma idea que antes, pero para la ciudad.\n",
    "* Tambi√©n recibe el runtime y el contexto.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    \"\"\"Obtener la ciudad preferida\"\"\"\n",
    "```\n",
    "\n",
    "* Descripci√≥n de la herramienta para el LLM.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    return runtime.context.city\n",
    "```\n",
    "\n",
    "* Lee `\"San Francisco\"` del contexto y lo devuelve.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Creando el agente\n",
    "\n",
    "```python\n",
    "# Aqu√≠ es donde a√±adimos las herramientas y el contexto personalizado al agente\n",
    "\n",
    "agent = create_agent(\n",
    "```\n",
    "\n",
    "* Esto crea un **agente de IA**.\n",
    "* Un agente puede:\n",
    "\n",
    "  * Leer mensajes\n",
    "  * Decidir qu√© herramientas llamar\n",
    "  * Producir una respuesta final\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    model=\"gpt-4o-mini\",\n",
    "```\n",
    "\n",
    "* Esto le dice al agente qu√© modelo de lenguaje usar.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    tools=[get_preferred_vehicle, get_preferred_city],\n",
    "```\n",
    "\n",
    "* Estas son las **√∫nicas herramientas que el agente puede usar**.\n",
    "* El agente puede llamarlas si piensa que ayudan a responder la pregunta.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    context_schema=Preferences\n",
    "```\n",
    "\n",
    "* Esto es muy importante.\n",
    "* Le dice a LangChain:\n",
    "\n",
    "  * \"Este agente usa un contexto con forma de `Preferences`\"\n",
    "* LangChain ahora sabe qu√© campos existen en `runtime.context`.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Ejecutando el agente\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "```\n",
    "\n",
    "* Esto ejecuta el agente.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    {\"messages\": [HumanMessage(content=\"¬øCu√°l es el veh√≠culo preferido?\")]},\n",
    "```\n",
    "\n",
    "* Le envi√°is un **mensaje de chat** al agente.\n",
    "* El usuario est√° preguntando:\n",
    "  üëâ \"¬øCu√°l es el veh√≠culo preferido?\"\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    context=Preferences()\n",
    "```\n",
    "\n",
    "* Proporcion√°is una **instancia del contexto**.\n",
    "* Como no hab√©is sobrescrito nada:\n",
    "\n",
    "  * `vehicle = \"Vespa\"`\n",
    "  * `city = \"San Francisco\"`\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Imprimiendo la respuesta final\n",
    "\n",
    "```python\n",
    "print(response['messages'][-1].content)\n",
    "```\n",
    "\n",
    "* `response['messages']` es el historial de la conversaci√≥n.\n",
    "* `[-1]` significa \"el √∫ltimo mensaje\" (la respuesta del agente).\n",
    "* `.content` es el texto de la respuesta.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Qu√© pasa internamente (versi√≥n simple)\n",
    "\n",
    "1. El agente recibe la pregunta\n",
    "   **\"¬øCu√°l es el veh√≠culo preferido?\"**\n",
    "2. El modelo ve que:\n",
    "\n",
    "   * Hay una herramienta llamada `get_preferred_vehicle`\n",
    "   * Esa herramienta devuelve exactamente lo que pide la pregunta\n",
    "3. El agente llama a la herramienta\n",
    "4. La herramienta lee `runtime.context.vehicle`\n",
    "5. El agente responde con algo como:\n",
    "\n",
    "```\n",
    "El veh√≠culo preferido es una Vespa.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. Modelo mental de una frase\n",
    "\n",
    "> **Este c√≥digo ense√±a a un agente de IA c√≥mo leer preferencias de usuario estructuradas (contexto) usando herramientas y responder preguntas sobre ellas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d74bb6-04a7-46fc-8d77-2dfb89b41349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
