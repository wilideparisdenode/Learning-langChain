{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangSmith in the LangChain 1.0 Era: A Beginner's Guide\n",
    "\n",
    "## Introduction\n",
    "\n",
    "What exactly is LangSmith? How does it help you? And how has it evolved to support the new LangChain 1.0 and LangGraph 1.0 ecosystem?\n",
    "\n",
    "This notebook will explain everything in simple terms, with clear examples and analogies.\n",
    "\n",
    "### Think of it this way:\n",
    "\n",
    "- **LangChain/LangGraph** = Your car (the application you build)\n",
    "- **LangSmith** = Your car's dashboard + mechanic shop + test track\n",
    "\n",
    "Just like a car dashboard shows you speed, fuel, and engine status, LangSmith shows you what's happening inside your LLM application. And just like a mechanic shop helps you fix problems, LangSmith helps you debug and improve your agents.\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: What is LangSmith?\n",
    "\n",
    "### The Simple Answer\n",
    "\n",
    "**LangSmith is a platform that helps you build better LLM applications by showing you what's happening, testing if it works, and making it easy to deploy.**\n",
    "\n",
    "### The Three Pillars of LangSmith\n",
    "\n",
    "LangSmith has three main superpowers:\n",
    "\n",
    "1. **üëÅÔ∏è Observability (\"See what's happening\")** - Tracing and monitoring\n",
    "2. **‚úÖ Evaluation (\"Test if it works\")** - Quality assurance and testing\n",
    "3. **üöÄ Deployment (\"Ship it to production\")** - Hosting and scaling\n",
    "\n",
    "Let's explore each one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Pillar 1 - Observability (See What's Happening)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Imagine you built an agent that:\n",
    "1. Searches the web\n",
    "2. Reads a document\n",
    "3. Generates an answer\n",
    "\n",
    "But sometimes it gives wrong answers. **Why?**\n",
    "\n",
    "- Is the search returning bad results?\n",
    "- Is the LLM misunderstanding the document?\n",
    "- Is the prompt confusing?\n",
    "\n",
    "**Without LangSmith, you're debugging blind.** üôà\n",
    "\n",
    "### The Solution: Tracing\n",
    "\n",
    "LangSmith **records every single step** your agent takes. This is called **tracing**.\n",
    "\n",
    "Think of it like a flight recorder (black box) in an airplane - it records everything so you can understand what happened.\n",
    "\n",
    "### What Gets Traced?\n",
    "\n",
    "LangSmith records:\n",
    "- üìù **Every prompt** sent to the LLM\n",
    "- üí¨ **Every response** from the LLM\n",
    "- üîß **Every tool call** (search, database query, etc.)\n",
    "- üìä **Results** from each tool\n",
    "- ‚è±Ô∏è **How long** each step took\n",
    "- üí∞ **How much it cost** (tokens used)\n",
    "- ‚ùå **Any errors** that occurred\n",
    "\n",
    "### Example: Simple Agent Trace\n",
    "\n",
    "Let's see what a trace looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Enable LangSmith tracing\n",
    "import os\n",
    "\n",
    "# Set these environment variables to enable tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"your-api-key-here\"  # Get from smith.langchain.com\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"my-first-project\"  # Optional: organize traces\n",
    "\n",
    "# That's it! Now all your LangChain code will automatically trace to LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple agent that searches and answers\n",
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define a simple search tool\n",
    "@tool\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Search the web for information.\"\"\"\n",
    "    # Simulated search\n",
    "    return f\"Search results for: {query}\\nFound: LangChain is a framework for LLM apps.\"\n",
    "\n",
    "# Create an agent\n",
    "model = init_chat_model(\"gpt-4o\")\n",
    "agent = create_agent(model, tools=[search_web])\n",
    "\n",
    "# Run the agent - THIS WILL AUTOMATICALLY TRACE TO LANGSMITH!\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is LangChain?\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens in LangSmith:**\n",
    "\n",
    "When you run this code, LangSmith creates a **trace** that shows:\n",
    "\n",
    "```\n",
    "Run: Agent Execution\n",
    "  ‚îú‚îÄ Input: \"What is LangChain?\"\n",
    "  ‚îú‚îÄ LLM Call #1\n",
    "  ‚îÇ   ‚îú‚îÄ Prompt: \"You are a helpful assistant... What is LangChain?\"\n",
    "  ‚îÇ   ‚îú‚îÄ Response: Tool call to search_web(\"LangChain\")\n",
    "  ‚îÇ   ‚îú‚îÄ Tokens: 150\n",
    "  ‚îÇ   ‚îî‚îÄ Duration: 1.2s\n",
    "  ‚îú‚îÄ Tool Call: search_web\n",
    "  ‚îÇ   ‚îú‚îÄ Input: \"LangChain\"\n",
    "  ‚îÇ   ‚îú‚îÄ Output: \"LangChain is a framework...\"\n",
    "  ‚îÇ   ‚îî‚îÄ Duration: 0.3s\n",
    "  ‚îú‚îÄ LLM Call #2\n",
    "  ‚îÇ   ‚îú‚îÄ Prompt: \"You are a helpful assistant... [search results]\"\n",
    "  ‚îÇ   ‚îú‚îÄ Response: \"LangChain is a framework for building LLM applications...\"\n",
    "  ‚îÇ   ‚îú‚îÄ Tokens: 200\n",
    "  ‚îÇ   ‚îî‚îÄ Duration: 1.5s\n",
    "  ‚îî‚îÄ Output: Final answer\n",
    "  \n",
    "Total Duration: 3.0s\n",
    "Total Cost: $0.002\n",
    "```\n",
    "\n",
    "**You can see EVERYTHING!** This is incredibly powerful for debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: Selective Tracing\n",
    "\n",
    "Sometimes you don't want to trace EVERYTHING (especially in production). LangSmith lets you be selective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tracing import tracing_context\n",
    "\n",
    "# Only trace specific calls\n",
    "with tracing_context(enabled=True, project_name=\"production-debug\"):\n",
    "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Test query\"}]})\n",
    "    # This will be traced\n",
    "\n",
    "# This won't be traced\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Another query\"}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata and Tags\n",
    "\n",
    "You can add custom information to traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata to help filter and analyze traces\n",
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is LangChain?\"}]},\n",
    "    config={\n",
    "        \"metadata\": {\n",
    "            \"user_id\": \"user123\",\n",
    "            \"session_id\": \"session456\",\n",
    "            \"environment\": \"production\"\n",
    "        },\n",
    "        \"tags\": [\"customer-support\", \"premium-user\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Now you can filter traces by user, session, or tag in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Benefits of Tracing\n",
    "\n",
    "1. **üêõ Debug faster** - See exactly where things go wrong\n",
    "2. **‚ö° Optimize performance** - Find slow steps\n",
    "3. **üí∞ Control costs** - See which LLM calls are expensive\n",
    "4. **üìä Understand behavior** - See how your agent makes decisions\n",
    "5. **üîç Monitor production** - Catch issues before users complain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: LangSmith Studio (now the LangChain Team is proposing Agent Builder as a better alternative)\n",
    "\n",
    "### What is Studio?\n",
    "\n",
    "**LangSmith Studio is a FREE visual interface for developing and testing your agents locally.**\n",
    "\n",
    "Think of it as:\n",
    "- A **playground** where you can test your agent\n",
    "- A **debugger** that shows you every step in real-time\n",
    "- A **time machine** that lets you replay conversations\n",
    "\n",
    "### Key Features\n",
    "\n",
    "1. **üëÅÔ∏è Real-time visualization** - See your agent's execution as it happens\n",
    "2. **üîÑ Hot reloading** - Change your code and see updates instantly\n",
    "3. **‚èÆÔ∏è Thread replay** - Re-run conversations from any point\n",
    "4. **üî¨ Step-by-step inspection** - Examine every prompt, tool call, and response\n",
    "5. **üìä Metrics display** - See token counts, latency, and costs\n",
    "\n",
    "### Setup Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install the LangGraph CLI\n",
    "# Run in terminal:\n",
    "# pip install langgraph-cli\n",
    "\n",
    "# 2. Create a langgraph.json config file\n",
    "# In your project directory, create: langgraph.json\n",
    "'''\n",
    "{\n",
    "  \"dependencies\": [\".\"],\n",
    "  \"graphs\": {\n",
    "    \"my_agent\": \"./my_agent.py:agent\"\n",
    "  },\n",
    "  \"env\": \".env\"\n",
    "}\n",
    "'''\n",
    "\n",
    "# 3. Run the development server\n",
    "# In terminal:\n",
    "# langgraph dev\n",
    "\n",
    "# 4. Open the UI at http://localhost:8123\n",
    "# Now you have a visual interface to test your agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Studio\n",
    "\n",
    "Once Studio is running, you can:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  LangSmith Studio                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Input: \"What is LangChain?\"           ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ Step 1: Agent Decision            ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Prompt: \"You are a helpful...\"    ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Response: Call search_web tool    ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Tokens: 150 | Time: 1.2s          ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ Step 2: Tool Execution            ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Tool: search_web                  ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Input: \"LangChain\"                ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Output: \"LangChain is...\"         ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Time: 0.3s                        ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ\n",
    "‚îÇ  ‚îÇ Step 3: Final Answer              ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Prompt: \"Based on search...\"      ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Response: \"LangChain is a...\"     ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îÇ Tokens: 200 | Time: 1.5s          ‚îÇ ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Total Time: 3.0s | Total Cost: $0.002 ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "**You can click on each step to see:**\n",
    "- The exact prompt sent to the LLM\n",
    "- The full response\n",
    "- All metadata\n",
    "- Any errors\n",
    "\n",
    "**And you can:**\n",
    "- Re-run from any step\n",
    "- Try different inputs\n",
    "- Compare multiple runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Studio is Amazing for Beginners\n",
    "\n",
    "When you're learning, Studio helps you:\n",
    "\n",
    "1. **Understand how agents work** - See the decision-making process visually\n",
    "2. **Debug without frustration** - No more `print()` debugging!\n",
    "3. **Experiment quickly** - Test different prompts and see results immediately\n",
    "4. **Learn from mistakes** - See exactly what went wrong\n",
    "\n",
    "**It's like having a teacher show you step-by-step how your code works!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Pillar 2 - Evaluation (Test If It Works)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "You built an agent. It works on your test input. **But:**\n",
    "\n",
    "- Does it work on 100 different inputs?\n",
    "- Is it better than version 1?\n",
    "- Will it work for your users?\n",
    "\n",
    "**Testing manually is slow and unreliable.** You need automated evaluation.\n",
    "\n",
    "### The Solution: Datasets and Evaluators\n",
    "\n",
    "LangSmith helps you:\n",
    "1. Create **test datasets** (collections of inputs and expected outputs)\n",
    "2. Run your agent on the entire dataset\n",
    "3. Automatically **evaluate** the quality of responses\n",
    "4. Track improvements over time\n",
    "\n",
    "### Creating a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a dataset of test cases\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"langchain-qa-test\",\n",
    "    description=\"Test cases for LangChain Q&A agent\"\n",
    ")\n",
    "\n",
    "# Add examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is LangChain?\"},\n",
    "        \"expected_output\": \"LangChain is a framework for building LLM applications.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"What is LangGraph?\"},\n",
    "        \"expected_output\": \"LangGraph is a library for building stateful, multi-agent applications.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": {\"question\": \"How do I install LangChain?\"},\n",
    "        \"expected_output\": \"You can install LangChain using: pip install langchain\"\n",
    "    },\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    client.create_example(\n",
    "        inputs=example[\"input\"],\n",
    "        outputs={\"answer\": example[\"expected_output\"]},\n",
    "        dataset_id=dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define how to run your agent\n",
    "def run_agent(inputs):\n",
    "    result = agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": inputs[\"question\"]}]\n",
    "    })\n",
    "    return {\"answer\": result[\"messages\"][-1].content}\n",
    "\n",
    "# Define how to evaluate responses\n",
    "def evaluate_answer(run, example):\n",
    "    \"\"\"Check if the answer is correct.\"\"\"\n",
    "    predicted = run.outputs[\"answer\"]\n",
    "    expected = example.outputs[\"answer\"]\n",
    "    \n",
    "    # Simple check: does the answer contain key information?\n",
    "    score = 1.0 if expected.lower() in predicted.lower() else 0.0\n",
    "    \n",
    "    return {\"key\": \"correctness\", \"score\": score}\n",
    "\n",
    "# Run evaluation on the entire dataset\n",
    "results = evaluate(\n",
    "    run_agent,\n",
    "    data=\"langchain-qa-test\",\n",
    "    evaluators=[evaluate_answer],\n",
    "    experiment_prefix=\"agent-v1\"\n",
    ")\n",
    "\n",
    "# Results show:\n",
    "# - How many passed/failed\n",
    "# - Average score\n",
    "# - Detailed results for each example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: LLM-as-Judge\n",
    "\n",
    "Instead of writing rules, you can use an LLM to evaluate responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# Use an LLM to judge if the answer is helpful\n",
    "evaluator = load_evaluator(\n",
    "    \"labeled_criteria\",\n",
    "    criteria=\"helpfulness\",\n",
    "    llm=model\n",
    ")\n",
    "\n",
    "def llm_evaluate(run, example):\n",
    "    result = evaluator.evaluate_strings(\n",
    "        input=example.inputs[\"question\"],\n",
    "        prediction=run.outputs[\"answer\"],\n",
    "        reference=example.outputs[\"answer\"]\n",
    "    )\n",
    "    return {\n",
    "        \"key\": \"helpfulness\",\n",
    "        \"score\": result[\"score\"],\n",
    "        \"comment\": result[\"reasoning\"]\n",
    "    }\n",
    "\n",
    "# Now the LLM judges if responses are helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Versions\n",
    "\n",
    "LangSmith lets you compare different versions of your agent:\n",
    "\n",
    "```\n",
    "Version 1 (old prompt):\n",
    "  Correctness: 70%\n",
    "  Helpfulness: 3.5/5\n",
    "  Avg Response Time: 2.5s\n",
    "\n",
    "Version 2 (new prompt):\n",
    "  Correctness: 85%  ‚úÖ Improved!\n",
    "  Helpfulness: 4.2/5  ‚úÖ Improved!\n",
    "  Avg Response Time: 2.1s  ‚úÖ Faster!\n",
    "```\n",
    "\n",
    "This helps you make **data-driven decisions** about which version is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Benefits of Evaluation\n",
    "\n",
    "1. **üéØ Catch regressions** - Make sure updates don't break things\n",
    "2. **üìà Track improvements** - See if your changes actually help\n",
    "3. **üèÜ Compare approaches** - Test different prompts, models, or architectures\n",
    "4. **‚úÖ Quality assurance** - Ensure consistent quality before deployment\n",
    "5. **üí° Learn patterns** - Understand what works and what doesn't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Pillar 3 - Deployment (Ship It to Production)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Your agent works great on your laptop. Now you need to:\n",
    "- Make it available 24/7\n",
    "- Handle many users at once\n",
    "- Keep it fast and reliable\n",
    "- Monitor it in production\n",
    "\n",
    "**Traditional hosting platforms aren't built for stateful agents.** They're designed for simple web apps.\n",
    "\n",
    "### The Solution: LangSmith Deployments\n",
    "\n",
    "LangSmith provides **managed hosting specifically designed for LangGraph agents**.\n",
    "\n",
    "### Why Special Hosting for Agents?\n",
    "\n",
    "Traditional web hosting:\n",
    "```\n",
    "Request ‚Üí Process ‚Üí Response\n",
    "(Short-lived, stateless)\n",
    "```\n",
    "\n",
    "LangGraph agents:\n",
    "```\n",
    "Request ‚Üí Think ‚Üí Call Tool ‚Üí Wait ‚Üí Think ‚Üí Call Tool ‚Üí Response\n",
    "(Long-running, stateful)\n",
    "```\n",
    "\n",
    "LangSmith handles:\n",
    "- **State persistence** - Remember conversation history\n",
    "- **Background execution** - Agents can take minutes to complete\n",
    "- **Scaling** - Handle many concurrent users\n",
    "- **Streaming** - Send partial results as they're generated\n",
    "\n",
    "### Deploying Your Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deployment Process (Done through LangSmith UI):\n",
    "\n",
    "# 1. Push your code to GitHub\n",
    "# Your repository should have:\n",
    "# - Your agent code\n",
    "# - langgraph.json configuration\n",
    "# - requirements.txt with dependencies\n",
    "\n",
    "# 2. Connect GitHub to LangSmith\n",
    "# Go to smith.langchain.com ‚Üí Deployments ‚Üí Connect GitHub\n",
    "\n",
    "# 3. Select your repository and deploy\n",
    "# LangSmith will:\n",
    "# - Clone your repository\n",
    "# - Install dependencies\n",
    "# - Build your agent\n",
    "# - Deploy to infrastructure\n",
    "# (Takes ~15 minutes)\n",
    "\n",
    "# 4. Get your deployment URL\n",
    "# https://your-agent.langsmith.app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Your Deployed Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Using the Python SDK\n",
    "from langgraph_sdk import get_client\n",
    "\n",
    "client = get_client(url=\"https://your-agent.langsmith.app\")\n",
    "\n",
    "# Create a thread (conversation)\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# Send a message\n",
    "response = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"my_agent\",\n",
    "    input={\"messages\": [{\"role\": \"user\", \"content\": \"What is LangChain?\"}]}\n",
    ")\n",
    "\n",
    "# Stream the response\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    response[\"run_id\"]\n",
    "):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Using REST API (from any language)\n",
    "import requests\n",
    "\n",
    "url = \"https://your-agent.langsmith.app/threads\"\n",
    "\n",
    "# Create thread\n",
    "response = requests.post(url)\n",
    "thread_id = response.json()[\"thread_id\"]\n",
    "\n",
    "# Send message\n",
    "response = requests.post(\n",
    "    f\"{url}/{thread_id}/runs\",\n",
    "    json={\n",
    "        \"assistant_id\": \"my_agent\",\n",
    "        \"input\": {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"What is LangChain?\"}]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Monitoring\n",
    "\n",
    "Once deployed, LangSmith automatically:\n",
    "- ‚úÖ Traces all production requests\n",
    "- ‚úÖ Monitors performance and errors\n",
    "- ‚úÖ Tracks costs\n",
    "- ‚úÖ Shows usage analytics\n",
    "\n",
    "You can:\n",
    "- See which users are having problems\n",
    "- Find slow or expensive requests\n",
    "- Set up alerts for errors\n",
    "- Roll back to previous versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Benefits of Deployment\n",
    "\n",
    "1. **üöÄ Fast deployment** - From GitHub to production in 15 minutes\n",
    "2. **üìà Auto-scaling** - Handles traffic spikes automatically\n",
    "3. **üí™ Built for agents** - Supports long-running, stateful workflows\n",
    "4. **üìä Monitoring included** - Traces and metrics out of the box\n",
    "5. **üîß Easy updates** - Push to GitHub, automatic redeployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: How LangSmith Has Evolved\n",
    "\n",
    "### The Old Days (Pre-2024)\n",
    "\n",
    "**LangSmith 0.x:**\n",
    "- Primarily a **tracing tool**\n",
    "- Manual setup required\n",
    "- Limited evaluation features\n",
    "- No deployment capabilities\n",
    "- Basic UI\n",
    "\n",
    "**Development workflow:**\n",
    "```\n",
    "Code ‚Üí Test locally ‚Üí Deploy to separate platform ‚Üí Hope it works\n",
    "```\n",
    "\n",
    "### The New Era (LangChain 1.0 & LangGraph 1.0)\n",
    "\n",
    "**LangSmith in 2024-2025:**\n",
    "- **Complete platform** for the full development lifecycle\n",
    "- Automatic tracing for `create_agent`\n",
    "- Rich evaluation framework\n",
    "- Integrated deployment\n",
    "- Studio for local development\n",
    "- Production monitoring\n",
    "\n",
    "**New workflow:**\n",
    "```\n",
    "Develop in Studio ‚Üí Evaluate with datasets ‚Üí Deploy with one click ‚Üí Monitor in production\n",
    "```\n",
    "\n",
    "All in one platform!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Evolution Milestones\n",
    "\n",
    "#### 1. Studio Launch (2024)\n",
    "- **Before:** Test agents by running code repeatedly\n",
    "- **After:** Visual interface with hot reloading and replay\n",
    "\n",
    "#### 2. Automatic Tracing for `create_agent`\n",
    "- **Before:** Manual instrumentation required\n",
    "- **After:** Just set environment variables, tracing works automatically\n",
    "\n",
    "#### 3. Evaluation Framework\n",
    "- **Before:** Write custom test scripts\n",
    "- **After:** Built-in datasets, evaluators, and comparison tools\n",
    "\n",
    "#### 4. LangSmith Deployments\n",
    "- **Before:** Deploy to AWS/GCP/Azure manually\n",
    "- **After:** Deploy from GitHub in 15 minutes\n",
    "\n",
    "#### 5. Subgraph Tracing\n",
    "- **Before:** Only see top-level agent traces\n",
    "- **After:** See inside nested subgraphs and multi-agent systems\n",
    "\n",
    "#### 6. Thread Management\n",
    "- **Before:** Manage state manually\n",
    "- **After:** Built-in thread and checkpoint management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with LangChain 1.0\n",
    "\n",
    "LangSmith is now **deeply integrated** with LangChain 1.0:\n",
    "\n",
    "```python\n",
    "# LangChain 0.x\n",
    "from langchain.callbacks import LangChainTracer\n",
    "tracer = LangChainTracer()\n",
    "# ... manual setup ...\n",
    "\n",
    "# LangChain 1.0\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# That's it! Everything traces automatically\n",
    "```\n",
    "\n",
    "No more callbacks, no more manual instrumentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with LangGraph 1.0\n",
    "\n",
    "LangGraph 1.0 is **designed for LangSmith**:\n",
    "\n",
    "- **Checkpointing** - LangSmith stores conversation state\n",
    "- **Subgraph visibility** - See every level of multi-agent systems\n",
    "- **Human-in-the-loop** - Built-in interrupt and resume\n",
    "- **Studio support** - Visualize complex graphs\n",
    "- **Deployment optimized** - LangGraph apps deploy seamlessly\n",
    "\n",
    "```python\n",
    "# LangGraph automatically works with LangSmith\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "graph = StateGraph(...)\n",
    "# ... build graph ...\n",
    "graph = graph.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Stream with subgraph visibility\n",
    "for chunk in graph.stream(input, subgraphs=True):\n",
    "    print(chunk)\n",
    "    # All of this is traced in LangSmith!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Complete Workflow Example\n",
    "\n",
    "Let's see how everything comes together in a real project:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Development (Using Studio)\n",
    "\n",
    "```bash\n",
    "# Terminal\n",
    "langgraph dev\n",
    "# Opens Studio at http://localhost:8123\n",
    "```\n",
    "\n",
    "In Studio:\n",
    "1. Test different prompts visually\n",
    "2. See what works and what doesn't\n",
    "3. Fix bugs by seeing exact execution\n",
    "4. Hot reload changes instantly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Evaluation (Creating Test Suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "dataset = client.create_dataset(\"production-test-suite\")\n",
    "\n",
    "# Add 100 test cases\n",
    "test_cases = [\n",
    "    {\"question\": \"What is LangChain?\", \"expected\": \"...\"},\n",
    "    {\"question\": \"How do I use tools?\", \"expected\": \"...\"},\n",
    "    # ... 98 more ...\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": case[\"question\"]},\n",
    "        outputs={\"answer\": case[\"expected\"]},\n",
    "        dataset_id=dataset.id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "results = evaluate(\n",
    "    lambda x: agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": x[\"question\"]}]}),\n",
    "    data=\"production-test-suite\",\n",
    "    evaluators=[correctness_evaluator, helpfulness_evaluator],\n",
    "    experiment_prefix=\"pre-deployment\"\n",
    ")\n",
    "\n",
    "print(f\"Correctness: {results['correctness']}%\")\n",
    "print(f\"Helpfulness: {results['helpfulness']}/5\")\n",
    "\n",
    "# Only deploy if scores are good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to GitHub\n",
    "# git push origin main\n",
    "\n",
    "# In LangSmith UI:\n",
    "# 1. Go to Deployments\n",
    "# 2. Select repository\n",
    "# 3. Click Deploy\n",
    "# 4. Wait 15 minutes\n",
    "# 5. Get deployment URL\n",
    "\n",
    "# Now your agent is live!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 4: Production Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All production requests are automatically traced\n",
    "# Go to LangSmith UI to see:\n",
    "\n",
    "# Dashboard:\n",
    "# - Total requests: 10,000\n",
    "# - Success rate: 98%\n",
    "# - Average latency: 2.3s\n",
    "# - Total cost: $45.67\n",
    "\n",
    "# Errors:\n",
    "# - 200 requests failed\n",
    "# - Click to see traces\n",
    "# - Identify the problem\n",
    "# - Fix and redeploy\n",
    "\n",
    "# User feedback:\n",
    "# - 95% positive\n",
    "# - See specific issues\n",
    "# - Improve based on feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 5: Continuous Improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use production traces to improve\n",
    "\n",
    "# 1. Find problematic traces\n",
    "# LangSmith UI: Filter by \"Status: Error\" or \"Duration > 10s\"\n",
    "\n",
    "# 2. Add them to test dataset\n",
    "client.create_examples_from_traces(\n",
    "    trace_ids=[\"trace-1\", \"trace-2\", \"trace-3\"],\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "\n",
    "# 3. Fix the issues\n",
    "# Update prompts, add error handling, etc.\n",
    "\n",
    "# 4. Re-evaluate\n",
    "results = evaluate(..., experiment_prefix=\"v2\")\n",
    "\n",
    "# 5. Compare versions\n",
    "# LangSmith UI: Compare v1 vs v2\n",
    "# See: v2 is 10% better!\n",
    "\n",
    "# 6. Deploy v2\n",
    "# git push origin main\n",
    "\n",
    "# 7. Repeat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: LangSmith Pricing & Plans\n",
    "\n",
    "### Free Tier (Great for Learning!)\n",
    "\n",
    "**Includes:**\n",
    "- ‚úÖ Studio (unlimited local development)\n",
    "- ‚úÖ Tracing (up to 5,000 traces/month)\n",
    "- ‚úÖ Basic evaluation\n",
    "- ‚úÖ Small datasets\n",
    "\n",
    "**Perfect for:**\n",
    "- Learning LangChain and LangGraph\n",
    "- Personal projects\n",
    "- Prototypes\n",
    "- Small side projects\n",
    "\n",
    "### Paid Tiers (For Production)\n",
    "\n",
    "**Developer ($39/month):**\n",
    "- More traces\n",
    "- Larger datasets\n",
    "- Advanced evaluations\n",
    "- Team features\n",
    "\n",
    "**Team ($199/month):**\n",
    "- Unlimited traces\n",
    "- Deployments included\n",
    "- Priority support\n",
    "- SSO and security features\n",
    "\n",
    "**Enterprise (Custom pricing):**\n",
    "- Self-hosted option\n",
    "- Custom SLAs\n",
    "- Dedicated support\n",
    "- Advanced compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Getting Started - Step by Step\n",
    "\n",
    "### Step 1: Sign Up (Free)\n",
    "\n",
    "1. Go to [smith.langchain.com](https://smith.langchain.com)\n",
    "2. Click \"Sign Up\"\n",
    "3. Create an account (free!)\n",
    "4. Get your API key\n",
    "\n",
    "### Step 2: Enable Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your code or .env file\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"lsv2_pt_...\"  # From smith.langchain.com\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"my-first-project\"\n",
    "\n",
    "# That's it! Now run your LangChain code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run Some Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4o\")\n",
    "agent = create_agent(model, tools=[])\n",
    "\n",
    "# This will be traced!\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n",
    "})\n",
    "\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: View Your First Trace\n",
    "\n",
    "1. Go to [smith.langchain.com](https://smith.langchain.com)\n",
    "2. Click on your project\n",
    "3. See your trace!\n",
    "4. Click to explore\n",
    "\n",
    "**You'll see:**\n",
    "- The exact prompt\n",
    "- The model's response\n",
    "- How long it took\n",
    "- How much it cost\n",
    "- All metadata\n",
    "\n",
    "### Step 5: Set Up Studio (Optional but Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CLI\n",
    "# pip install langgraph-cli\n",
    "\n",
    "# Create config file: langgraph.json\n",
    "'''\n",
    "{\n",
    "  \"dependencies\": [\".\"],\n",
    "  \"graphs\": {\n",
    "    \"my_agent\": \"./agent.py:agent\"\n",
    "  },\n",
    "  \"env\": \".env\"\n",
    "}\n",
    "'''\n",
    "\n",
    "# Run\n",
    "# langgraph dev\n",
    "\n",
    "# Open http://localhost:8123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Experiment!\n",
    "\n",
    "Now you have everything set up. Try:\n",
    "\n",
    "1. **Build a simple agent**\n",
    "2. **Watch traces** in LangSmith\n",
    "3. **Test in Studio**\n",
    "4. **Create a test dataset**\n",
    "5. **Run evaluations**\n",
    "6. **Improve based on results**\n",
    "\n",
    "You're learning by doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Common Questions\n",
    "\n",
    "### Q: Do I need LangSmith to use LangChain?\n",
    "\n",
    "**A:** No! LangChain works fine without LangSmith. But LangSmith makes development much easier by showing you what's happening inside your agents.\n",
    "\n",
    "### Q: Is LangSmith free?\n",
    "\n",
    "**A:** Yes! The free tier is generous (5,000 traces/month, unlimited Studio use). Perfect for learning and small projects.\n",
    "\n",
    "### Q: Can I use LangSmith with other frameworks?\n",
    "\n",
    "**A:** Yes! LangSmith works with any Python code that calls LLMs. You can manually instrument non-LangChain code.\n",
    "\n",
    "### Q: Does LangSmith store my data?\n",
    "\n",
    "**A:** Yes, traces are stored in LangSmith. But you can:\n",
    "- Control what gets traced\n",
    "- Self-host LangSmith\n",
    "- Use data retention policies\n",
    "- LangSmith is SOC 2 Type 2, GDPR, and HIPAA compliant\n",
    "\n",
    "### Q: How do I debug without LangSmith?\n",
    "\n",
    "**A:** You can use `print()` statements, but it's much harder. LangSmith shows you the complete picture with nice visualizations.\n",
    "\n",
    "### Q: Can I deploy without LangSmith Deployments?\n",
    "\n",
    "**A:** Yes! You can deploy to AWS, GCP, Azure, or any other platform. But LangSmith Deployments makes it much easier.\n",
    "\n",
    "### Q: How is Studio different from Jupyter notebooks?\n",
    "\n",
    "**A:** Jupyter is for running code. Studio is specifically for testing agents visually. It shows execution flow, lets you replay threads, and updates when you change code.\n",
    "\n",
    "### Q: Can I use LangSmith for production monitoring?\n",
    "\n",
    "**A:** Yes! That's one of its main uses. All production traces are automatically captured, and you can set up alerts, dashboards, and analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Best Practices\n",
    "\n",
    "### For Development\n",
    "\n",
    "1. **Always use Studio** - See your agent's execution visually\n",
    "2. **Use descriptive project names** - `my-chatbot-v1` not `test`\n",
    "3. **Add metadata to traces** - User IDs, session IDs help debugging\n",
    "4. **Tag your traces** - Makes filtering easier\n",
    "5. **Review traces regularly** - Learn from your agent's behavior\n",
    "\n",
    "### For Testing\n",
    "\n",
    "1. **Start with a small dataset** - 10-20 examples to begin\n",
    "2. **Add failing cases** - Every bug should become a test\n",
    "3. **Use multiple evaluators** - Correctness, helpfulness, safety, etc.\n",
    "4. **Compare versions** - Always A/B test improvements\n",
    "5. **Automate evaluation** - Run tests before deployment\n",
    "\n",
    "### For Production\n",
    "\n",
    "1. **Sample traces in production** - Don't trace 100% (use 10-20%)\n",
    "2. **Monitor key metrics** - Success rate, latency, cost\n",
    "3. **Set up alerts** - Get notified of errors\n",
    "4. **Use production traces for improvement** - Add edge cases to datasets\n",
    "5. **Version your prompts** - Track what changed when\n",
    "\n",
    "### For Teams\n",
    "\n",
    "1. **Use consistent project naming** - `team-name/agent-name/version`\n",
    "2. **Document evaluators** - Explain what each one checks\n",
    "3. **Share interesting traces** - LangSmith has sharing links\n",
    "4. **Review traces together** - Great for learning\n",
    "5. **Set quality standards** - \"Must pass 90% of tests before deploy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "LangSmith is a **complete platform** for building LLM applications:\n",
    "\n",
    "1. **üëÅÔ∏è Observability** - See what's happening with automatic tracing\n",
    "2. **‚úÖ Evaluation** - Test if it works with datasets and evaluators\n",
    "3. **üöÄ Deployment** - Ship to production in 15 minutes\n",
    "\n",
    "### How It Has Evolved\n",
    "\n",
    "**Old:** Just a tracing tool\n",
    "**Now:** Complete development, testing, and deployment platform\n",
    "\n",
    "**Old:** Manual setup required\n",
    "**Now:** Automatic integration with LangChain 1.0 and LangGraph 1.0\n",
    "\n",
    "**Old:** Limited to observability\n",
    "**Now:** Studio, evaluation, deployment, monitoring - everything!\n",
    "\n",
    "### Why Use LangSmith?\n",
    "\n",
    "**For Learning:**\n",
    "- See how agents actually work\n",
    "- Debug problems visually\n",
    "- Learn from mistakes\n",
    "\n",
    "**For Building:**\n",
    "- Develop faster with Studio\n",
    "- Test thoroughly with evaluation\n",
    "- Deploy easily with one click\n",
    "\n",
    "**For Production:**\n",
    "- Monitor everything\n",
    "- Catch issues early\n",
    "- Improve continuously\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "1. Sign up at [smith.langchain.com](https://smith.langchain.com) (free!)\n",
    "2. Set environment variables\n",
    "3. Run your LangChain code\n",
    "4. View traces in the UI\n",
    "5. Install Studio for local development\n",
    "\n",
    "That's it! You're ready to build better LLM applications with LangSmith.\n",
    "\n",
    "Happy building! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Official Documentation\n",
    "- [LangSmith Documentation](https://docs.langchain.com/langsmith)\n",
    "- [LangChain Observability Guide](https://docs.langchain.com/oss/python/langchain/observability)\n",
    "- [LangChain Studio Guide](https://docs.langchain.com/oss/python/langchain/studio)\n",
    "- [LangSmith Deployment Guide](https://docs.langchain.com/oss/python/langchain/deploy)\n",
    "\n",
    "\n",
    "Remember: The best way to learn is by doing. Build something, trace it, test it, deploy it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
