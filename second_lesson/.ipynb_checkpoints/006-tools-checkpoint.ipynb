{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55174e41-abf9-414d-b2d4-1ce572cdfd16",
   "metadata": {},
   "source": [
    "# Using Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f63b6ee-f6b3-4203-af2d-be53326f7f68",
   "metadata": {},
   "source": [
    "## What are tools?\n",
    "* Some use cases require Models to interface directly with tools: external systemsâ€”such as APIs, databases, or file systemsâ€”using structured input.\n",
    "* Some Models (e.g., OpenAI, Anthropic, and Gemini) feature built-in tools that are executed server-side, such as web search and code interpreters. See the [integrations](https://docs.langchain.com/oss/python/integrations) page for more info about it.\n",
    "* You can also create custom tools.\n",
    "* Tools extend the Model capabilities by letting it interact with the world through well-defined inputs and outputs.\n",
    "* Agentic LLM Applications (aka Agents) can decide if they need to use an available tool to achieve a goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5badac8f-3262-402d-96b2-e24ac6fe072c",
   "metadata": {},
   "source": [
    "## As always, let's load the .env file first\n",
    "* Remember, in the .env file we have our OpenAI API key and the Tavily API key (this will allow us to use the tool later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0004362f-c1d2-4d45-a043-82d74ddf593f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d06587e-beac-4305-94de-413893f5b9a4",
   "metadata": {},
   "source": [
    "#### Remember how you can get your Tavily API key\n",
    "\n",
    "Here's how to get your Tavily API key in simple steps:\n",
    "\n",
    "**1. Go to Tavily's website**\n",
    "Visit [tavily.com](https://tavily.com) and click on \"Get API Key\" or \"Sign Up\"\n",
    "\n",
    "**2. Create an account**\n",
    "Sign up using your email address or through a social login (Google, GitHub, etc.)\n",
    "\n",
    "**3. Verify your email**\n",
    "Check your inbox for a verification email and click the link to confirm your account\n",
    "\n",
    "**4. Get your free API key**\n",
    "Once logged in, you'll see your API key on the dashboard. Tavily offers a free tier that includes 1,000 API calls per month, which is more than enough for learning and doing course exercises\n",
    "\n",
    "**5. Copy and save the key**\n",
    "Copy the API key (it looks like a long string of letters and numbers). Store it somewhere safe - you'll need it for your LangChain exercises\n",
    "\n",
    "**6. Add it in your .env file**\n",
    "```\n",
    "TAVILY_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "**Important tip:** Never share your API key publicly or commit it to GitHub. The free tier should be sufficient for most course exercises, but you can always check your usage on the Tavily dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f990caf-8f89-4914-afb3-f2450e6b97d4",
   "metadata": {},
   "source": [
    "## First, let's try an \"Agent\" without tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a19e499-b0f5-480c-bc75-e3066ea32e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to real-time data or forecasts for specific dates beyond my last training cut-off in October 2023. For the most accurate and current weather information for San Francisco on January 3rd, 2026, I recommend checking a reliable weather website or app.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "system_prompt = \"You are a weather forecast assistant.\"\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"How is the weather today (Jan 3rd, 2026) in San Francisco?\")]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737fbaa8-59cb-4c15-8c1b-08fbdb0db3c2",
   "metadata": {},
   "source": [
    "## Then, let's try a very basic Agent using one external online search tool (Tavily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb1448a-98f4-4ab6-9be9-ef5408a7da8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'How is the weather today (Jan 3rd, 2026) in San Francisco?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'Weather in San Francisco',\n",
       "   'url': 'https://www.weatherapi.com/',\n",
       "   'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1767508796, 'localtime': '2026-01-03 22:39'}, 'current': {'last_updated_epoch': 1767508200, 'last_updated': '2026-01-03 22:30', 'temp_c': 12.8, 'temp_f': 55.0, 'is_day': 0, 'condition': {'text': 'Partly cloudy', 'icon': '//cdn.weatherapi.com/weather/64x64/night/116.png', 'code': 1003}, 'wind_mph': 15.2, 'wind_kph': 24.5, 'wind_degree': 179, 'wind_dir': 'S', 'pressure_mb': 1012.0, 'pressure_in': 29.88, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 89, 'cloud': 75, 'feelslike_c': 10.6, 'feelslike_f': 51.0, 'windchill_c': 10.9, 'windchill_f': 51.7, 'heatindex_c': 12.6, 'heatindex_f': 54.8, 'dewpoint_c': 10.7, 'dewpoint_f': 51.2, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 0.0, 'gust_mph': 23.8, 'gust_kph': 38.3}}\",\n",
       "   'score': 0.9999698,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=January',\n",
       "   'title': 'San Francisco weather in January 2026',\n",
       "   'content': 'The temperatures in San Francisco in January are quite cold with temperatures between 44Â°F and 59Â°F, warm clothes are a must. You can expect about 3 to 8 days',\n",
       "   'score': 0.99988043,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://en.climate-data.org/north-america/united-states-of-america/california/san-francisco-385/t/january-1/',\n",
       "   'title': 'Weather San Francisco in January 2026',\n",
       "   'content': 'Temperatures in the first third of the month average 13.6Â°C | 56.5Â°F for daily highs and 6.2Â°C | 43.2Â°F for lows. From the 11th to 20th, highs and lows are 14.2',\n",
       "   'score': 0.9998313,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://weathershogun.com/weather/usa/ca/san-francisco/480/january/2026-01-03',\n",
       "   'title': 'Saturday, January 3, 2026. San Francisco, CA - Weather ...',\n",
       "   'content': 'San Francisco, California weather forecast for Saturday, January 3, 2026. Get the latest on temperature, precipitation, wind speed, and UV.',\n",
       "   'score': 0.9998313,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://weatherspark.com/h/m/145212/2026/1/Historical-Weather-in-January-2026-at-San-Francisco-International-Airport-California-United-States',\n",
       "   'title': 'January 2026 Historical Weather at San Francisco ...',\n",
       "   'content': '6 Call Sign KSFO Temp. 57.9Â°F cool Dew Pt. 55.0Â°F comfortable Mostly Clear 800 ft Partly Cloudy 2,300 ft Vis. 29.93 inHg. This report shows the past weather',\n",
       "   'score': 0.9997818,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.26,\n",
       " 'request_id': 'f9fe49ea-a351-4f7e-a401-3fdafe1da681'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from typing import Dict, Any\n",
    "from tavily import TavilyClient\n",
    "\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "\n",
    "    return tavily_client.search(query)\n",
    "\n",
    "#testing the tool\n",
    "web_search.invoke(\"How is the weather today (Jan 3rd, 2026) in San Francisco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a7ca65-f0dc-4f7e-ad4b-22689510fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "weather_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[web_search]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f05b6ba0-5cdb-4851-8d64-f0f423a95303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today, January 3, 2026, the weather in San Francisco is as follows:\n",
      "\n",
      "- **Temperature**: 12.8Â°C (55.0Â°F)\n",
      "- **Condition**: Partly cloudy\n",
      "- **Wind**: 15.2 mph from the south (gusts up to 23.8 mph)\n",
      "- **Humidity**: 89%\n",
      "- **Visibility**: 16.0 km (9 miles)\n",
      "- **Pressure**: 1012 mb\n",
      "- **Feels Like**: 10.6Â°C (51.0Â°F)\n",
      "- **Precipitation**: 0.0 mm (no rain)\n",
      "\n",
      "Additionally, there is a coastal flood warning in effect due to high astronomical tides and storms, with expected flooding. \n",
      "\n",
      "Stay safe if you're out and about today!\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(content=\"How is the weather today (Jan 3rd, 2026) in San Francisco?\")\n",
    "\n",
    "response = weather_agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8725dd-94f8-4946-a82e-a7fd1c87bf9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='How is the weather today (Jan 3rd, 2026) in San Francisco?', additional_kwargs={}, response_metadata={}, id='2cad44a5-1e6f-48e8-b34b-f00fd104b096'),\n",
      " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 62, 'total_tokens': 84, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-CuC4BhHJ2igd8wNAUT51DXbXIS4zE', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b87bb-bdc7-7173-8daf-296d0dbbe094-0', tool_calls=[{'name': 'web_search', 'args': {'query': 'San Francisco weather January 3 2026'}, 'id': 'call_PZ0dH76oFQtkScQtbYr7ZqdX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 62, 'output_tokens': 22, 'total_tokens': 84, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      " ToolMessage(content='{\"query\": \"San Francisco weather January 3 2026\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"Weather in San Francisco, California\", \"url\": \"https://www.weatherapi.com/\", \"content\": \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.775, \\'lon\\': -122.4183, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1767508778, \\'localtime\\': \\'2026-01-03 22:39\\'}, \\'current\\': {\\'last_updated_epoch\\': 1767508200, \\'last_updated\\': \\'2026-01-03 22:30\\', \\'temp_c\\': 12.8, \\'temp_f\\': 55.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 15.2, \\'wind_kph\\': 24.5, \\'wind_degree\\': 179, \\'wind_dir\\': \\'S\\', \\'pressure_mb\\': 1012.0, \\'pressure_in\\': 29.88, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 89, \\'cloud\\': 75, \\'feelslike_c\\': 10.6, \\'feelslike_f\\': 51.0, \\'windchill_c\\': 10.9, \\'windchill_f\\': 51.7, \\'heatindex_c\\': 12.6, \\'heatindex_f\\': 54.8, \\'dewpoint_c\\': 10.7, \\'dewpoint_f\\': 51.2, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 0.0, \\'gust_mph\\': 23.8, \\'gust_kph\\': 38.3}}\", \"score\": 0.9828183, \"raw_content\": null}, {\"url\": \"https://world-weather.info/forecast/usa/san_francisco/january-2026/\", \"title\": \"Weather in San Francisco in January 2026 (California)\", \"content\": \"# Weather in San Francisco in January 2026. San Francisco Weather Forecast for January 2026 is based on long term prognosis and previous years\\' statistical data. 8.5 mph SE 29.9 inHg92 %07:25 AM05:02 PM. 16.3 mph S 29.8 inHg78 %07:25 AM05:03 PM. 11 mph S 29.8 inHg89 %07:25 AM05:05 PM. 12.5 mph W 30 inHg66 %07:25 AM05:06 PM. 11 mph NW 30.1 inHg55 %07:23 AM05:14 PM. 6 mph E 30.7 inHg49 %07:22 AM05:16 PM. 9.2 mph W 30 inHg73 %07:21 AM05:19 PM. 10.7 mph W 30 inHg70 %07:21 AM05:20 PM. 7.8 mph W 30.1 inHg68 %07:19 AM05:23 PM. 7.2 mph W 30 inHg75 %07:19 AM05:24 PM. 8.9 mph W 30 inHg70 %07:18 AM05:25 PM. 6.9 mph W 30.1 inHg71 %07:16 AM05:28 PM. 8.7 mph W 30 inHg71 %07:15 AM05:30 PM. 10.7 mph SE 30 inHg71 %07:14 AM05:32 PM. Wind direction chart for San Francisco (daytime), based on January 2026 wind statistics. ## Extended weather forecast in San Francisco.\", \"score\": 0.9416167, \"raw_content\": null}, {\"url\": \"https://weathershogun.com/weather/usa/ca/san-francisco/480/january/2026-01-03\", \"title\": \"Saturday, January 3, 2026. San Francisco, CA - Weather ...\", \"content\": \"### Coastal Flood Warning. Coastal Flood Warning issued January 2 at 6:42AM PST until January 3 at 2:00PM PST by NWS San Francisco CA. flooding expected due to high astronomical tides and storm. For the Coastal Flood Advisory, up to 2.0 ft inundation is. WHEN: For the Coastal Flood Warning, from 7 AM Friday to 2 PM. For the Coastal Flood Advisory, from 2 PM. Wind Advisory issued January 2 at 5:44AM PST until January 3 at 1:00PM PST by NWS San Francisco CA. Seashore, San Francisco Peninsula Coast, and East Bay Hills. ### Coastal Flood Advisory. Coastal Flood Advisory issued January 2 at 6:42AM PST until January 4 at 2:00PM PST by NWS San Francisco CA. flooding expected due to high astronomical tides and storm. For the Coastal Flood Advisory, up to 2.0 ft inundation is. WHEN: For the Coastal Flood Warning, from 7 AM Friday to 2 PM. For the Coastal Flood Advisory, from 2 PM.\", \"score\": 0.9387167, \"raw_content\": null}, {\"url\": \"https://www.weather25.com/north-america/usa/california/san-francisco?page=month&month=January\", \"title\": \"San Francisco weather in January 2026\", \"content\": \"Location was added to My Locations. Location was removed from My Locations. # San Francisco weather in January 2026. Click on a day for an hourly weather forecast. You can expect about **3 to 8 days of rain** in San Francisco during the month of January. Our weather forecast can give you a great sense of what weather to expect in **San Francisco in January 2026**. | 1  15Â° /10Â° | 2  14Â° /12Â° | 3  15Â° /12Â° |. Click on a day for an hourly weather forecast. ## Explore the weather in San Francisco in other months. | March | **17Â°** / 8Â° | 5 | 26 | 0 | 62 mm | Good | San Francisco in March |. | November | **18Â°** / 10Â° | 3 | 27 | 0 | 37 mm | Good | San Francisco in November |. Click on hotel for more details. Book NowThe Fairmont Heritage Place Ghirardelli Square.\", \"score\": 0.9344013, \"raw_content\": null}, {\"url\": \"https://robinhood.com/us/en/prediction-markets/climate/events/san-francisco-daily-temperature-high-january-3-2026-jan-03-2026/\", \"title\": \"San Francisco Daily Temperature High January 3 2026\", \"content\": \"# San Francisco Daily Temperature High January 3 2026. ## Select a contract. Determines the outcome of the contract. Usually within 1 hour of event resolution. What will the highest temperature in San Francisco be on January 3, 2026? Get $1 for every contract you own if your prediction is correct. Read the full contract terms and conditions. What will the highest temperature in LA be on January 3, 2026? What will the highest temperature in Dallas be on January 3, 2026? Snow in Salt Lake City in Jan 2026Snow in Philadelphia in Jan 2026What will the highest temperature in Seattle be on January 3, 2026? Snow in Detroit in Jan 2026What will the highest temperature in Denver be on January 3, 2026? Rain in Miami in Jan 2026What will the highest temperature in Chicago be on January 3, 2026? Read the Event Contracts Risk Disclosure for more information about the risks associated with event contracts. Event contracts are offered by Robinhood Derivatives, LLC through either KalshiEX LLC or ForecastEX, LLC.\", \"score\": 0.83678174, \"raw_content\": null}], \"response_time\": 2.0, \"request_id\": \"7746b44c-c984-47f6-bf33-1a8cc0cba857\"}', name='web_search', id='1213318c-8f9b-4f23-a874-956289832654', tool_call_id='call_PZ0dH76oFQtkScQtbYr7ZqdX'),\n",
      " AIMessage(content=\"Today, January 3, 2026, the weather in San Francisco is as follows:\\n\\n- **Temperature**: 12.8Â°C (55.0Â°F)\\n- **Condition**: Partly cloudy\\n- **Wind**: 15.2 mph from the south (gusts up to 23.8 mph)\\n- **Humidity**: 89%\\n- **Visibility**: 16.0 km (9 miles)\\n- **Pressure**: 1012 mb\\n- **Feels Like**: 10.6Â°C (51.0Â°F)\\n- **Precipitation**: 0.0 mm (no rain)\\n\\nAdditionally, there is a coastal flood warning in effect due to high astronomical tides and storms, with expected flooding. \\n\\nStay safe if you're out and about today!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 166, 'prompt_tokens': 1917, 'total_tokens': 2083, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bbc38b4db', 'id': 'chatcmpl-CuC4FfYRBrAcirjg2hEW5VuNIDTPu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b87bb-cc59-7b80-8f37-88f28309bf5e-0', usage_metadata={'input_tokens': 1917, 'output_tokens': 166, 'total_tokens': 2083, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff40859-5362-4bad-9503-9855e0909c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'web_search', 'args': {'query': 'San Francisco weather January 3 2026'}, 'id': 'call_PZ0dH76oFQtkScQtbYr7ZqdX', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][1].tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e932c6-fe53-4a38-a18f-6f604a6f17b3",
   "metadata": {},
   "source": [
    "## Let's explain the previous code in simple terms\n",
    "\n",
    "Below is the same code, explained **in simple terms, line by line**, as if youâ€™re new to Python + LangChain.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1) Load environment variables (API keys, secrets)\n",
    "\n",
    "```python\n",
    "from dotenv import load_dotenv\n",
    "```\n",
    "\n",
    "* Imports a helper function from the `python-dotenv` package.\n",
    "* This package lets you store secrets (like API keys) in a `.env` file.\n",
    "\n",
    "```python\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "* Reads your `.env` file and loads its values into â€œenvironment variablesâ€.\n",
    "* Example: if your `.env` contains `TAVILY_API_KEY=...`, your program can now access it automatically.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2) Imports for making a LangChain tool + typing\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "```\n",
    "\n",
    "* Imports the `@tool` decorator.\n",
    "* A â€œtoolâ€ in LangChain is basically a normal Python function that the agent is allowed to call.\n",
    "\n",
    "```python\n",
    "from typing import Dict, Any\n",
    "```\n",
    "\n",
    "* Imports type hints:\n",
    "\n",
    "  * `Dict[str, Any]` means â€œa dictionary with string keys and any type of valuesâ€.\n",
    "* Type hints help humans (and editors) understand what the function returns.\n",
    "\n",
    "```python\n",
    "from tavily import TavilyClient\n",
    "```\n",
    "\n",
    "* Imports Tavilyâ€™s Python client.\n",
    "* Tavily is a web search API service.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3) Create a Tavily client (the object that performs searches)\n",
    "\n",
    "```python\n",
    "tavily_client = TavilyClient()\n",
    "```\n",
    "\n",
    "* Creates a Tavily client instance.\n",
    "* Behind the scenes, this client reads your Tavily API key from the environment variables (which is why you loaded `.env` earlier).\n",
    "\n",
    "---\n",
    "\n",
    "#### 4) Define a LangChain tool function: `web_search`\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def web_search(query: str) -> Dict[str, Any]:\n",
    "```\n",
    "\n",
    "* `@tool` turns this Python function into a **LangChain tool**.\n",
    "* The agent can call it later.\n",
    "* The function takes one input:\n",
    "\n",
    "  * `query: str` â†’ a search string\n",
    "* It returns:\n",
    "\n",
    "  * `Dict[str, Any]` â†’ search results in dictionary form\n",
    "\n",
    "```python\n",
    "    \"\"\"Search the web for information\"\"\"\n",
    "```\n",
    "\n",
    "* This is the functionâ€™s docstring (a description).\n",
    "* Agents can sometimes read this description to decide when to use the tool.\n",
    "\n",
    "```python\n",
    "    return tavily_client.search(query)\n",
    "```\n",
    "\n",
    "* Actually performs a web search using Tavily.\n",
    "* Whatever Tavily returns (usually a dict with results) is returned by your tool.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Test the tool directly (without an agent)\n",
    "\n",
    "```python\n",
    "#testing the tool\n",
    "web_search.invoke(\"How is the weather today (Jan 3rd, 2026) in San Francisco?\")\n",
    "```\n",
    "\n",
    "* This calls the tool **manually**.\n",
    "* `invoke(...)` is LangChainâ€™s standard way to run a tool.\n",
    "* This is just to confirm â€œthe tool worksâ€ before giving it to an agent.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Create an agent and give it your tool\n",
    "\n",
    "```python\n",
    "from langchain.agents import create_agent\n",
    "```\n",
    "\n",
    "* Imports a helper that builds an â€œagentâ€.\n",
    "* An **agent** is an LLM app that can decide:\n",
    "\n",
    "  * â€œI should answer directlyâ€ OR\n",
    "  * â€œI should call a tool to get informationâ€\n",
    "\n",
    "```python\n",
    "weather_agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[web_search]\n",
    ")\n",
    "```\n",
    "\n",
    "* Creates an agent named `weather_agent`.\n",
    "* `model=\"gpt-4o-mini\"`: chooses which LLM to use.\n",
    "* `tools=[web_search]`: gives the agent permission to call your `web_search` tool.\n",
    "* So the agent can do things like: â€œTo answer how is the weather today in a particular location, I should search the web.â€ (Remember: as we saw with the previous Agent, the LLM cannot answer this question directly since it was trained with past data, not today's data. That is why it will need Tavily for that).\n",
    "\n",
    "---\n",
    "\n",
    "## 7) Create a user message\n",
    "\n",
    "```python\n",
    "from langchain.messages import HumanMessage\n",
    "```\n",
    "\n",
    "* Imports the â€œmessage objectâ€ representing something a human/user says.\n",
    "\n",
    "```python\n",
    "question = HumanMessage(content=\"How is the weather today (Jan 3rd, 2026) in San Francisco?\")\n",
    "```\n",
    "\n",
    "* Builds a human message with your question inside it.\n",
    "* Agents often work with chat-like â€œmessagesâ€ rather than plain strings.\n",
    "\n",
    "---\n",
    "\n",
    "## 8) Ask the agent the question (agent may call tools)\n",
    "\n",
    "```python\n",
    "response = weather_agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "```\n",
    "\n",
    "* Runs the agent.\n",
    "* You pass a dictionary with `\"messages\"` containing the conversation so far.\n",
    "* The agent will:\n",
    "\n",
    "  1. Read the question\n",
    "  2. Decide if it needs the web\n",
    "  3. Potentially call `web_search(...)`\n",
    "  4. Produce a final answer message\n",
    "\n",
    "The result (`response`) is typically a dictionary containing a `\"messages\"` list (the conversation including tool calls + final answer).\n",
    "\n",
    "---\n",
    "\n",
    "## 9) Print the final answer text\n",
    "\n",
    "```python\n",
    "print(response['messages'][-1].content)\n",
    "```\n",
    "\n",
    "* `response['messages']` is a list of messages.\n",
    "* `[-1]` means â€œthe last messageâ€.\n",
    "* `.content` is the text of that message.\n",
    "* So this prints the agentâ€™s final answer.\n",
    "\n",
    "---\n",
    "\n",
    "## 10) Pretty-print the full message history\n",
    "\n",
    "```python\n",
    "from pprint import pprint\n",
    "```\n",
    "\n",
    "* Imports a nicer printing function for complex Python objects.\n",
    "\n",
    "```python\n",
    "pprint(response['messages'])\n",
    "```\n",
    "\n",
    "* Prints *all* messages nicely.\n",
    "* This often includes:\n",
    "\n",
    "  * your HumanMessage\n",
    "  * tool call request(s)\n",
    "  * tool output message(s)\n",
    "  * final AI answer\n",
    "\n",
    "---\n",
    "\n",
    "## 11) Inspect the tool calls the agent made\n",
    "\n",
    "```python\n",
    "print(response[\"messages\"][1].tool_calls)\n",
    "```\n",
    "\n",
    "* Looks at the **second message** in the list (`[1]`).\n",
    "* Often, message `[1]` is the agent message where it decided to call tools (depends on the agent/framework behavior).\n",
    "* `.tool_calls` shows what tool(s) it tried to call, including arguments (like the query it used).\n",
    "\n",
    "---\n",
    "\n",
    "# Mental model summary (super simple)\n",
    "\n",
    "* You created a **web search tool** (`web_search`) that uses Tavily.\n",
    "* You created an **agent** that can use that tool.\n",
    "* You asked: â€œWhatâ€™s the weather?â€\n",
    "* The agent can decide: â€œI need the web â†’ call `web_search` â†’ read results â†’ answer.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c3bfa-b7d8-4c42-bd41-04c50572b767",
   "metadata": {},
   "source": [
    "## How to run this code from Visual Studio Code\n",
    "* Open Terminal.\n",
    "* Make sure you are in the project folder.\n",
    "* Make sure you have the poetry env activated.\n",
    "* Enter and run the following command:\n",
    "    * `python 006-tools.py` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c011c1bf-684d-4230-a771-e787c2bf6a3e",
   "metadata": {},
   "source": [
    "## Advanced Tip: Tools are most powerful when they can access agent state, runtime context, and long-term memory.\n",
    "This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations.\n",
    "\n",
    "Tools can access runtime information through the `ToolRuntime` parameter, which provides:\n",
    "* State - Mutable data that flows through execution (e.g., messages, counters, custom fields)\n",
    "* Context - Immutable configuration like user IDs, session details, or application-specific configuration\n",
    "* Store - Persistent long-term memory across conversations\n",
    "* Stream Writer - Stream custom updates as tools execute\n",
    "* Config - RunnableConfig for the execution\n",
    "* Tool Call ID - ID of the current tool call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e8ff0d-d613-4f89-9128-d510f0afb204",
   "metadata": {},
   "source": [
    "## The `ToolRuntime` parameter\n",
    "Let us explain the `ToolRuntime` parameter in simple terms.\n",
    "\n",
    "#### What is `ToolRuntime`?\n",
    "\n",
    "Think of `ToolRuntime` as a **magic backpack** that your tool automatically gets access to when it runs. This backpack contains important information and capabilities that your tool might need.\n",
    "\n",
    "\n",
    "#### What's Inside the Backpack?\n",
    "\n",
    "`ToolRuntime` gives your tool access to 6 important things:\n",
    "\n",
    "1. **State** - Information that changes as the conversation progresses (like the messages in the chat)\n",
    "2. **Context** - Fixed settings (like who the user is)\n",
    "3. **Store** - Long-term memory (like a database to remember things between conversations)\n",
    "4. **Streaming** - Ability to send updates while the tool is working\n",
    "5. **Config** - Technical settings\n",
    "6. **Tool Call ID** - A unique identifier for this specific tool run\n",
    "\n",
    "\n",
    "#### Why is This Useful?\n",
    "\n",
    "Before `ToolRuntime`, you had to use 4 different complicated methods to access these things. Now, you just need **one simple parameter** - much easier'. `ToolRuntime` replaces the older pattern of using separate\n",
    "    * InjectedState,\n",
    "    * InjectedStore,\n",
    "    * get_runtime,\n",
    "    * and InjectedToolCallId annotations.\n",
    "\n",
    "#### Example 1: Counting Messages\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Access the current conversation state\n",
    "@tool\n",
    "def summarize_conversation(\n",
    "    runtime: ToolRuntime\n",
    ") -> str:\n",
    "    \"\"\"Summarize the conversation so far.\"\"\"\n",
    "    messages = runtime.state[\"messages\"]\n",
    "\n",
    "    human_msgs = sum(1 for m in messages if m.__class__.__name__ == \"HumanMessage\")\n",
    "    ai_msgs = sum(1 for m in messages if m.__class__.__name__ == \"AIMessage\")\n",
    "    tool_msgs = sum(1 for m in messages if m.__class__.__name__ == \"ToolMessage\")\n",
    "\n",
    "    return f\"Conversation has {human_msgs} user messages, {ai_msgs} AI responses, and {tool_msgs} tool results\"\n",
    "```\n",
    "\n",
    "**What's happening here:**\n",
    "- The tool receives `runtime` automatically (you don't pass it manually)\n",
    "- It looks inside `runtime.state` to find all the messages in the conversation\n",
    "- It counts how many messages came from the human, AI, and tools\n",
    "- Returns a summary like \"Conversation has 3 user messages, 3 AI responses, and 2 tool results\"\n",
    "\n",
    "\n",
    "#### Example 2: Getting User Preferences\n",
    "\n",
    "```python\n",
    "# Access custom state fields\n",
    "@tool\n",
    "def get_user_preference(\n",
    "    pref_name: str,\n",
    "    runtime: ToolRuntime  # ToolRuntime parameter is not visible to the model\n",
    ") -> str:\n",
    "    \"\"\"Get a user preference value.\"\"\"\n",
    "    preferences = runtime.state.get(\"user_preferences\", {})\n",
    "    return preferences.get(pref_name, \"Not set\")\n",
    "```\n",
    "\n",
    "**What's happening here:**\n",
    "- This tool takes TWO parameters: `pref_name` (what preference to look up) and `runtime`\n",
    "- **Important**: The AI model only \"sees\" `pref_name` - it doesn't know about `runtime`\n",
    "- `runtime` is automatically provided by the system behind the scenes\n",
    "- The tool looks in the conversation's state for user preferences\n",
    "- If the preference exists, it returns the value; otherwise returns \"Not set\"\n",
    "\n",
    "\n",
    "#### Key Takeaway\n",
    "\n",
    "`ToolRuntime` is like having a **special helper** that gives your tool access to all the context it needs without cluttering your tool's visible parameters. It's automatically provided, the AI doesn't see it, and it makes your tools much more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5bf1b5-4517-4d78-82f4-ed12f27ce3c0",
   "metadata": {},
   "source": [
    "## Basic Example: using ToolRuntime to access Custom Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a556e3-a846-4ba4-8618-f8fbde4277f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The preferred vehicle is a Vespa.\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "# Here is where we create the custom context\n",
    "@dataclass\n",
    "class Preferences:\n",
    "    vehicle: str = \"Vespa\"\n",
    "    city: str = \"San Francisco\"\n",
    "\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "\n",
    "# Here is where we use the custom context\n",
    "@tool\n",
    "def get_preferred_vehicle(runtime: ToolRuntime[Preferences]) -> str:\n",
    "    \"\"\"Get the preferred vehicle\"\"\"\n",
    "    return runtime.context.vehicle\n",
    "\n",
    "@tool\n",
    "def get_preferred_city(runtime: ToolRuntime[Preferences]) -> str:\n",
    "    \"\"\"Get the preferred city\"\"\"\n",
    "    return runtime.context.city\n",
    "\n",
    "# Here is where we add the tools and the custom context to the agent\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools=[get_preferred_vehicle, get_preferred_city],\n",
    "    context_schema=Preferences\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What is the preferred vehicle?\")]},\n",
    "    context=Preferences()\n",
    ")\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeebe1c8-5e2f-444e-839e-22abad49f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What is the preferred vehicle?', additional_kwargs={}, response_metadata={}, id='e6def2af-b65d-4b3e-b99b-3160b9ea6c49'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 59, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuCDrZdrTzuJJacWAJUNLrZIzfmk3', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b87c4-e749-7653-823f-e50e633cbcce-0', tool_calls=[{'name': 'get_preferred_vehicle', 'args': {}, 'id': 'call_XbddXUYz6W0jDlTxRsg5Hxbn', 'type': 'tool_call'}], usage_metadata={'input_tokens': 59, 'output_tokens': 12, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content='Vespa', name='get_preferred_vehicle', id='c52b52dd-e445-4e7d-9465-3ba82b4cd1ab', tool_call_id='call_XbddXUYz6W0jDlTxRsg5Hxbn'),\n",
      "              AIMessage(content='The preferred vehicle is a Vespa.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 84, 'total_tokens': 93, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_29330a9688', 'id': 'chatcmpl-CuCDs8eofUZVKzNsUYTJpA3KwW6Rk', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b87c4-ec65-7710-8d45-e3e007df4fd2-0', usage_metadata={'input_tokens': 84, 'output_tokens': 9, 'total_tokens': 93, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387c7f7-a64d-4b9f-a320-2929e34b3f84",
   "metadata": {},
   "source": [
    "## Let's explain the previous code in simple terms\n",
    "\n",
    "Below is a **beginner-friendly, line-by-line explanation** of the code. We will explain **what each part does** and **why it exists**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Importing `dataclass`\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "```\n",
    "\n",
    "* This imports `dataclass`, a Python helper that makes it easy to create simple classes used to store data.\n",
    "* Think of a `dataclass` as a **clean container for variables**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Creating a custom context (shared memory)\n",
    "\n",
    "```python\n",
    "# Here is where we create the custom context\n",
    "\n",
    "@dataclass\n",
    "class Preferences:\n",
    "```\n",
    "\n",
    "* You are defining a **custom context object** called `Preferences`.\n",
    "* â€œContextâ€ means **information the agent and tools can access while running**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    vehicle: str = \"Vespa\"\n",
    "    city: str = \"San Francisco\"\n",
    "```\n",
    "\n",
    "* This context has two fields:\n",
    "\n",
    "  * `vehicle` â†’ default value `\"Vespa\"`\n",
    "  * `city` â†’ default value `\"San Francisco\"`\n",
    "* If you donâ€™t provide anything else, these defaults are used.\n",
    "* You can think of this like a **user profile or settings object**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Importing LangChain tool utilities\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool, ToolRuntime\n",
    "```\n",
    "\n",
    "* `@tool` is a decorator that turns a Python function into a **tool an agent can call**.\n",
    "* `ToolRuntime` is an object LangChain passes into tools so they can:\n",
    "\n",
    "  * Access context\n",
    "  * Access runtime data (like memory, config, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Creating a tool that reads from context\n",
    "\n",
    "```python\n",
    "# Here is where we use the custom context\n",
    "\n",
    "@tool\n",
    "def get_preferred_vehicle(runtime: ToolRuntime[Preferences]) -> str:\n",
    "```\n",
    "\n",
    "* This defines a **tool** named `get_preferred_vehicle`.\n",
    "* `@tool` tells LangChain:\n",
    "  ðŸ‘‰ â€œThe agent is allowed to call this function.â€\n",
    "* `runtime: ToolRuntime[Preferences]` means:\n",
    "\n",
    "  * LangChain will pass in a `runtime` object\n",
    "  * That runtime **contains a `Preferences` context**\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    \"\"\"Get the preferred vehicle\"\"\"\n",
    "```\n",
    "\n",
    "* This is a docstring.\n",
    "* LangChain uses this description to help the LLM understand **when to use the tool**.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    return runtime.context.vehicle\n",
    "```\n",
    "\n",
    "* `runtime.context` is the current `Preferences` object.\n",
    "* `.vehicle` reads the `vehicle` field.\n",
    "* This returns `\"Vespa\"` (unless the context was changed).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Second tool: preferred city\n",
    "\n",
    "```python\n",
    "@tool\n",
    "def get_preferred_city(runtime: ToolRuntime[Preferences]) -> str:\n",
    "```\n",
    "\n",
    "* Same idea as before, but for the city.\n",
    "* It also receives the runtime and context.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    \"\"\"Get the preferred city\"\"\"\n",
    "```\n",
    "\n",
    "* Tool description for the LLM.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    return runtime.context.city\n",
    "```\n",
    "\n",
    "* Reads `\"San Francisco\"` from the context and returns it.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Creating the agent\n",
    "\n",
    "```python\n",
    "# Here is where we add the tools and the custom context to the agent\n",
    "\n",
    "agent = create_agent(\n",
    "```\n",
    "\n",
    "* This creates an **AI agent**.\n",
    "* An agent can:\n",
    "\n",
    "  * Read messages\n",
    "  * Decide what tools to call\n",
    "  * Produce a final answer\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    model=\"gpt-4o-mini\",\n",
    "```\n",
    "\n",
    "* This tells the agent which language model to use.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    tools=[get_preferred_vehicle, get_preferred_city],\n",
    "```\n",
    "\n",
    "* These are the **only tools the agent is allowed to use**.\n",
    "* The agent can call them if it thinks they help answer the question.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    context_schema=Preferences\n",
    "```\n",
    "\n",
    "* This is very important.\n",
    "* It tells LangChain:\n",
    "\n",
    "  * â€œThis agent uses a context shaped like `Preferences`â€\n",
    "* LangChain now knows what fields exist in `runtime.context`.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Running the agent\n",
    "\n",
    "```python\n",
    "response = agent.invoke(\n",
    "```\n",
    "\n",
    "* This runs the agent.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    {\"messages\": [HumanMessage(content=\"What is the preferred vehicle?\")]},\n",
    "```\n",
    "\n",
    "* You send a **chat message** to the agent.\n",
    "* The user is asking:\n",
    "  ðŸ‘‰ â€œWhat is the preferred vehicle?â€\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "    context=Preferences()\n",
    "```\n",
    "\n",
    "* You provide an **instance of the context**.\n",
    "* Since you didnâ€™t override anything:\n",
    "\n",
    "  * `vehicle = \"Vespa\"`\n",
    "  * `city = \"San Francisco\"`\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Printing the final answer\n",
    "\n",
    "```python\n",
    "print(response['messages'][-1].content)\n",
    "```\n",
    "\n",
    "* `response['messages']` is the conversation history.\n",
    "* `[-1]` means â€œthe last messageâ€ (the agentâ€™s reply).\n",
    "* `.content` is the text of the reply.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. What happens internally (simple version)\n",
    "\n",
    "1. The agent receives the question\n",
    "   **â€œWhat is the preferred vehicle?â€**\n",
    "2. The model sees that:\n",
    "\n",
    "   * There is a tool named `get_preferred_vehicle`\n",
    "   * That tool returns exactly what the question asks\n",
    "3. The agent calls the tool\n",
    "4. The tool reads `runtime.context.vehicle`\n",
    "5. The agent responds with something like:\n",
    "\n",
    "```\n",
    "The preferred vehicle is Vespa.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. One-sentence mental model\n",
    "\n",
    "> **This code teaches an AI agent how to read structured user preferences (context) using tools and answer questions about them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d74bb6-04a7-46fc-8d77-2dfb89b41349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
