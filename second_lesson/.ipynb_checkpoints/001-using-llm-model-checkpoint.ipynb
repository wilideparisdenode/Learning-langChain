{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db618cb5-48f4-4bec-af5f-84f46f97b029",
   "metadata": {},
   "source": [
    "# Using an LLM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af86638-b9b2-4839-9059-776d4725359f",
   "metadata": {},
   "source": [
    "## LLM (aka Models)\n",
    "* LLMs (Large Language Models, aka \"Models\") can interpret and generate text like humans. Theyâ€™re versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2032b13-9b0a-4d93-a7ae-6d01124151f1",
   "metadata": {},
   "source": [
    "## What else can Models do?\n",
    "In addition to text generation, many models support:\n",
    "* **Tool calling** - calling external tools (like databases queries or API calls) and use results in their responses.\n",
    "* **Structured output** - where the modelâ€™s response is constrained to follow a defined format.\n",
    "* **Multimodality** - process and return data other than text, such as\n",
    "    * images,\n",
    "    * audio,\n",
    "    * and video.\n",
    "* **Reasoning** - models perform multi-step reasoning to arrive at a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98b418-72e5-44e0-b079-4e0c876c40ce",
   "metadata": {},
   "source": [
    "## The role of Models in LLM Apps and AI Agents\n",
    "Models can be used in two ways:\n",
    "* With agents - Models can be dynamically specified when creating an agent.\n",
    "* Standalone - Models can be called directly (outside of the agent loop) for tasks like\n",
    "    * text generation,\n",
    "    * classification,\n",
    "    * or extraction\n",
    "    * without the need for an agent framework.\n",
    "\n",
    "The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed.\n",
    "* **Models are the reasoning engine of LLM Apps and AI Agents**. They drive the agentâ€™s decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer.\n",
    "* **The quality and capabilities of the model you choose directly impact your agentâ€™s baseline reliability and performance**. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97684c2c-1e8e-4a7a-a9a9-6496968c3c13",
   "metadata": {},
   "source": [
    "## Remember, to use a Model you will need to load the .env file to make sure LangChain knows the API Key of the model you want to use\n",
    "* You will load the .env file at the beginning of your exercise like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0858d887-cd4b-4d6a-8df9-b27bbd400956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a22e8-a8df-4e43-8f65-64741ae73ba2",
   "metadata": {},
   "source": [
    "## LangChain provides a standarized way to use Models\n",
    "* Each model has a different API. Instead of having to learn each of these APIs, with LangChain you have an standarized way of accesing and using most Models our there.\n",
    "* For a full list of supported models in LangChain, see the [integrations](https://docs.langchain.com/oss/python/integrations/providers/overview) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4976ce-494f-45f6-9a87-4aeded027d9e",
   "metadata": {},
   "source": [
    "## What is the best OpenAI LLM Model to use in Beginner Class?\n",
    "For a LangChain **beginner** class in January 2026, we will pick **gpt-4o-mini** as the default.\n",
    "\n",
    "### Why **gpt-4o-mini** is the best â€œteaching defaultâ€ now?\n",
    "\n",
    "* **Predictable behavior for agents/tool-calling**: Beginners need runs that â€œjust workâ€ so they can learn *LangChain concepts* (messages, tools, memory, structured output) without chasing model quirks.\n",
    "* **Very low cost** while still being strong at instruction-following and basic coding help.\n",
    "* **Plenty of context** for class demos (128k context).\n",
    "\n",
    "### When to use **gpt-5-mini**\n",
    "\n",
    "We will use **gpt-5-mini** when we teach:\n",
    "\n",
    "* more complex **agentic patterns**,\n",
    "* more robust **tool selection/planning**,\n",
    "* longer chains where a bit more â€œbrainsâ€ helps students see fewer weird failures.\n",
    "\n",
    "Itâ€™s still relatively affordable compared to flagship models, but notably pricier than 4o-mini.\n",
    "\n",
    "### Why we would *not* use **gpt-5-nano** as the default (as the LangChain team does often in the LangChain documentation)\n",
    "\n",
    "* Itâ€™s marketed as the **fastest/cheapest** GPT-5 variant and is great for **summarization/classification**.\n",
    "* But there have been **real-world LangChain integration issues** reported where `gpt-5-nano` can return **empty content** without obvious errorsâ€”exactly the kind of â€œwhy didnâ€™t anything print?â€ confusion that derails a beginner class.\n",
    "\n",
    "### Our practical recommendation\n",
    "\n",
    "* **Default recommendation for all students:** **gpt-4o-mini**\n",
    "* **â€œAdvanced / bonusâ€ section:** switch to **gpt-5-mini**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081f954b-aac5-48f1-9ac6-1eac5b6b8a49",
   "metadata": {},
   "source": [
    "## Connect with the LLM Model\n",
    "* For this exercise, we will connect with the gpt-5-nano model from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5e09c43-c6aa-4ae7-b7df-6b393b8ba65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5589364-dd7b-454e-bc82-a9277ecba3cc",
   "metadata": {},
   "source": [
    "## What this code is doing\n",
    "\n",
    "```python\n",
    "model = init_chat_model(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0\n",
    ")\n",
    "```\n",
    "\n",
    "This code **creates a chat-based AI model** that your LangChain app can talk to.\n",
    "\n",
    "Think of it like **plugging an AI brain into your program** and deciding *how it should behave*.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. `init_chat_model(...)`\n",
    "\n",
    "This is a **LangChain helper function** that:\n",
    "\n",
    "* Connects to an OpenAI chat model\n",
    "* Wraps it in a LangChain-friendly interface\n",
    "* Returns an object you can use to send messages and get replies\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "> â€œSet up an AI chat model so my code can talk to it.â€\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. `model=\"gpt-4o-mini\"`\n",
    "\n",
    "This chooses **which AI model** you want to use.\n",
    "\n",
    "* **gpt-4o-mini** is:\n",
    "\n",
    "  * Fast âš¡\n",
    "  * Cheaper ðŸ’¸\n",
    "  * Still very capable for reasoning, summaries, classification, and drafting\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "> â€œUse a lightweight but smart version of GPT-4.â€\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. `temperature=0.0`\n",
    "\n",
    "This controls **how creative vs. predictable** the model is.\n",
    "\n",
    "* `temperature = 0.0` â†’ very factual, consistent, boring\n",
    "* `temperature = 1.0` â†’ more creative, varied, human-like\n",
    "* `temperature > 1.0` â†’ very imaginative, sometimes chaotic\n",
    "\n",
    "So here:\n",
    "\n",
    "> â€œProvide very factual, consistent, responses.â€\n",
    "\n",
    "\n",
    "### One-sentence summary\n",
    "\n",
    "> This code sets up a fast GPT-4-level chat model with factual responses so your LangChain app can talk to an AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf73780-0018-4c2d-a7ac-beb528154b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "President John F. Kennedy was assassinated on November 22, 1963, in Dallas, Texas. The official investigation, conducted by the Warren Commission, concluded that Lee Harvey Oswald acted alone in the assassination. However, various conspiracy theories have emerged over the years, suggesting the involvement of different individuals or groups. Despite extensive investigations, no definitive evidence has emerged to conclusively support these alternative theories.\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"Who killed JFK?\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da88ac-b763-44f0-aac4-088310c83daa",
   "metadata": {},
   "source": [
    "## .invoke vs. .stream to use the Model in LangChain\n",
    ".invoke and .stream are the most frequent ways of using a Model.\n",
    "* .invoke will generate responses in small chunks.\n",
    "* .stream will gererate responses in a continuous flow, like what you can see in chatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d3ac8b-1a6c-46f0-8d41-6107c86fc873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco is home to many excellent coffee shops, and the \"best\" one can vary depending on personal preferences. However, some popular and highly regarded options include:\n",
      "\n",
      "1. **Blue Bottle Coffee** - Known for its high-quality beans and meticulous brewing methods, Blue Bottle has several locations throughout the city.\n",
      "\n",
      "2. **Stumptown Coffee Roasters** - This Portland-based roaster has a location in the Mission District and is famous for its rich, flavorful coffee.\n",
      "\n",
      "3. **Philz Coffee** - A local favorite, Philz is known for its custom blends and unique brewing style, where each cup is made to order.\n",
      "\n",
      "4. **Ritual Coffee Roasters** - With a focus on direct trade and sustainability, Ritual offers a variety of single-origin coffees and blends.\n",
      "\n",
      "5. **Verve Coffee Roasters** - This Santa Cruz-based roaster has a location in the Mission and is known for its bright, flavorful coffees.\n",
      "\n",
      "6. **Sightglass Coffee** - A local roaster with a focus on quality and transparency, Sightglass has a beautiful flagship location in the SoMa district.\n",
      "\n",
      "Each of these coffee shops has its own unique atmosphere and offerings, so it might be worth trying a few to find your personal favorite!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(\"What is the best coffee shop in San Francisco?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9512d6d8-961c-4888-8f1a-765e017e96f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|San| Francisco| has| several| excellent| Spanish| restaurants|,| and| the| \"|best|\"| can| vary| depending| on| personal| preferences|.| However|,| a| few| highly| regarded| options| include|:\n",
      "\n",
      "|1|.| **|La| Mediterr|anee|**| -| Known| for| its| Mediterranean|-inspired| dishes|,| this| restaurant| offers| a| cozy| atmosphere| and| a| variety| of| tapas|.\n",
      "\n",
      "|2|.| **|B|oc|ad|illos|**| -| A| popular| spot| for| authentic| Spanish| tapas|,| Boc|ad|illos| features| a| menu| that| highlights| traditional| flavors| and| ingredients|.\n",
      "\n",
      "|3|.| **|P|iper|ade|**| -| This| restaurant| combines| Bas|que| and| French| influences|,| offering| a| unique| take| on| Spanish| cuisine| with| a| focus| on| seasonal| ingredients|.\n",
      "\n",
      "|4|.| **|Can|ela|**| -| A| modern| Spanish| restaurant| that| serves| a| range| of| tapas| and| pa|ellas|,| Can|ela| is| known| for| its| vibrant| atmosphere| and| creative| dishes|.\n",
      "\n",
      "|5|.| **|T|aver|na| Avent|ine|**| -| While| it| le|ans| more| towards| Italian|,| it| offers| a| selection| of| Spanish|-inspired| dishes| and| a| great| wine| list|.\n",
      "\n",
      "|It's| always| a| good| idea| to| check| recent| reviews| and| make| reservations|,| as| the| dining| scene| can| change| frequently|.||||"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"What is the best Spanish Restaurant in San Francisco?\"):\n",
    "    print(chunk.text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abeecb-e017-4720-a268-4b76587c679d",
   "metadata": {},
   "source": [
    "#### What `.invoke()` vs `.stream()` do:\n",
    "\n",
    "- **`.invoke()`**: Waits for the entire response, then gives it to you all at once\n",
    "- **`.stream()`**: Gives you the response piece-by-piece as it's being generated (like watching ChatGPT type in real-time)\n",
    "\n",
    "#### What `end=\"|\"` does in the .stream() example:\n",
    "\n",
    "Normally, Python's `print()` function adds a **newline** at the end, so each print appears on a new line:\n",
    "```\n",
    "chunk1\n",
    "chunk2\n",
    "chunk3\n",
    "```\n",
    "\n",
    "By using `end=\"|\"`, you're telling Python: \"Instead of a newline, put a pipe character `|` at the end.\" This makes everything print on the **same line**, separated by pipes:\n",
    "```\n",
    "chunk1|chunk2|chunk3|\n",
    "```\n",
    "\n",
    "#### Why use it in this example?\n",
    "\n",
    "The `end=\"|\"` is mainly **for demonstration purposes** - it helps you **visualize** that the response is arriving in separate chunks rather than all at once. You can literally see where each piece starts and ends.\n",
    "\n",
    "In a real application, you'd will usually use `end=\"\"` instead (which means \"add nothing at the end\"), so the text flows smoothly without separators - just like ChatGPT appears to you.\n",
    "\n",
    "The `flush=True` part ensures each chunk displays immediately instead of waiting in a buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f0912c-e391-4cff-9f98-5c9a2e65e295",
   "metadata": {},
   "source": [
    "## See how .stream works in most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b646a944-4fb5-445d-96ac-02e165c87c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are several Meetup groups in San Francisco that cater to European expats. While the \"best\" group can vary based on personal preferences and interests, here are a few popular options:\n",
      "\n",
      "1. **European Expats in San Francisco**: This group often hosts social events, networking opportunities, and cultural gatherings for Europeans living in the area.\n",
      "\n",
      "2. **International Friends in San Francisco**: While not exclusively for Europeans, this group includes many expats from various countries and focuses on socializing and making new friends.\n",
      "\n",
      "3. **Cultural Events and Activities**: Look for meetups that focus on specific European cultures, such as French, German, or Italian groups, which often host events related to their respective cultures.\n",
      "\n",
      "4. **Language Exchange Meetups**: These groups can be a great way to meet fellow Europeans while practicing languages and sharing cultural experiences.\n",
      "\n",
      "To find the best fit for you, consider checking the Meetup website or app for current events, group sizes, and member reviews. Additionally, attending a few different meetups can help you find the community that resonates most with you."
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"What is the best Meetup for European Expats in San Francisco?\"):\n",
    "    print(chunk.text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bbd0a0-716b-494c-90e0-9e5211920273",
   "metadata": {},
   "source": [
    "## Advanced options for Streaming in LangChain 1.0\n",
    "In LangChain 1.0 we have some advanced options with streaming:\n",
    "* Stream agent progress â€” get state updates after each agent step.\n",
    "    * This option is very common in most production-level AI Agents. \n",
    "* Stream LLM tokens â€” stream language model tokens as theyâ€™re generated.\n",
    "* Stream custom updates â€” emit user-defined signals (e.g., \"Fetched 10/100 records\").\n",
    "* Stream multiple modes â€” choose from updates (agent progress), messages (LLM tokens + metadata), or custom (arbitrary user data).\n",
    "\n",
    "You can see how to implement these advanced approaches in this [LangChain Documentation Page](https://docs.langchain.com/oss/python/langchain/streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e2b46-392a-49b9-8cfd-0ab7ce565b33",
   "metadata": {},
   "source": [
    "## Alternative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed938b3-5709-4f39-b6cf-7944194055da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = init_chat_model(model=\"claude-sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951eddf8-67f2-4e76-ae96-a19b6e30b7f9",
   "metadata": {},
   "source": [
    "#### Some models like Gemini require custom imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee5e56-5849-4e91-8c44-d5aebee751ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model3 = ChatGoogleGenerativeAI(model=\"gemini-3-pro-preview\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7307c00-89d2-45f4-b230-dab45d6feeb8",
   "metadata": {},
   "source": [
    "## How to run this code from Visual Studio Code\n",
    "* Open Terminal.\n",
    "* Make sure you are in the project folder.\n",
    "* Make sure you have the poetry env activated.\n",
    "* Enter and run the following command:\n",
    "    * `python 001-using-llm-model.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfe9f5-a509-478f-a7dc-bc696fa91a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
